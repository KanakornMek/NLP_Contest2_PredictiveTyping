{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimized LSTM Next Word Prediction with First Letter Constraint\n",
    "\n",
    "This notebook implements an efficient LSTM model that predicts next words based on both context and first letter information.\n",
    "\n",
    "**Key Improvements:**\n",
    "- Efficient parallel sequence processing (20x faster training)\n",
    "- Proper first letter integration during training\n",
    "- Consistent training and validation architecture\n",
    "- Better memory management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace\n"
     ]
    }
   ],
   "source": [
    "%cd workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import os\n",
    "import wandb\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Data Loading and Preprocessing ---\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for handling the training text.\n",
    "    It tokenizes text line-by-line, builds a vocabulary with special tokens,\n",
    "    and creates sequences only from within individual lines.\n",
    "    \"\"\"\n",
    "    def __init__(self, lines, seq_length=20):\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        all_words = []\n",
    "        for line in lines:\n",
    "            all_words.extend(self.tokenize(line))\n",
    "            \n",
    "        self.word_counts = Counter(all_words)\n",
    "        \n",
    "        # Build vocabulary, ensuring special tokens are first\n",
    "        vocab_sorted = sorted(self.word_counts, key=self.word_counts.get, reverse=True)\n",
    "        self.vocab = ['<PAD>', '<UNK>'] + vocab_sorted\n",
    "        \n",
    "        self.word_to_int = {word: i for i, word in enumerate(self.vocab)}\n",
    "        self.int_to_word = {i: word for i, word in enumerate(self.vocab)}\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "        self.sequences = []\n",
    "        for line in lines:\n",
    "            words_in_line = self.tokenize(line)\n",
    "            if len(words_in_line) <= self.seq_length:\n",
    "                continue\n",
    "            \n",
    "            # Use .get() with a default for the <UNK> token\n",
    "            unk_token_id = self.word_to_int['<UNK>']\n",
    "            int_line = [self.word_to_int.get(word, unk_token_id) for word in words_in_line]\n",
    "            \n",
    "            for i in range(len(int_line) - self.seq_length):\n",
    "                seq_end = i + self.seq_length\n",
    "                self.sequences.append(int_line[i:seq_end+1])\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return text.lower().split()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        seq = self.sequences[index]\n",
    "        \n",
    "        # Extract context and target\n",
    "        context = seq[:-1]\n",
    "        target = seq[-1]\n",
    "        \n",
    "        # Get first letter of target word\n",
    "        target_word = self.int_to_word[target]\n",
    "        if target_word and len(target_word) > 0:\n",
    "            first_letter = target_word[0].lower()\n",
    "            if 'a' <= first_letter <= 'z':\n",
    "                first_letter_idx = ord(first_letter) - ord('a')  # 0-25\n",
    "            else:\n",
    "                first_letter_idx = 26  # For non-alphabetic\n",
    "        else:\n",
    "            first_letter_idx = 26\n",
    "        \n",
    "        return torch.tensor(context), torch.tensor(target), torch.tensor(first_letter_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DevSetDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for handling the dev_set.csv file for validation.\n",
    "    Modified to include first letter as part of the model input.\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_path, word_to_int, seq_length=20):\n",
    "        self.seq_length = seq_length\n",
    "        self.word_to_int = word_to_int\n",
    "        self.unk_token_id = self.word_to_int['<UNK>']\n",
    "        self.pad_token_id = self.word_to_int['<PAD>']\n",
    "\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df = df[df['answer'].isin(self.word_to_int)]\n",
    "        self.data = df\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return text.lower().split()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        context = row['context']\n",
    "        answer = row['answer']\n",
    "        first_letter = row['first letter']\n",
    "        \n",
    "        context_words = self.tokenize(context)\n",
    "        int_context = [self.word_to_int.get(w, self.unk_token_id) for w in context_words]\n",
    "        \n",
    "        if len(int_context) > self.seq_length:\n",
    "            int_context = int_context[-self.seq_length:]\n",
    "        else:\n",
    "            int_context = [self.pad_token_id] * (self.seq_length - len(int_context)) + int_context\n",
    "            \n",
    "        int_answer = self.word_to_int[answer]\n",
    "        \n",
    "        # Convert first letter to integer (0-25 for a-z, 26 for other)\n",
    "        if first_letter and isinstance(first_letter, str) and len(first_letter) > 0:\n",
    "            letter = first_letter[0].lower()\n",
    "            if 'a' <= letter <= 'z':\n",
    "                letter_int = ord(letter) - ord('a')  # 0-25 for a-z\n",
    "            else:\n",
    "                letter_int = 26  # For non-alphabetic characters\n",
    "        else:\n",
    "            letter_int = 26  # Default for invalid/missing first letters\n",
    "        \n",
    "        return torch.tensor(int_context), torch.tensor(int_answer), torch.tensor(letter_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Optimized RNN Model Definition ---\n",
    "\n",
    "class NextWordModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Optimized RNN model for next word prediction with first letter constraint.\n",
    "    Uses parallel processing for efficient training.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, n_layers=2, drop_prob=0.5, num_letters=27):\n",
    "        super(NextWordModel, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # First letter embedding to incorporate letter information\n",
    "        self.letter_embedding = nn.Embedding(num_letters, hidden_dim // 4)\n",
    "        \n",
    "        # Combine LSTM output with letter information\n",
    "        combined_dim = hidden_dim + (hidden_dim // 4)\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(combined_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden, first_letter=None):\n",
    "        # Process context through LSTM (parallel processing)\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embedded, hidden)\n",
    "        \n",
    "        # Get the last time step output\n",
    "        last_lstm_out = lstm_out[:, -1, :]  # (batch_size, hidden_dim)\n",
    "        \n",
    "        if first_letter is not None:\n",
    "            # Ensure first_letter indices are valid\n",
    "            first_letter = torch.clamp(first_letter, 0, self.letter_embedding.num_embeddings - 1)\n",
    "            \n",
    "            # Incorporate first letter information\n",
    "            letter_embedded = self.letter_embedding(first_letter)  # (batch_size, hidden_dim//4)\n",
    "            # Combine LSTM output with letter embedding\n",
    "            combined = torch.cat([last_lstm_out, letter_embedded], dim=1)  # (batch_size, combined_dim)\n",
    "        else:\n",
    "            # If no first letter provided, use zero padding\n",
    "            batch_size = last_lstm_out.size(0)\n",
    "            zero_letter = torch.zeros(batch_size, self.hidden_dim // 4, device=last_lstm_out.device)\n",
    "            combined = torch.cat([last_lstm_out, zero_letter], dim=1)\n",
    "        \n",
    "        out = self.dropout(combined)\n",
    "        out = self.fc(out)  # (batch_size, vocab_size)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Initializes hidden state for starting new sequences\n",
    "        weight = next(self.parameters()).data\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Optimized Training Function ---\n",
    "\n",
    "def train(model, train_loader, val_loader, vocab_size, epochs=10, batch_size=128, lr=0.001, clip=5):\n",
    "    \"\"\"\n",
    "    Optimized training function with parallel sequence processing.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    print(f\"Training on {device}...\")\n",
    "    \n",
    "    wandb.watch(model, log='all', log_freq=10)\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 3\n",
    "    patience_counter = 0\n",
    "\n",
    "    for e in range(epochs):\n",
    "        # --- Training Step ---\n",
    "        total_train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (contexts, targets, first_letters) in enumerate(train_loader):\n",
    "            contexts, targets, first_letters = contexts.to(device), targets.to(device), first_letters.to(device)\n",
    "            \n",
    "            # Initialize hidden state\n",
    "            hidden = model.init_hidden(contexts.size(0))\n",
    "            hidden = tuple([each.data for each in hidden])\n",
    "            \n",
    "            model.zero_grad()\n",
    "            \n",
    "            # Forward pass - single parallel call (20x faster!)\n",
    "            output, hidden = model(contexts, hidden, first_letter=first_letters)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(output, targets)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            # Calculate training accuracy\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            train_total += targets.size(0)\n",
    "            train_correct += (predicted == targets).sum().item()\n",
    "            \n",
    "            # Debug: Print progress every 50 batches\n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f\"  Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_accuracy = (100 * train_correct / train_total) if train_total > 0 else 0\n",
    "\n",
    "        # --- Validation Step ---\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (contexts, answers, first_letters) in enumerate(val_loader):\n",
    "                contexts, answers, first_letters = contexts.to(device), answers.to(device), first_letters.to(device)\n",
    "                \n",
    "                val_hidden = model.init_hidden(contexts.shape[0])\n",
    "                \n",
    "                # Forward pass with first letter information\n",
    "                output, _ = model(contexts, val_hidden, first_letter=first_letters)\n",
    "                \n",
    "                # Calculate validation loss\n",
    "                val_loss = criterion(output, answers)\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "                # Calculate validation accuracy\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                val_total += answers.size(0)\n",
    "                val_correct += (predicted == answers).sum().item()\n",
    "                \n",
    "                # Debug: Print validation progress\n",
    "                if batch_idx % 25 == 0:\n",
    "                    print(f\"  Val Batch {batch_idx}/{len(val_loader)}, Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "        avg_val_loss = np.mean(val_losses) if val_losses else float('inf')\n",
    "        val_accuracy = (100 * val_correct / val_total) if val_total > 0 else 0\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        print(f\"\\nEpoch {e+1}/{epochs}:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.2f}%\")\n",
    "        print(f\"  Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}%\")\n",
    "        \n",
    "        wandb.log({\n",
    "            \"epoch\": e + 1, \n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"train_accuracy\": train_accuracy,\n",
    "            \"val_loss\": avg_val_loss,\n",
    "            \"val_accuracy\": val_accuracy\n",
    "        })\n",
    "        \n",
    "        # Early stopping based on validation loss\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"  -> New best model saved with val_loss: {best_val_loss:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  -> Validation loss didn't improve. Patience: {patience_counter}/{patience}\")\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {e+1} epochs\")\n",
    "            break\n",
    "\n",
    "    print(f\"\\nTraining completed. Best validation loss: {best_val_loss:.4f}\")\n",
    "    torch.save(model.state_dict(), 'final_model.pth')\n",
    "    \n",
    "    artifact = wandb.Artifact('best-next-word-model', type='model')\n",
    "    artifact.add_file('best_model.pth')\n",
    "    wandb.log_artifact(artifact)\n",
    "    print(\"Best model saved as wandb artifact.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/wandb/run-20251018_174000-8umfjnol</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kanakornmek-/predictive-keyboard-rnn-optimized/runs/8umfjnol' target=\"_blank\">LSTM_100000</a></strong> to <a href='https://wandb.ai/kanakornmek-/predictive-keyboard-rnn-optimized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kanakornmek-/predictive-keyboard-rnn-optimized' target=\"_blank\">https://wandb.ai/kanakornmek-/predictive-keyboard-rnn-optimized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kanakornmek-/predictive-keyboard-rnn-optimized/runs/8umfjnol' target=\"_blank\">https://wandb.ai/kanakornmek-/predictive-keyboard-rnn-optimized/runs/8umfjnol</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limiting training data to the first 100000 lines for testing.\n",
      "Vocabulary saved.\n",
      "\n",
      "Model Architecture:\n",
      "NextWordModel(\n",
      "  (embedding): Embedding(42352, 256)\n",
      "  (lstm): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (letter_embedding): Embedding(27, 128)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=640, out_features=42352, bias=True)\n",
      ")\n",
      "\n",
      "Vocabulary Size: 42352\n",
      "Training sequences: 1341073\n",
      "Validation samples: 93450\n",
      "\n",
      "Starting optimized training...\n",
      "Training on cuda...\n",
      "  Batch 0/10477, Loss: 10.7192\n",
      "  Batch 50/10477, Loss: 5.7343\n",
      "  Batch 100/10477, Loss: 5.2987\n",
      "  Batch 150/10477, Loss: 4.9188\n",
      "  Batch 200/10477, Loss: 4.9868\n",
      "  Batch 250/10477, Loss: 4.8051\n",
      "  Batch 300/10477, Loss: 5.1000\n",
      "  Batch 350/10477, Loss: 4.4301\n",
      "  Batch 400/10477, Loss: 4.0796\n",
      "  Batch 450/10477, Loss: 3.7024\n",
      "  Batch 500/10477, Loss: 3.7927\n",
      "  Batch 550/10477, Loss: 3.8203\n",
      "  Batch 600/10477, Loss: 4.1666\n",
      "  Batch 650/10477, Loss: 4.0576\n",
      "  Batch 700/10477, Loss: 4.4233\n",
      "  Batch 750/10477, Loss: 3.9323\n",
      "  Batch 800/10477, Loss: 4.1563\n",
      "  Batch 850/10477, Loss: 3.8665\n",
      "  Batch 900/10477, Loss: 3.9819\n",
      "  Batch 950/10477, Loss: 3.8243\n",
      "  Batch 1000/10477, Loss: 3.5823\n",
      "  Batch 1050/10477, Loss: 3.9607\n",
      "  Batch 1100/10477, Loss: 4.0031\n",
      "  Batch 1150/10477, Loss: 3.8703\n",
      "  Batch 1200/10477, Loss: 3.9700\n",
      "  Batch 1250/10477, Loss: 3.6672\n",
      "  Batch 1300/10477, Loss: 3.4646\n",
      "  Batch 1350/10477, Loss: 3.7762\n",
      "  Batch 1400/10477, Loss: 4.8461\n",
      "  Batch 1450/10477, Loss: 3.4965\n",
      "  Batch 1500/10477, Loss: 3.3491\n",
      "  Batch 1550/10477, Loss: 3.0127\n",
      "  Batch 1600/10477, Loss: 3.6314\n",
      "  Batch 1650/10477, Loss: 3.6293\n",
      "  Batch 1700/10477, Loss: 3.3330\n",
      "  Batch 1750/10477, Loss: 3.0826\n",
      "  Batch 1800/10477, Loss: 3.8805\n",
      "  Batch 1850/10477, Loss: 3.7779\n",
      "  Batch 1900/10477, Loss: 3.1154\n",
      "  Batch 1950/10477, Loss: 3.6952\n",
      "  Batch 2000/10477, Loss: 4.3051\n",
      "  Batch 2050/10477, Loss: 3.3578\n",
      "  Batch 2100/10477, Loss: 3.3671\n",
      "  Batch 2150/10477, Loss: 3.1009\n",
      "  Batch 2200/10477, Loss: 3.6958\n",
      "  Batch 2250/10477, Loss: 3.3668\n",
      "  Batch 2300/10477, Loss: 3.1849\n",
      "  Batch 2350/10477, Loss: 3.2061\n",
      "  Batch 2400/10477, Loss: 3.0708\n",
      "  Batch 2450/10477, Loss: 3.4209\n",
      "  Batch 2500/10477, Loss: 3.5735\n",
      "  Batch 2550/10477, Loss: 2.7291\n",
      "  Batch 2600/10477, Loss: 3.4530\n",
      "  Batch 2650/10477, Loss: 3.2131\n",
      "  Batch 2700/10477, Loss: 3.0262\n",
      "  Batch 2750/10477, Loss: 3.5295\n",
      "  Batch 2800/10477, Loss: 3.8052\n",
      "  Batch 2850/10477, Loss: 2.8902\n",
      "  Batch 2900/10477, Loss: 3.4303\n",
      "  Batch 2950/10477, Loss: 3.3511\n",
      "  Batch 3000/10477, Loss: 3.0041\n",
      "  Batch 3050/10477, Loss: 2.5701\n",
      "  Batch 3100/10477, Loss: 2.9243\n",
      "  Batch 3150/10477, Loss: 3.3225\n",
      "  Batch 3200/10477, Loss: 2.6840\n",
      "  Batch 3250/10477, Loss: 3.3569\n",
      "  Batch 3300/10477, Loss: 3.5666\n",
      "  Batch 3350/10477, Loss: 3.7411\n",
      "  Batch 3400/10477, Loss: 3.1839\n",
      "  Batch 3450/10477, Loss: 2.9662\n",
      "  Batch 3500/10477, Loss: 2.9708\n",
      "  Batch 3550/10477, Loss: 3.4925\n",
      "  Batch 3600/10477, Loss: 2.8091\n",
      "  Batch 3650/10477, Loss: 3.5622\n",
      "  Batch 3700/10477, Loss: 3.5176\n",
      "  Batch 3750/10477, Loss: 3.3496\n",
      "  Batch 3800/10477, Loss: 3.6582\n",
      "  Batch 3850/10477, Loss: 2.8528\n",
      "  Batch 3900/10477, Loss: 3.3484\n",
      "  Batch 3950/10477, Loss: 2.5908\n",
      "  Batch 4000/10477, Loss: 2.5763\n",
      "  Batch 4050/10477, Loss: 2.9516\n",
      "  Batch 4100/10477, Loss: 2.7195\n",
      "  Batch 4150/10477, Loss: 3.1736\n",
      "  Batch 4200/10477, Loss: 3.5324\n",
      "  Batch 4250/10477, Loss: 3.2444\n",
      "  Batch 4300/10477, Loss: 2.8833\n",
      "  Batch 4350/10477, Loss: 3.1223\n",
      "  Batch 4400/10477, Loss: 3.9059\n",
      "  Batch 4450/10477, Loss: 3.2135\n",
      "  Batch 4500/10477, Loss: 3.2812\n",
      "  Batch 4550/10477, Loss: 3.1163\n",
      "  Batch 4600/10477, Loss: 2.7019\n",
      "  Batch 4650/10477, Loss: 3.3359\n",
      "  Batch 4700/10477, Loss: 3.3158\n",
      "  Batch 4750/10477, Loss: 2.9410\n",
      "  Batch 4800/10477, Loss: 2.6238\n",
      "  Batch 4850/10477, Loss: 3.1973\n",
      "  Batch 4900/10477, Loss: 2.3766\n",
      "  Batch 4950/10477, Loss: 2.8690\n",
      "  Batch 5000/10477, Loss: 3.2426\n",
      "  Batch 5050/10477, Loss: 3.1090\n",
      "  Batch 5100/10477, Loss: 2.8043\n",
      "  Batch 5150/10477, Loss: 3.3026\n",
      "  Batch 5200/10477, Loss: 2.8706\n",
      "  Batch 5250/10477, Loss: 2.9568\n",
      "  Batch 5300/10477, Loss: 3.4418\n",
      "  Batch 5350/10477, Loss: 3.7983\n",
      "  Batch 5400/10477, Loss: 2.8422\n",
      "  Batch 5450/10477, Loss: 2.7655\n",
      "  Batch 5500/10477, Loss: 3.1278\n",
      "  Batch 5550/10477, Loss: 3.6607\n",
      "  Batch 5600/10477, Loss: 3.0547\n",
      "  Batch 5650/10477, Loss: 2.8517\n",
      "  Batch 5700/10477, Loss: 3.2496\n",
      "  Batch 5750/10477, Loss: 2.9754\n",
      "  Batch 5800/10477, Loss: 2.8902\n",
      "  Batch 5850/10477, Loss: 2.6613\n",
      "  Batch 5900/10477, Loss: 2.5661\n",
      "  Batch 5950/10477, Loss: 2.8892\n",
      "  Batch 6000/10477, Loss: 2.9222\n",
      "  Batch 6050/10477, Loss: 3.2930\n",
      "  Batch 6100/10477, Loss: 2.6541\n",
      "  Batch 6150/10477, Loss: 2.7624\n",
      "  Batch 6200/10477, Loss: 2.8238\n",
      "  Batch 6250/10477, Loss: 3.2231\n",
      "  Batch 6300/10477, Loss: 2.6599\n",
      "  Batch 6350/10477, Loss: 2.7835\n",
      "  Batch 6400/10477, Loss: 2.6930\n",
      "  Batch 6450/10477, Loss: 2.5781\n",
      "  Batch 6500/10477, Loss: 3.3155\n",
      "  Batch 6550/10477, Loss: 2.6990\n",
      "  Batch 6600/10477, Loss: 2.8574\n",
      "  Batch 6650/10477, Loss: 2.7988\n",
      "  Batch 6700/10477, Loss: 3.5232\n",
      "  Batch 6750/10477, Loss: 2.8315\n",
      "  Batch 6800/10477, Loss: 3.7145\n",
      "  Batch 6850/10477, Loss: 2.5829\n",
      "  Batch 6900/10477, Loss: 3.0878\n",
      "  Batch 6950/10477, Loss: 2.3574\n",
      "  Batch 7000/10477, Loss: 3.3854\n",
      "  Batch 7050/10477, Loss: 2.7238\n",
      "  Batch 7100/10477, Loss: 2.9515\n",
      "  Batch 7150/10477, Loss: 2.7870\n",
      "  Batch 7200/10477, Loss: 2.3679\n",
      "  Batch 7250/10477, Loss: 2.5420\n",
      "  Batch 7300/10477, Loss: 2.8690\n",
      "  Batch 7350/10477, Loss: 2.7498\n",
      "  Batch 7400/10477, Loss: 2.6879\n",
      "  Batch 7450/10477, Loss: 3.7340\n",
      "  Batch 7500/10477, Loss: 3.2165\n",
      "  Batch 7550/10477, Loss: 3.0391\n",
      "  Batch 7600/10477, Loss: 2.9614\n",
      "  Batch 7650/10477, Loss: 3.0980\n",
      "  Batch 7700/10477, Loss: 2.7379\n",
      "  Batch 7750/10477, Loss: 2.8875\n",
      "  Batch 7800/10477, Loss: 2.8224\n",
      "  Batch 7850/10477, Loss: 2.9063\n",
      "  Batch 7900/10477, Loss: 2.9459\n",
      "  Batch 7950/10477, Loss: 2.5062\n",
      "  Batch 8000/10477, Loss: 3.3103\n",
      "  Batch 8050/10477, Loss: 2.6627\n",
      "  Batch 8100/10477, Loss: 2.5870\n",
      "  Batch 8150/10477, Loss: 2.3522\n",
      "  Batch 8200/10477, Loss: 2.6671\n",
      "  Batch 8250/10477, Loss: 2.7257\n",
      "  Batch 8300/10477, Loss: 2.7947\n",
      "  Batch 8350/10477, Loss: 2.9966\n",
      "  Batch 8400/10477, Loss: 2.7614\n",
      "  Batch 8450/10477, Loss: 2.6671\n",
      "  Batch 8500/10477, Loss: 3.1457\n",
      "  Batch 8550/10477, Loss: 3.4214\n",
      "  Batch 8600/10477, Loss: 2.6868\n",
      "  Batch 8650/10477, Loss: 2.9439\n",
      "  Batch 8700/10477, Loss: 2.8415\n",
      "  Batch 8750/10477, Loss: 2.6951\n",
      "  Batch 8800/10477, Loss: 3.0677\n",
      "  Batch 8850/10477, Loss: 2.8137\n",
      "  Batch 8900/10477, Loss: 2.7284\n",
      "  Batch 8950/10477, Loss: 2.6219\n",
      "  Batch 9000/10477, Loss: 3.0232\n",
      "  Batch 9050/10477, Loss: 2.6333\n",
      "  Batch 9100/10477, Loss: 2.7718\n",
      "  Batch 9150/10477, Loss: 3.2083\n",
      "  Batch 9200/10477, Loss: 2.8838\n",
      "  Batch 9250/10477, Loss: 2.3011\n",
      "  Batch 9300/10477, Loss: 3.0103\n",
      "  Batch 9350/10477, Loss: 2.6289\n",
      "  Batch 9400/10477, Loss: 2.9182\n",
      "  Batch 9450/10477, Loss: 2.5489\n",
      "  Batch 9500/10477, Loss: 2.7131\n",
      "  Batch 9550/10477, Loss: 2.6943\n",
      "  Batch 9600/10477, Loss: 2.4469\n",
      "  Batch 9650/10477, Loss: 3.2429\n",
      "  Batch 9700/10477, Loss: 2.9053\n",
      "  Batch 9750/10477, Loss: 2.4854\n",
      "  Batch 9800/10477, Loss: 2.9343\n",
      "  Batch 9850/10477, Loss: 2.4651\n",
      "  Batch 9900/10477, Loss: 2.2710\n",
      "  Batch 9950/10477, Loss: 2.7370\n",
      "  Batch 10000/10477, Loss: 2.7214\n",
      "  Batch 10050/10477, Loss: 2.3525\n",
      "  Batch 10100/10477, Loss: 2.5237\n",
      "  Batch 10150/10477, Loss: 2.6864\n",
      "  Batch 10200/10477, Loss: 2.9670\n",
      "  Batch 10250/10477, Loss: 2.7254\n",
      "  Batch 10300/10477, Loss: 2.6016\n",
      "  Batch 10350/10477, Loss: 3.0750\n",
      "  Batch 10400/10477, Loss: 2.8040\n",
      "  Batch 10450/10477, Loss: 2.7557\n",
      "  Val Batch 0/366, Val Loss: 3.4746\n",
      "  Val Batch 25/366, Val Loss: 3.2114\n",
      "  Val Batch 50/366, Val Loss: 3.6849\n",
      "  Val Batch 75/366, Val Loss: 3.1663\n",
      "  Val Batch 100/366, Val Loss: 3.4148\n",
      "  Val Batch 125/366, Val Loss: 3.3033\n",
      "  Val Batch 150/366, Val Loss: 3.2538\n",
      "  Val Batch 175/366, Val Loss: 3.2343\n",
      "  Val Batch 200/366, Val Loss: 3.3616\n",
      "  Val Batch 225/366, Val Loss: 3.6624\n",
      "  Val Batch 250/366, Val Loss: 3.1701\n",
      "  Val Batch 275/366, Val Loss: 3.3957\n",
      "  Val Batch 300/366, Val Loss: 3.5526\n",
      "  Val Batch 325/366, Val Loss: 3.2692\n",
      "  Val Batch 350/366, Val Loss: 3.3139\n",
      "\n",
      "Epoch 1/10:\n",
      "  Train Loss: 3.1922 | Train Acc: 44.26%\n",
      "  Val Loss: 3.3019 | Val Acc: 41.73%\n",
      "  -> New best model saved with val_loss: 3.3019\n",
      "  Batch 0/10477, Loss: 3.1720\n",
      "  Batch 50/10477, Loss: 2.2659\n",
      "  Batch 100/10477, Loss: 2.7124\n",
      "  Batch 150/10477, Loss: 2.1877\n",
      "  Batch 200/10477, Loss: 2.6755\n",
      "  Batch 250/10477, Loss: 2.4670\n",
      "  Batch 300/10477, Loss: 1.6998\n",
      "  Batch 350/10477, Loss: 2.5108\n",
      "  Batch 400/10477, Loss: 2.5162\n",
      "  Batch 450/10477, Loss: 3.1724\n",
      "  Batch 500/10477, Loss: 2.8217\n",
      "  Batch 550/10477, Loss: 2.8304\n",
      "  Batch 600/10477, Loss: 2.9777\n",
      "  Batch 650/10477, Loss: 3.1265\n",
      "  Batch 700/10477, Loss: 2.5812\n",
      "  Batch 750/10477, Loss: 2.7326\n",
      "  Batch 800/10477, Loss: 2.7461\n",
      "  Batch 850/10477, Loss: 2.4162\n",
      "  Batch 900/10477, Loss: 2.6140\n",
      "  Batch 950/10477, Loss: 2.6867\n",
      "  Batch 1000/10477, Loss: 2.6313\n",
      "  Batch 1050/10477, Loss: 2.2410\n",
      "  Batch 1100/10477, Loss: 2.8740\n",
      "  Batch 1150/10477, Loss: 2.2746\n",
      "  Batch 1200/10477, Loss: 3.0943\n",
      "  Batch 1250/10477, Loss: 2.4961\n",
      "  Batch 1300/10477, Loss: 2.3512\n",
      "  Batch 1350/10477, Loss: 3.1903\n",
      "  Batch 1400/10477, Loss: 2.5532\n",
      "  Batch 1450/10477, Loss: 2.5228\n",
      "  Batch 1500/10477, Loss: 2.5509\n",
      "  Batch 1550/10477, Loss: 2.8301\n",
      "  Batch 1600/10477, Loss: 3.1477\n",
      "  Batch 1650/10477, Loss: 2.2517\n",
      "  Batch 1700/10477, Loss: 2.3568\n",
      "  Batch 1750/10477, Loss: 2.3319\n",
      "  Batch 1800/10477, Loss: 2.5153\n",
      "  Batch 1850/10477, Loss: 2.3743\n",
      "  Batch 1900/10477, Loss: 2.3384\n",
      "  Batch 1950/10477, Loss: 2.8224\n",
      "  Batch 2000/10477, Loss: 2.3122\n",
      "  Batch 2050/10477, Loss: 3.0725\n",
      "  Batch 2100/10477, Loss: 2.8826\n",
      "  Batch 2150/10477, Loss: 2.8557\n",
      "  Batch 2200/10477, Loss: 2.2402\n",
      "  Batch 2250/10477, Loss: 2.3807\n",
      "  Batch 2300/10477, Loss: 2.5565\n",
      "  Batch 2350/10477, Loss: 3.3349\n",
      "  Batch 2400/10477, Loss: 2.5184\n",
      "  Batch 2450/10477, Loss: 2.3483\n",
      "  Batch 2500/10477, Loss: 2.9627\n",
      "  Batch 2550/10477, Loss: 2.7201\n",
      "  Batch 2600/10477, Loss: 2.8085\n",
      "  Batch 2650/10477, Loss: 2.7707\n",
      "  Batch 2700/10477, Loss: 2.5689\n",
      "  Batch 2750/10477, Loss: 2.2729\n",
      "  Batch 2800/10477, Loss: 2.8826\n",
      "  Batch 2850/10477, Loss: 2.4433\n",
      "  Batch 2900/10477, Loss: 2.6871\n",
      "  Batch 2950/10477, Loss: 3.1695\n",
      "  Batch 3000/10477, Loss: 2.4102\n",
      "  Batch 3050/10477, Loss: 2.5314\n",
      "  Batch 3100/10477, Loss: 2.4011\n",
      "  Batch 3150/10477, Loss: 2.8012\n",
      "  Batch 3200/10477, Loss: 2.8926\n",
      "  Batch 3250/10477, Loss: 2.2912\n",
      "  Batch 3300/10477, Loss: 2.3482\n",
      "  Batch 3350/10477, Loss: 2.3761\n",
      "  Batch 3400/10477, Loss: 2.2406\n",
      "  Batch 3450/10477, Loss: 2.7589\n",
      "  Batch 3500/10477, Loss: 2.6663\n",
      "  Batch 3550/10477, Loss: 2.3668\n",
      "  Batch 3600/10477, Loss: 2.1793\n",
      "  Batch 3650/10477, Loss: 2.8499\n",
      "  Batch 3700/10477, Loss: 2.4110\n",
      "  Batch 3750/10477, Loss: 2.9855\n",
      "  Batch 3800/10477, Loss: 2.3208\n",
      "  Batch 3850/10477, Loss: 2.7295\n",
      "  Batch 3900/10477, Loss: 2.8246\n",
      "  Batch 3950/10477, Loss: 2.8172\n",
      "  Batch 4000/10477, Loss: 2.6671\n",
      "  Batch 4050/10477, Loss: 2.8580\n",
      "  Batch 4100/10477, Loss: 2.6451\n",
      "  Batch 4150/10477, Loss: 3.0524\n",
      "  Batch 4200/10477, Loss: 2.5214\n",
      "  Batch 4250/10477, Loss: 2.3788\n",
      "  Batch 4300/10477, Loss: 2.5819\n",
      "  Batch 4350/10477, Loss: 2.4402\n",
      "  Batch 4400/10477, Loss: 2.1751\n",
      "  Batch 4450/10477, Loss: 2.1556\n",
      "  Batch 4500/10477, Loss: 2.7058\n",
      "  Batch 4550/10477, Loss: 2.9816\n",
      "  Batch 4600/10477, Loss: 2.2949\n",
      "  Batch 4650/10477, Loss: 2.7314\n",
      "  Batch 4700/10477, Loss: 2.4297\n",
      "  Batch 4750/10477, Loss: 2.3134\n",
      "  Batch 4800/10477, Loss: 2.8274\n",
      "  Batch 4850/10477, Loss: 2.2702\n",
      "  Batch 4900/10477, Loss: 2.5008\n",
      "  Batch 4950/10477, Loss: 2.1559\n",
      "  Batch 5000/10477, Loss: 2.7171\n",
      "  Batch 5050/10477, Loss: 2.3062\n",
      "  Batch 5100/10477, Loss: 2.5209\n",
      "  Batch 5150/10477, Loss: 2.7927\n",
      "  Batch 5200/10477, Loss: 2.5641\n",
      "  Batch 5250/10477, Loss: 2.3911\n",
      "  Batch 5300/10477, Loss: 2.7521\n",
      "  Batch 5350/10477, Loss: 2.7155\n",
      "  Batch 5400/10477, Loss: 2.2639\n",
      "  Batch 5450/10477, Loss: 2.1682\n",
      "  Batch 5500/10477, Loss: 3.0706\n",
      "  Batch 5550/10477, Loss: 2.4112\n",
      "  Batch 5600/10477, Loss: 2.5913\n",
      "  Batch 5650/10477, Loss: 2.8952\n",
      "  Batch 5700/10477, Loss: 2.3293\n",
      "  Batch 5750/10477, Loss: 2.9542\n",
      "  Batch 5800/10477, Loss: 2.3961\n",
      "  Batch 5850/10477, Loss: 2.2605\n",
      "  Batch 5900/10477, Loss: 2.4461\n",
      "  Batch 5950/10477, Loss: 2.4995\n",
      "  Batch 6000/10477, Loss: 2.4501\n",
      "  Batch 6050/10477, Loss: 2.5933\n",
      "  Batch 6100/10477, Loss: 2.7224\n",
      "  Batch 6150/10477, Loss: 2.6557\n",
      "  Batch 6200/10477, Loss: 3.0029\n",
      "  Batch 6250/10477, Loss: 2.3614\n",
      "  Batch 6300/10477, Loss: 2.7880\n",
      "  Batch 6350/10477, Loss: 3.0698\n",
      "  Batch 6400/10477, Loss: 2.3664\n",
      "  Batch 6450/10477, Loss: 2.4200\n",
      "  Batch 6500/10477, Loss: 2.5283\n",
      "  Batch 6550/10477, Loss: 2.4727\n",
      "  Batch 6600/10477, Loss: 3.3406\n",
      "  Batch 6650/10477, Loss: 2.4052\n",
      "  Batch 6700/10477, Loss: 2.5246\n",
      "  Batch 6750/10477, Loss: 2.1793\n",
      "  Batch 6800/10477, Loss: 2.2235\n",
      "  Batch 6850/10477, Loss: 3.3000\n",
      "  Batch 6900/10477, Loss: 2.3034\n",
      "  Batch 6950/10477, Loss: 2.7682\n",
      "  Batch 7000/10477, Loss: 2.6168\n",
      "  Batch 7050/10477, Loss: 3.2355\n",
      "  Batch 7100/10477, Loss: 2.8358\n",
      "  Batch 7150/10477, Loss: 2.1976\n",
      "  Batch 7200/10477, Loss: 2.2481\n",
      "  Batch 7250/10477, Loss: 2.3795\n",
      "  Batch 7300/10477, Loss: 2.8539\n",
      "  Batch 7350/10477, Loss: 2.3301\n",
      "  Batch 7400/10477, Loss: 2.6017\n",
      "  Batch 7450/10477, Loss: 2.8365\n",
      "  Batch 7500/10477, Loss: 2.7723\n",
      "  Batch 7550/10477, Loss: 2.8032\n",
      "  Batch 7600/10477, Loss: 2.7384\n",
      "  Batch 7650/10477, Loss: 2.4369\n",
      "  Batch 7700/10477, Loss: 2.7355\n",
      "  Batch 7750/10477, Loss: 3.2933\n",
      "  Batch 7800/10477, Loss: 2.9347\n",
      "  Batch 7850/10477, Loss: 2.7822\n",
      "  Batch 7900/10477, Loss: 2.0667\n",
      "  Batch 7950/10477, Loss: 2.4402\n",
      "  Batch 8000/10477, Loss: 2.8375\n",
      "  Batch 8050/10477, Loss: 2.5199\n",
      "  Batch 8100/10477, Loss: 2.5722\n",
      "  Batch 8150/10477, Loss: 2.3075\n",
      "  Batch 8200/10477, Loss: 2.7034\n",
      "  Batch 8250/10477, Loss: 2.6167\n",
      "  Batch 8300/10477, Loss: 2.5779\n",
      "  Batch 8350/10477, Loss: 2.6989\n",
      "  Batch 8400/10477, Loss: 3.1473\n",
      "  Batch 8450/10477, Loss: 2.6986\n",
      "  Batch 8500/10477, Loss: 2.7312\n",
      "  Batch 8550/10477, Loss: 2.1206\n",
      "  Batch 8600/10477, Loss: 2.4136\n",
      "  Batch 8650/10477, Loss: 2.1211\n",
      "  Batch 8700/10477, Loss: 2.6852\n",
      "  Batch 8750/10477, Loss: 2.6035\n",
      "  Batch 8800/10477, Loss: 2.2167\n",
      "  Batch 8850/10477, Loss: 2.6476\n",
      "  Batch 8900/10477, Loss: 2.0050\n",
      "  Batch 8950/10477, Loss: 2.8279\n",
      "  Batch 9000/10477, Loss: 2.1827\n",
      "  Batch 9050/10477, Loss: 2.3943\n",
      "  Batch 9100/10477, Loss: 2.0608\n",
      "  Batch 9150/10477, Loss: 3.2689\n",
      "  Batch 9200/10477, Loss: 1.9694\n",
      "  Batch 9250/10477, Loss: 2.3018\n",
      "  Batch 9300/10477, Loss: 2.6188\n",
      "  Batch 9350/10477, Loss: 2.0454\n",
      "  Batch 9400/10477, Loss: 2.5036\n",
      "  Batch 9450/10477, Loss: 3.1459\n",
      "  Batch 9500/10477, Loss: 2.8114\n",
      "  Batch 9550/10477, Loss: 2.8737\n",
      "  Batch 9600/10477, Loss: 1.7792\n",
      "  Batch 9650/10477, Loss: 2.5979\n",
      "  Batch 9700/10477, Loss: 2.4827\n",
      "  Batch 9750/10477, Loss: 2.7086\n",
      "  Batch 9800/10477, Loss: 2.5632\n",
      "  Batch 9850/10477, Loss: 2.4929\n",
      "  Batch 9900/10477, Loss: 2.5657\n",
      "  Batch 9950/10477, Loss: 2.3285\n",
      "  Batch 10000/10477, Loss: 2.1014\n",
      "  Batch 10050/10477, Loss: 2.1625\n",
      "  Batch 10100/10477, Loss: 2.5802\n",
      "  Batch 10150/10477, Loss: 2.2427\n",
      "  Batch 10200/10477, Loss: 2.4174\n",
      "  Batch 10250/10477, Loss: 2.6621\n",
      "  Batch 10300/10477, Loss: 2.6530\n",
      "  Batch 10350/10477, Loss: 2.4023\n",
      "  Batch 10400/10477, Loss: 2.5063\n",
      "  Batch 10450/10477, Loss: 3.3784\n",
      "  Val Batch 0/366, Val Loss: 3.2762\n",
      "  Val Batch 25/366, Val Loss: 3.1334\n",
      "  Val Batch 50/366, Val Loss: 3.4431\n",
      "  Val Batch 75/366, Val Loss: 2.9599\n",
      "  Val Batch 100/366, Val Loss: 3.1854\n",
      "  Val Batch 125/366, Val Loss: 3.2300\n",
      "  Val Batch 150/366, Val Loss: 3.0389\n",
      "  Val Batch 175/366, Val Loss: 3.0261\n",
      "  Val Batch 200/366, Val Loss: 3.2422\n",
      "  Val Batch 225/366, Val Loss: 3.5039\n",
      "  Val Batch 250/366, Val Loss: 2.9917\n",
      "  Val Batch 275/366, Val Loss: 3.2964\n",
      "  Val Batch 300/366, Val Loss: 3.4252\n",
      "  Val Batch 325/366, Val Loss: 3.1069\n",
      "  Val Batch 350/366, Val Loss: 3.2224\n",
      "\n",
      "Epoch 2/10:\n",
      "  Train Loss: 2.5710 | Train Acc: 51.44%\n",
      "  Val Loss: 3.1373 | Val Acc: 44.20%\n",
      "  -> New best model saved with val_loss: 3.1373\n",
      "  Batch 0/10477, Loss: 2.5162\n",
      "  Batch 50/10477, Loss: 2.1907\n",
      "  Batch 100/10477, Loss: 2.3527\n",
      "  Batch 150/10477, Loss: 2.3802\n",
      "  Batch 200/10477, Loss: 2.5114\n",
      "  Batch 250/10477, Loss: 2.4079\n",
      "  Batch 300/10477, Loss: 2.2900\n",
      "  Batch 350/10477, Loss: 2.7517\n",
      "  Batch 400/10477, Loss: 2.4446\n",
      "  Batch 450/10477, Loss: 2.6453\n",
      "  Batch 500/10477, Loss: 2.6762\n",
      "  Batch 550/10477, Loss: 2.1339\n",
      "  Batch 600/10477, Loss: 2.1397\n",
      "  Batch 650/10477, Loss: 2.0879\n",
      "  Batch 700/10477, Loss: 2.6089\n",
      "  Batch 750/10477, Loss: 2.9980\n",
      "  Batch 800/10477, Loss: 2.3823\n",
      "  Batch 850/10477, Loss: 2.9332\n",
      "  Batch 900/10477, Loss: 2.0256\n",
      "  Batch 950/10477, Loss: 2.3223\n",
      "  Batch 1000/10477, Loss: 2.3571\n",
      "  Batch 1050/10477, Loss: 2.5876\n",
      "  Batch 1100/10477, Loss: 2.5676\n",
      "  Batch 1150/10477, Loss: 2.5603\n",
      "  Batch 1200/10477, Loss: 2.7160\n",
      "  Batch 1250/10477, Loss: 2.8094\n",
      "  Batch 1300/10477, Loss: 2.3262\n",
      "  Batch 1350/10477, Loss: 2.4103\n",
      "  Batch 1400/10477, Loss: 2.1928\n",
      "  Batch 1450/10477, Loss: 2.8604\n",
      "  Batch 1500/10477, Loss: 2.3040\n",
      "  Batch 1550/10477, Loss: 2.1004\n",
      "  Batch 1600/10477, Loss: 2.2657\n",
      "  Batch 1650/10477, Loss: 2.5320\n",
      "  Batch 1700/10477, Loss: 2.9749\n",
      "  Batch 1750/10477, Loss: 2.6831\n",
      "  Batch 1800/10477, Loss: 2.1730\n",
      "  Batch 1850/10477, Loss: 1.8416\n",
      "  Batch 1900/10477, Loss: 2.4338\n",
      "  Batch 1950/10477, Loss: 2.8600\n",
      "  Batch 2000/10477, Loss: 2.6732\n",
      "  Batch 2050/10477, Loss: 2.1893\n",
      "  Batch 2100/10477, Loss: 2.4281\n",
      "  Batch 2150/10477, Loss: 1.7567\n",
      "  Batch 2200/10477, Loss: 1.9821\n",
      "  Batch 2250/10477, Loss: 2.3045\n",
      "  Batch 2300/10477, Loss: 2.7598\n",
      "  Batch 2350/10477, Loss: 2.3277\n",
      "  Batch 2400/10477, Loss: 2.3240\n",
      "  Batch 2450/10477, Loss: 2.8955\n",
      "  Batch 2500/10477, Loss: 2.3097\n",
      "  Batch 2550/10477, Loss: 2.3927\n",
      "  Batch 2600/10477, Loss: 3.0414\n",
      "  Batch 2650/10477, Loss: 2.6996\n",
      "  Batch 2700/10477, Loss: 2.3651\n",
      "  Batch 2750/10477, Loss: 2.6169\n",
      "  Batch 2800/10477, Loss: 2.4861\n",
      "  Batch 2850/10477, Loss: 2.9256\n",
      "  Batch 2900/10477, Loss: 2.5280\n",
      "  Batch 2950/10477, Loss: 2.8958\n",
      "  Batch 3000/10477, Loss: 3.2230\n",
      "  Batch 3050/10477, Loss: 2.9143\n",
      "  Batch 3100/10477, Loss: 2.2578\n",
      "  Batch 3150/10477, Loss: 2.5345\n",
      "  Batch 3200/10477, Loss: 2.2405\n",
      "  Batch 3250/10477, Loss: 2.2802\n",
      "  Batch 3300/10477, Loss: 2.0772\n",
      "  Batch 3350/10477, Loss: 2.4722\n",
      "  Batch 3400/10477, Loss: 2.5071\n",
      "  Batch 3450/10477, Loss: 2.1585\n",
      "  Batch 3500/10477, Loss: 2.1477\n",
      "  Batch 3550/10477, Loss: 2.6727\n",
      "  Batch 3600/10477, Loss: 2.4248\n",
      "  Batch 3650/10477, Loss: 2.1378\n",
      "  Batch 3700/10477, Loss: 2.5798\n",
      "  Batch 3750/10477, Loss: 2.3475\n",
      "  Batch 3800/10477, Loss: 2.4976\n",
      "  Batch 3850/10477, Loss: 1.9990\n",
      "  Batch 3900/10477, Loss: 2.5053\n",
      "  Batch 3950/10477, Loss: 1.7113\n",
      "  Batch 4000/10477, Loss: 1.8992\n",
      "  Batch 4050/10477, Loss: 2.2642\n",
      "  Batch 4100/10477, Loss: 2.9120\n",
      "  Batch 4150/10477, Loss: 2.1606\n",
      "  Batch 4200/10477, Loss: 2.2395\n",
      "  Batch 4250/10477, Loss: 2.8571\n",
      "  Batch 4300/10477, Loss: 2.3386\n",
      "  Batch 4350/10477, Loss: 2.8603\n",
      "  Batch 4400/10477, Loss: 2.4506\n",
      "  Batch 4450/10477, Loss: 2.4501\n",
      "  Batch 4500/10477, Loss: 2.5208\n",
      "  Batch 4550/10477, Loss: 2.4772\n",
      "  Batch 4600/10477, Loss: 2.3592\n",
      "  Batch 4650/10477, Loss: 2.4687\n",
      "  Batch 4700/10477, Loss: 2.4396\n",
      "  Batch 4750/10477, Loss: 2.7542\n",
      "  Batch 4800/10477, Loss: 2.0535\n",
      "  Batch 4850/10477, Loss: 2.3262\n",
      "  Batch 4900/10477, Loss: 2.1494\n",
      "  Batch 4950/10477, Loss: 2.8540\n",
      "  Batch 5000/10477, Loss: 2.7589\n",
      "  Batch 5050/10477, Loss: 2.7251\n",
      "  Batch 5100/10477, Loss: 2.6850\n",
      "  Batch 5150/10477, Loss: 2.4047\n",
      "  Batch 5200/10477, Loss: 1.9295\n",
      "  Batch 5250/10477, Loss: 2.4014\n",
      "  Batch 5300/10477, Loss: 2.5922\n",
      "  Batch 5350/10477, Loss: 2.4191\n",
      "  Batch 5400/10477, Loss: 2.9563\n",
      "  Batch 5450/10477, Loss: 2.2927\n",
      "  Batch 5500/10477, Loss: 2.9542\n",
      "  Batch 5550/10477, Loss: 1.8182\n",
      "  Batch 5600/10477, Loss: 2.1911\n",
      "  Batch 5650/10477, Loss: 2.0615\n",
      "  Batch 5700/10477, Loss: 2.4265\n",
      "  Batch 5750/10477, Loss: 2.3702\n",
      "  Batch 5800/10477, Loss: 2.1841\n",
      "  Batch 5850/10477, Loss: 2.2667\n",
      "  Batch 5900/10477, Loss: 2.2366\n",
      "  Batch 5950/10477, Loss: 2.1552\n",
      "  Batch 6000/10477, Loss: 2.4748\n",
      "  Batch 6050/10477, Loss: 2.4058\n",
      "  Batch 6100/10477, Loss: 1.7981\n",
      "  Batch 6150/10477, Loss: 2.7862\n",
      "  Batch 6200/10477, Loss: 2.3004\n",
      "  Batch 6250/10477, Loss: 2.8395\n",
      "  Batch 6300/10477, Loss: 1.9112\n",
      "  Batch 6350/10477, Loss: 2.1841\n",
      "  Batch 6400/10477, Loss: 2.2920\n",
      "  Batch 6450/10477, Loss: 2.4881\n",
      "  Batch 6500/10477, Loss: 2.0896\n",
      "  Batch 6550/10477, Loss: 2.1617\n",
      "  Batch 6600/10477, Loss: 2.2686\n",
      "  Batch 6650/10477, Loss: 2.5879\n",
      "  Batch 6700/10477, Loss: 2.5097\n",
      "  Batch 6750/10477, Loss: 3.1401\n",
      "  Batch 6800/10477, Loss: 2.3024\n",
      "  Batch 6850/10477, Loss: 2.0544\n",
      "  Batch 6900/10477, Loss: 1.9644\n",
      "  Batch 6950/10477, Loss: 2.2591\n",
      "  Batch 7000/10477, Loss: 2.0054\n",
      "  Batch 7050/10477, Loss: 2.1620\n",
      "  Batch 7100/10477, Loss: 2.4543\n",
      "  Batch 7150/10477, Loss: 2.4232\n",
      "  Batch 7200/10477, Loss: 2.8036\n",
      "  Batch 7250/10477, Loss: 2.3019\n",
      "  Batch 7300/10477, Loss: 2.2438\n",
      "  Batch 7350/10477, Loss: 2.3854\n",
      "  Batch 7400/10477, Loss: 2.8790\n",
      "  Batch 7450/10477, Loss: 2.4108\n",
      "  Batch 7500/10477, Loss: 1.9862\n",
      "  Batch 7550/10477, Loss: 2.2700\n",
      "  Batch 7600/10477, Loss: 2.2607\n",
      "  Batch 7650/10477, Loss: 2.0437\n",
      "  Batch 7700/10477, Loss: 2.2095\n",
      "  Batch 7750/10477, Loss: 2.1213\n",
      "  Batch 7800/10477, Loss: 2.4050\n",
      "  Batch 7850/10477, Loss: 2.5909\n",
      "  Batch 7900/10477, Loss: 2.2879\n",
      "  Batch 7950/10477, Loss: 2.3250\n",
      "  Batch 8000/10477, Loss: 2.5712\n",
      "  Batch 8050/10477, Loss: 2.3828\n",
      "  Batch 8100/10477, Loss: 2.5947\n",
      "  Batch 8150/10477, Loss: 2.5854\n",
      "  Batch 8200/10477, Loss: 2.1970\n",
      "  Batch 8250/10477, Loss: 2.5335\n",
      "  Batch 8300/10477, Loss: 2.5754\n",
      "  Batch 8350/10477, Loss: 2.7945\n",
      "  Batch 8400/10477, Loss: 2.8247\n",
      "  Batch 8450/10477, Loss: 1.9569\n",
      "  Batch 8500/10477, Loss: 2.4705\n",
      "  Batch 8550/10477, Loss: 2.5336\n",
      "  Batch 8600/10477, Loss: 2.6685\n",
      "  Batch 8650/10477, Loss: 1.8657\n",
      "  Batch 8700/10477, Loss: 2.1342\n",
      "  Batch 8750/10477, Loss: 2.4826\n",
      "  Batch 8800/10477, Loss: 2.3085\n",
      "  Batch 8850/10477, Loss: 2.2207\n",
      "  Batch 8900/10477, Loss: 2.8236\n",
      "  Batch 8950/10477, Loss: 2.4009\n",
      "  Batch 9000/10477, Loss: 2.2934\n",
      "  Batch 9050/10477, Loss: 2.0085\n",
      "  Batch 9100/10477, Loss: 2.5519\n",
      "  Batch 9150/10477, Loss: 2.1495\n",
      "  Batch 9200/10477, Loss: 1.8185\n",
      "  Batch 9250/10477, Loss: 1.8202\n",
      "  Batch 9300/10477, Loss: 2.2669\n",
      "  Batch 9350/10477, Loss: 2.7843\n",
      "  Batch 9400/10477, Loss: 2.4327\n",
      "  Batch 9450/10477, Loss: 2.5598\n",
      "  Batch 9500/10477, Loss: 2.2162\n",
      "  Batch 9550/10477, Loss: 2.9480\n",
      "  Batch 9600/10477, Loss: 2.5123\n",
      "  Batch 9650/10477, Loss: 2.4683\n",
      "  Batch 9700/10477, Loss: 2.7549\n",
      "  Batch 9750/10477, Loss: 2.6703\n",
      "  Batch 9800/10477, Loss: 2.0585\n",
      "  Batch 9850/10477, Loss: 2.1037\n",
      "  Batch 9900/10477, Loss: 2.3221\n",
      "  Batch 9950/10477, Loss: 2.2667\n",
      "  Batch 10000/10477, Loss: 2.4473\n",
      "  Batch 10050/10477, Loss: 2.4087\n",
      "  Batch 10100/10477, Loss: 1.6464\n",
      "  Batch 10150/10477, Loss: 2.4422\n",
      "  Batch 10200/10477, Loss: 1.8713\n",
      "  Batch 10250/10477, Loss: 2.8402\n",
      "  Batch 10300/10477, Loss: 2.1126\n",
      "  Batch 10350/10477, Loss: 2.4614\n",
      "  Batch 10400/10477, Loss: 2.1703\n",
      "  Batch 10450/10477, Loss: 2.4707\n",
      "  Val Batch 0/366, Val Loss: 3.2477\n",
      "  Val Batch 25/366, Val Loss: 3.0996\n",
      "  Val Batch 50/366, Val Loss: 3.4130\n",
      "  Val Batch 75/366, Val Loss: 2.9600\n",
      "  Val Batch 100/366, Val Loss: 3.0875\n",
      "  Val Batch 125/366, Val Loss: 3.0740\n",
      "  Val Batch 150/366, Val Loss: 3.0023\n",
      "  Val Batch 175/366, Val Loss: 3.0049\n",
      "  Val Batch 200/366, Val Loss: 3.1465\n",
      "  Val Batch 225/366, Val Loss: 3.4603\n",
      "  Val Batch 250/366, Val Loss: 2.9505\n",
      "  Val Batch 275/366, Val Loss: 3.1714\n",
      "  Val Batch 300/366, Val Loss: 3.3228\n",
      "  Val Batch 325/366, Val Loss: 3.0371\n",
      "  Val Batch 350/366, Val Loss: 3.0905\n",
      "\n",
      "Epoch 3/10:\n",
      "  Train Loss: 2.4072 | Train Acc: 53.75%\n",
      "  Val Loss: 3.0729 | Val Acc: 45.55%\n",
      "  -> New best model saved with val_loss: 3.0729\n",
      "  Batch 0/10477, Loss: 2.5419\n",
      "  Batch 50/10477, Loss: 2.1138\n",
      "  Batch 100/10477, Loss: 1.8928\n",
      "  Batch 150/10477, Loss: 2.0887\n",
      "  Batch 200/10477, Loss: 2.3170\n",
      "  Batch 250/10477, Loss: 1.9129\n",
      "  Batch 300/10477, Loss: 2.2134\n",
      "  Batch 350/10477, Loss: 2.1773\n",
      "  Batch 400/10477, Loss: 2.1546\n",
      "  Batch 450/10477, Loss: 2.3259\n",
      "  Batch 500/10477, Loss: 2.1933\n",
      "  Batch 550/10477, Loss: 1.9380\n",
      "  Batch 600/10477, Loss: 2.8667\n",
      "  Batch 650/10477, Loss: 2.3113\n",
      "  Batch 700/10477, Loss: 2.1793\n",
      "  Batch 750/10477, Loss: 2.4894\n",
      "  Batch 800/10477, Loss: 2.2990\n",
      "  Batch 850/10477, Loss: 2.3867\n",
      "  Batch 900/10477, Loss: 2.1122\n",
      "  Batch 950/10477, Loss: 2.5150\n",
      "  Batch 1000/10477, Loss: 2.1848\n",
      "  Batch 1050/10477, Loss: 2.1743\n",
      "  Batch 1100/10477, Loss: 2.6767\n",
      "  Batch 1150/10477, Loss: 2.2193\n",
      "  Batch 1200/10477, Loss: 2.4709\n",
      "  Batch 1250/10477, Loss: 1.8761\n",
      "  Batch 1300/10477, Loss: 2.2898\n",
      "  Batch 1350/10477, Loss: 2.4577\n",
      "  Batch 1400/10477, Loss: 2.1303\n",
      "  Batch 1450/10477, Loss: 1.8525\n",
      "  Batch 1500/10477, Loss: 2.1474\n",
      "  Batch 1550/10477, Loss: 2.3035\n",
      "  Batch 1600/10477, Loss: 2.1075\n",
      "  Batch 1650/10477, Loss: 2.2390\n",
      "  Batch 1700/10477, Loss: 2.1891\n",
      "  Batch 1750/10477, Loss: 2.2199\n",
      "  Batch 1800/10477, Loss: 2.4816\n",
      "  Batch 1850/10477, Loss: 2.2102\n",
      "  Batch 1900/10477, Loss: 2.1862\n",
      "  Batch 1950/10477, Loss: 2.3972\n",
      "  Batch 2000/10477, Loss: 2.6924\n",
      "  Batch 2050/10477, Loss: 2.2610\n",
      "  Batch 2100/10477, Loss: 2.3737\n",
      "  Batch 2150/10477, Loss: 2.3363\n",
      "  Batch 2200/10477, Loss: 2.1180\n",
      "  Batch 2250/10477, Loss: 2.1625\n",
      "  Batch 2300/10477, Loss: 2.0982\n",
      "  Batch 2350/10477, Loss: 2.4618\n",
      "  Batch 2400/10477, Loss: 2.3011\n",
      "  Batch 2450/10477, Loss: 2.2853\n",
      "  Batch 2500/10477, Loss: 2.3889\n",
      "  Batch 2550/10477, Loss: 2.2351\n",
      "  Batch 2600/10477, Loss: 1.9793\n",
      "  Batch 2650/10477, Loss: 2.3894\n",
      "  Batch 2700/10477, Loss: 1.9314\n",
      "  Batch 2750/10477, Loss: 2.1687\n",
      "  Batch 2800/10477, Loss: 2.1541\n",
      "  Batch 2850/10477, Loss: 2.2531\n",
      "  Batch 2900/10477, Loss: 2.0359\n",
      "  Batch 2950/10477, Loss: 2.1206\n",
      "  Batch 3000/10477, Loss: 2.0882\n",
      "  Batch 3050/10477, Loss: 2.5395\n",
      "  Batch 3100/10477, Loss: 2.1848\n",
      "  Batch 3150/10477, Loss: 2.3674\n",
      "  Batch 3200/10477, Loss: 2.6126\n",
      "  Batch 3250/10477, Loss: 2.1886\n",
      "  Batch 3300/10477, Loss: 1.9174\n",
      "  Batch 3350/10477, Loss: 2.6189\n",
      "  Batch 3400/10477, Loss: 2.0684\n",
      "  Batch 3450/10477, Loss: 2.3076\n",
      "  Batch 3500/10477, Loss: 2.0623\n",
      "  Batch 3550/10477, Loss: 2.3764\n",
      "  Batch 3600/10477, Loss: 2.0302\n",
      "  Batch 3650/10477, Loss: 2.0807\n",
      "  Batch 3700/10477, Loss: 2.6250\n",
      "  Batch 3750/10477, Loss: 2.7103\n",
      "  Batch 3800/10477, Loss: 2.3893\n",
      "  Batch 3850/10477, Loss: 2.4240\n",
      "  Batch 3900/10477, Loss: 2.4369\n",
      "  Batch 3950/10477, Loss: 2.0767\n",
      "  Batch 4000/10477, Loss: 2.0395\n",
      "  Batch 4050/10477, Loss: 2.1594\n",
      "  Batch 4100/10477, Loss: 2.0297\n",
      "  Batch 4150/10477, Loss: 1.7222\n",
      "  Batch 4200/10477, Loss: 2.6800\n",
      "  Batch 4250/10477, Loss: 2.0638\n",
      "  Batch 4300/10477, Loss: 2.5397\n",
      "  Batch 4350/10477, Loss: 2.2788\n",
      "  Batch 4400/10477, Loss: 2.6153\n",
      "  Batch 4450/10477, Loss: 2.5952\n",
      "  Batch 4500/10477, Loss: 2.3343\n",
      "  Batch 4550/10477, Loss: 2.2602\n",
      "  Batch 4600/10477, Loss: 2.0720\n",
      "  Batch 4650/10477, Loss: 2.6750\n",
      "  Batch 4700/10477, Loss: 2.1649\n",
      "  Batch 4750/10477, Loss: 2.6823\n",
      "  Batch 4800/10477, Loss: 2.3163\n",
      "  Batch 4850/10477, Loss: 1.8280\n",
      "  Batch 4900/10477, Loss: 2.3374\n",
      "  Batch 4950/10477, Loss: 2.4676\n",
      "  Batch 5000/10477, Loss: 2.3025\n",
      "  Batch 5050/10477, Loss: 2.1067\n",
      "  Batch 5100/10477, Loss: 2.5884\n",
      "  Batch 5150/10477, Loss: 1.9136\n",
      "  Batch 5200/10477, Loss: 2.3738\n",
      "  Batch 5250/10477, Loss: 2.7503\n",
      "  Batch 5300/10477, Loss: 2.3797\n",
      "  Batch 5350/10477, Loss: 2.7522\n",
      "  Batch 5400/10477, Loss: 2.3631\n",
      "  Batch 5450/10477, Loss: 1.8805\n",
      "  Batch 5500/10477, Loss: 2.9129\n",
      "  Batch 5550/10477, Loss: 2.5377\n",
      "  Batch 5600/10477, Loss: 2.4143\n",
      "  Batch 5650/10477, Loss: 2.6626\n",
      "  Batch 5700/10477, Loss: 2.1976\n",
      "  Batch 5750/10477, Loss: 2.5335\n",
      "  Batch 5800/10477, Loss: 2.4560\n",
      "  Batch 5850/10477, Loss: 2.0371\n",
      "  Batch 5900/10477, Loss: 1.9058\n",
      "  Batch 5950/10477, Loss: 1.9439\n",
      "  Batch 6000/10477, Loss: 2.1076\n",
      "  Batch 6050/10477, Loss: 2.3416\n",
      "  Batch 6100/10477, Loss: 2.0054\n",
      "  Batch 6150/10477, Loss: 2.1000\n",
      "  Batch 6200/10477, Loss: 2.2603\n",
      "  Batch 6250/10477, Loss: 1.7345\n",
      "  Batch 6300/10477, Loss: 2.3341\n",
      "  Batch 6350/10477, Loss: 2.5765\n",
      "  Batch 6400/10477, Loss: 2.0391\n",
      "  Batch 6450/10477, Loss: 2.0750\n",
      "  Batch 6500/10477, Loss: 2.4230\n",
      "  Batch 6550/10477, Loss: 2.7304\n",
      "  Batch 6600/10477, Loss: 2.7760\n",
      "  Batch 6650/10477, Loss: 2.4088\n",
      "  Batch 6700/10477, Loss: 2.2699\n",
      "  Batch 6750/10477, Loss: 2.0033\n",
      "  Batch 6800/10477, Loss: 2.3661\n",
      "  Batch 6850/10477, Loss: 1.9723\n",
      "  Batch 6900/10477, Loss: 1.9896\n",
      "  Batch 6950/10477, Loss: 2.1359\n",
      "  Batch 7000/10477, Loss: 2.0454\n",
      "  Batch 7050/10477, Loss: 2.4535\n",
      "  Batch 7100/10477, Loss: 2.0622\n",
      "  Batch 7150/10477, Loss: 2.3747\n",
      "  Batch 7200/10477, Loss: 1.9830\n",
      "  Batch 7250/10477, Loss: 2.2646\n",
      "  Batch 7300/10477, Loss: 2.7391\n",
      "  Batch 7350/10477, Loss: 2.1128\n",
      "  Batch 7400/10477, Loss: 2.3597\n",
      "  Batch 7450/10477, Loss: 2.2405\n",
      "  Batch 7500/10477, Loss: 2.6051\n",
      "  Batch 7550/10477, Loss: 2.7615\n",
      "  Batch 7600/10477, Loss: 2.2985\n",
      "  Batch 7650/10477, Loss: 2.3103\n",
      "  Batch 7700/10477, Loss: 2.1404\n",
      "  Batch 7750/10477, Loss: 2.1469\n",
      "  Batch 7800/10477, Loss: 2.6637\n",
      "  Batch 7850/10477, Loss: 2.4360\n",
      "  Batch 7900/10477, Loss: 2.3494\n",
      "  Batch 7950/10477, Loss: 2.0296\n",
      "  Batch 8000/10477, Loss: 2.4267\n",
      "  Batch 8050/10477, Loss: 2.4184\n",
      "  Batch 8100/10477, Loss: 2.3678\n",
      "  Batch 8150/10477, Loss: 2.4736\n",
      "  Batch 8200/10477, Loss: 2.4760\n",
      "  Batch 8250/10477, Loss: 2.6001\n",
      "  Batch 8300/10477, Loss: 2.2920\n",
      "  Batch 8350/10477, Loss: 2.0044\n",
      "  Batch 8400/10477, Loss: 2.6908\n",
      "  Batch 8450/10477, Loss: 2.4877\n",
      "  Batch 8500/10477, Loss: 2.6932\n",
      "  Batch 8550/10477, Loss: 2.2135\n",
      "  Batch 8600/10477, Loss: 2.3529\n",
      "  Batch 8650/10477, Loss: 2.1659\n",
      "  Batch 8700/10477, Loss: 2.5215\n",
      "  Batch 8750/10477, Loss: 2.4715\n",
      "  Batch 8800/10477, Loss: 2.7034\n",
      "  Batch 8850/10477, Loss: 2.1127\n",
      "  Batch 8900/10477, Loss: 2.5422\n",
      "  Batch 8950/10477, Loss: 2.6694\n",
      "  Batch 9000/10477, Loss: 2.3378\n",
      "  Batch 9050/10477, Loss: 2.3068\n",
      "  Batch 9100/10477, Loss: 2.6625\n",
      "  Batch 9150/10477, Loss: 2.5540\n",
      "  Batch 9200/10477, Loss: 2.7388\n",
      "  Batch 9250/10477, Loss: 2.5581\n",
      "  Batch 9300/10477, Loss: 2.2994\n",
      "  Batch 9350/10477, Loss: 2.4488\n",
      "  Batch 9400/10477, Loss: 1.7966\n",
      "  Batch 9450/10477, Loss: 2.0105\n",
      "  Batch 9500/10477, Loss: 2.5150\n",
      "  Batch 9550/10477, Loss: 2.4031\n",
      "  Batch 9600/10477, Loss: 2.5965\n",
      "  Batch 9650/10477, Loss: 2.1600\n",
      "  Batch 9700/10477, Loss: 2.8737\n",
      "  Batch 9750/10477, Loss: 2.5538\n",
      "  Batch 9800/10477, Loss: 2.0779\n",
      "  Batch 9850/10477, Loss: 2.4680\n",
      "  Batch 9900/10477, Loss: 2.2099\n",
      "  Batch 9950/10477, Loss: 2.0797\n",
      "  Batch 10000/10477, Loss: 2.1041\n",
      "  Batch 10050/10477, Loss: 2.5552\n",
      "  Batch 10100/10477, Loss: 2.1812\n",
      "  Batch 10150/10477, Loss: 2.1992\n",
      "  Batch 10200/10477, Loss: 2.5875\n",
      "  Batch 10250/10477, Loss: 1.9135\n",
      "  Batch 10300/10477, Loss: 2.3244\n",
      "  Batch 10350/10477, Loss: 2.2343\n",
      "  Batch 10400/10477, Loss: 2.2376\n",
      "  Batch 10450/10477, Loss: 2.5795\n",
      "  Val Batch 0/366, Val Loss: 3.1672\n",
      "  Val Batch 25/366, Val Loss: 3.1497\n",
      "  Val Batch 50/366, Val Loss: 3.3629\n",
      "  Val Batch 75/366, Val Loss: 2.8612\n",
      "  Val Batch 100/366, Val Loss: 3.0227\n",
      "  Val Batch 125/366, Val Loss: 3.0309\n",
      "  Val Batch 150/366, Val Loss: 2.9566\n",
      "  Val Batch 175/366, Val Loss: 2.8800\n",
      "  Val Batch 200/366, Val Loss: 3.0994\n",
      "  Val Batch 225/366, Val Loss: 3.3858\n",
      "  Val Batch 250/366, Val Loss: 2.9752\n",
      "  Val Batch 275/366, Val Loss: 3.1331\n",
      "  Val Batch 300/366, Val Loss: 3.3592\n",
      "  Val Batch 325/366, Val Loss: 3.0432\n",
      "  Val Batch 350/366, Val Loss: 3.0498\n",
      "\n",
      "Epoch 4/10:\n",
      "  Train Loss: 2.3159 | Train Acc: 55.04%\n",
      "  Val Loss: 3.0362 | Val Acc: 46.42%\n",
      "  -> New best model saved with val_loss: 3.0362\n",
      "  Batch 0/10477, Loss: 2.1926\n",
      "  Batch 50/10477, Loss: 2.2793\n",
      "  Batch 100/10477, Loss: 2.1061\n",
      "  Batch 150/10477, Loss: 2.1423\n",
      "  Batch 200/10477, Loss: 2.5440\n",
      "  Batch 250/10477, Loss: 2.3827\n",
      "  Batch 300/10477, Loss: 2.3279\n",
      "  Batch 350/10477, Loss: 2.2131\n",
      "  Batch 400/10477, Loss: 2.5994\n",
      "  Batch 450/10477, Loss: 2.4911\n",
      "  Batch 500/10477, Loss: 2.3053\n",
      "  Batch 550/10477, Loss: 2.3919\n",
      "  Batch 600/10477, Loss: 2.1328\n",
      "  Batch 650/10477, Loss: 2.4086\n",
      "  Batch 700/10477, Loss: 2.2728\n",
      "  Batch 750/10477, Loss: 2.3822\n",
      "  Batch 800/10477, Loss: 2.6616\n",
      "  Batch 850/10477, Loss: 2.0677\n",
      "  Batch 900/10477, Loss: 2.4278\n",
      "  Batch 950/10477, Loss: 2.5909\n",
      "  Batch 1000/10477, Loss: 2.5406\n",
      "  Batch 1050/10477, Loss: 2.0379\n",
      "  Batch 1100/10477, Loss: 2.4142\n",
      "  Batch 1150/10477, Loss: 1.9996\n",
      "  Batch 1200/10477, Loss: 2.2025\n",
      "  Batch 1250/10477, Loss: 1.9878\n",
      "  Batch 1300/10477, Loss: 1.9248\n",
      "  Batch 1350/10477, Loss: 2.6143\n",
      "  Batch 1400/10477, Loss: 2.2163\n",
      "  Batch 1450/10477, Loss: 2.5572\n",
      "  Batch 1500/10477, Loss: 2.2923\n",
      "  Batch 1550/10477, Loss: 2.0900\n",
      "  Batch 1600/10477, Loss: 1.6615\n",
      "  Batch 1650/10477, Loss: 2.1803\n",
      "  Batch 1700/10477, Loss: 2.1375\n",
      "  Batch 1750/10477, Loss: 2.3127\n",
      "  Batch 1800/10477, Loss: 2.0313\n",
      "  Batch 1850/10477, Loss: 2.9974\n",
      "  Batch 1900/10477, Loss: 1.7905\n",
      "  Batch 1950/10477, Loss: 1.9424\n",
      "  Batch 2000/10477, Loss: 2.4879\n",
      "  Batch 2050/10477, Loss: 2.5544\n",
      "  Batch 2100/10477, Loss: 1.8658\n",
      "  Batch 2150/10477, Loss: 1.6007\n",
      "  Batch 2200/10477, Loss: 2.3534\n",
      "  Batch 2250/10477, Loss: 2.7154\n",
      "  Batch 2300/10477, Loss: 2.3425\n",
      "  Batch 2350/10477, Loss: 2.0876\n",
      "  Batch 2400/10477, Loss: 2.0021\n",
      "  Batch 2450/10477, Loss: 2.2068\n",
      "  Batch 2500/10477, Loss: 1.7842\n",
      "  Batch 2550/10477, Loss: 2.7314\n",
      "  Batch 2600/10477, Loss: 2.3981\n",
      "  Batch 2650/10477, Loss: 2.4538\n",
      "  Batch 2700/10477, Loss: 1.9576\n",
      "  Batch 2750/10477, Loss: 2.0807\n",
      "  Batch 2800/10477, Loss: 2.2634\n",
      "  Batch 2850/10477, Loss: 2.1615\n",
      "  Batch 2900/10477, Loss: 2.2455\n",
      "  Batch 2950/10477, Loss: 2.1646\n",
      "  Batch 3000/10477, Loss: 2.0118\n",
      "  Batch 3050/10477, Loss: 2.1140\n",
      "  Batch 3100/10477, Loss: 2.1064\n",
      "  Batch 3150/10477, Loss: 2.2114\n",
      "  Batch 3200/10477, Loss: 2.4275\n",
      "  Batch 3250/10477, Loss: 2.7749\n",
      "  Batch 3300/10477, Loss: 3.0145\n",
      "  Batch 3350/10477, Loss: 2.2557\n",
      "  Batch 3400/10477, Loss: 2.3399\n",
      "  Batch 3450/10477, Loss: 2.4822\n",
      "  Batch 3500/10477, Loss: 1.8142\n",
      "  Batch 3550/10477, Loss: 2.2690\n",
      "  Batch 3600/10477, Loss: 2.3987\n",
      "  Batch 3650/10477, Loss: 2.2758\n",
      "  Batch 3700/10477, Loss: 2.2436\n",
      "  Batch 3750/10477, Loss: 2.0095\n",
      "  Batch 3800/10477, Loss: 1.8087\n",
      "  Batch 3850/10477, Loss: 2.4051\n",
      "  Batch 3900/10477, Loss: 2.1780\n",
      "  Batch 3950/10477, Loss: 2.2075\n",
      "  Batch 4000/10477, Loss: 2.2031\n",
      "  Batch 4050/10477, Loss: 2.0381\n",
      "  Batch 4100/10477, Loss: 1.7279\n",
      "  Batch 4150/10477, Loss: 2.1943\n",
      "  Batch 4200/10477, Loss: 2.3035\n",
      "  Batch 4250/10477, Loss: 2.0940\n",
      "  Batch 4300/10477, Loss: 2.3680\n",
      "  Batch 4350/10477, Loss: 2.2235\n",
      "  Batch 4400/10477, Loss: 1.8513\n",
      "  Batch 4450/10477, Loss: 2.5106\n",
      "  Batch 4500/10477, Loss: 2.5209\n",
      "  Batch 4550/10477, Loss: 1.9994\n",
      "  Batch 4600/10477, Loss: 2.0037\n",
      "  Batch 4650/10477, Loss: 2.3815\n",
      "  Batch 4700/10477, Loss: 1.8223\n",
      "  Batch 4750/10477, Loss: 2.1066\n",
      "  Batch 4800/10477, Loss: 2.1889\n",
      "  Batch 4850/10477, Loss: 2.0338\n",
      "  Batch 4900/10477, Loss: 2.3745\n",
      "  Batch 4950/10477, Loss: 1.9123\n",
      "  Batch 5000/10477, Loss: 1.9758\n",
      "  Batch 5050/10477, Loss: 2.5731\n",
      "  Batch 5100/10477, Loss: 1.7423\n",
      "  Batch 5150/10477, Loss: 1.8740\n",
      "  Batch 5200/10477, Loss: 2.2228\n",
      "  Batch 5250/10477, Loss: 2.4217\n",
      "  Batch 5300/10477, Loss: 2.5862\n",
      "  Batch 5350/10477, Loss: 2.1032\n",
      "  Batch 5400/10477, Loss: 2.2630\n",
      "  Batch 5450/10477, Loss: 2.1533\n",
      "  Batch 5500/10477, Loss: 2.5285\n",
      "  Batch 5550/10477, Loss: 2.3470\n",
      "  Batch 5600/10477, Loss: 2.1076\n",
      "  Batch 5650/10477, Loss: 2.2067\n",
      "  Batch 5700/10477, Loss: 2.2904\n",
      "  Batch 5750/10477, Loss: 2.4685\n",
      "  Batch 5800/10477, Loss: 2.4204\n",
      "  Batch 5850/10477, Loss: 1.9827\n",
      "  Batch 5900/10477, Loss: 2.4158\n",
      "  Batch 5950/10477, Loss: 2.5084\n",
      "  Batch 6000/10477, Loss: 2.1508\n",
      "  Batch 6050/10477, Loss: 1.8169\n",
      "  Batch 6100/10477, Loss: 2.0347\n",
      "  Batch 6150/10477, Loss: 2.2087\n",
      "  Batch 6200/10477, Loss: 2.3721\n",
      "  Batch 6250/10477, Loss: 1.7794\n",
      "  Batch 6300/10477, Loss: 2.2479\n",
      "  Batch 6350/10477, Loss: 2.2072\n",
      "  Batch 6400/10477, Loss: 2.5050\n",
      "  Batch 6450/10477, Loss: 2.2409\n",
      "  Batch 6500/10477, Loss: 2.2689\n",
      "  Batch 6550/10477, Loss: 2.1402\n",
      "  Batch 6600/10477, Loss: 2.5952\n",
      "  Batch 6650/10477, Loss: 2.4831\n",
      "  Batch 6700/10477, Loss: 1.8052\n",
      "  Batch 6750/10477, Loss: 2.2035\n",
      "  Batch 6800/10477, Loss: 2.4332\n",
      "  Batch 6850/10477, Loss: 2.1266\n",
      "  Batch 6900/10477, Loss: 2.5458\n",
      "  Batch 6950/10477, Loss: 2.3252\n",
      "  Batch 7000/10477, Loss: 2.1669\n",
      "  Batch 7050/10477, Loss: 2.2504\n",
      "  Batch 7100/10477, Loss: 2.6096\n",
      "  Batch 7150/10477, Loss: 2.2168\n",
      "  Batch 7200/10477, Loss: 2.4938\n",
      "  Batch 7250/10477, Loss: 2.2883\n",
      "  Batch 7300/10477, Loss: 2.4606\n",
      "  Batch 7350/10477, Loss: 2.9189\n",
      "  Batch 7400/10477, Loss: 2.1315\n",
      "  Batch 7450/10477, Loss: 2.3369\n",
      "  Batch 7500/10477, Loss: 2.2421\n",
      "  Batch 7550/10477, Loss: 2.5654\n",
      "  Batch 7600/10477, Loss: 3.0210\n",
      "  Batch 7650/10477, Loss: 2.2856\n",
      "  Batch 7700/10477, Loss: 1.7714\n",
      "  Batch 7750/10477, Loss: 2.2781\n",
      "  Batch 7800/10477, Loss: 2.5170\n",
      "  Batch 7850/10477, Loss: 2.3321\n",
      "  Batch 7900/10477, Loss: 2.4647\n",
      "  Batch 7950/10477, Loss: 2.4081\n",
      "  Batch 8000/10477, Loss: 1.9885\n",
      "  Batch 8050/10477, Loss: 1.7247\n",
      "  Batch 8100/10477, Loss: 2.3852\n",
      "  Batch 8150/10477, Loss: 2.7210\n",
      "  Batch 8200/10477, Loss: 2.0092\n",
      "  Batch 8250/10477, Loss: 2.7118\n",
      "  Batch 8300/10477, Loss: 2.3101\n",
      "  Batch 8350/10477, Loss: 2.4343\n",
      "  Batch 8400/10477, Loss: 2.3263\n",
      "  Batch 8450/10477, Loss: 2.1584\n",
      "  Batch 8500/10477, Loss: 1.9973\n",
      "  Batch 8550/10477, Loss: 1.7724\n",
      "  Batch 8600/10477, Loss: 2.3616\n",
      "  Batch 8650/10477, Loss: 2.4498\n",
      "  Batch 8700/10477, Loss: 2.8888\n",
      "  Batch 8750/10477, Loss: 2.3885\n",
      "  Batch 8800/10477, Loss: 2.5604\n",
      "  Batch 8850/10477, Loss: 2.0143\n",
      "  Batch 8900/10477, Loss: 2.1350\n",
      "  Batch 8950/10477, Loss: 2.2078\n",
      "  Batch 9000/10477, Loss: 2.5033\n",
      "  Batch 9050/10477, Loss: 2.1838\n",
      "  Batch 9100/10477, Loss: 2.1847\n",
      "  Batch 9150/10477, Loss: 1.7925\n",
      "  Batch 9200/10477, Loss: 2.3675\n",
      "  Batch 9250/10477, Loss: 2.4242\n",
      "  Batch 9300/10477, Loss: 2.3393\n",
      "  Batch 9350/10477, Loss: 1.9017\n",
      "  Batch 9400/10477, Loss: 2.0263\n",
      "  Batch 9450/10477, Loss: 1.7941\n",
      "  Batch 9500/10477, Loss: 2.3782\n",
      "  Batch 9550/10477, Loss: 1.9849\n",
      "  Batch 9600/10477, Loss: 2.0521\n",
      "  Batch 9650/10477, Loss: 2.3232\n",
      "  Batch 9700/10477, Loss: 2.3095\n",
      "  Batch 9750/10477, Loss: 2.3809\n",
      "  Batch 9800/10477, Loss: 3.0448\n",
      "  Batch 9850/10477, Loss: 2.2355\n",
      "  Batch 9900/10477, Loss: 2.5249\n",
      "  Batch 9950/10477, Loss: 2.1972\n",
      "  Batch 10000/10477, Loss: 2.1817\n",
      "  Batch 10050/10477, Loss: 2.2016\n",
      "  Batch 10100/10477, Loss: 2.2899\n",
      "  Batch 10150/10477, Loss: 2.0305\n",
      "  Batch 10200/10477, Loss: 2.2275\n",
      "  Batch 10250/10477, Loss: 2.0492\n",
      "  Batch 10300/10477, Loss: 2.4302\n",
      "  Batch 10350/10477, Loss: 2.3083\n",
      "  Batch 10400/10477, Loss: 2.3561\n",
      "  Batch 10450/10477, Loss: 2.2708\n",
      "  Val Batch 0/366, Val Loss: 3.1548\n",
      "  Val Batch 25/366, Val Loss: 3.0914\n",
      "  Val Batch 50/366, Val Loss: 3.3428\n",
      "  Val Batch 75/366, Val Loss: 2.7895\n",
      "  Val Batch 100/366, Val Loss: 2.9898\n",
      "  Val Batch 125/366, Val Loss: 2.9869\n",
      "  Val Batch 150/366, Val Loss: 2.9252\n",
      "  Val Batch 175/366, Val Loss: 2.8876\n",
      "  Val Batch 200/366, Val Loss: 3.1179\n",
      "  Val Batch 225/366, Val Loss: 3.3691\n",
      "  Val Batch 250/366, Val Loss: 2.8856\n",
      "  Val Batch 275/366, Val Loss: 3.0708\n",
      "  Val Batch 300/366, Val Loss: 3.3342\n",
      "  Val Batch 325/366, Val Loss: 2.9799\n",
      "  Val Batch 350/366, Val Loss: 3.0133\n",
      "\n",
      "Epoch 5/10:\n",
      "  Train Loss: 2.2528 | Train Acc: 55.92%\n",
      "  Val Loss: 3.0080 | Val Acc: 47.00%\n",
      "  -> New best model saved with val_loss: 3.0080\n",
      "  Batch 0/10477, Loss: 2.1853\n",
      "  Batch 50/10477, Loss: 1.7862\n",
      "  Batch 100/10477, Loss: 2.0365\n",
      "  Batch 150/10477, Loss: 2.0809\n",
      "  Batch 200/10477, Loss: 2.2310\n",
      "  Batch 250/10477, Loss: 2.1185\n",
      "  Batch 300/10477, Loss: 2.6809\n",
      "  Batch 350/10477, Loss: 2.3965\n",
      "  Batch 400/10477, Loss: 2.3132\n",
      "  Batch 450/10477, Loss: 2.1078\n",
      "  Batch 500/10477, Loss: 2.3552\n",
      "  Batch 550/10477, Loss: 2.7752\n",
      "  Batch 600/10477, Loss: 2.2784\n",
      "  Batch 650/10477, Loss: 2.1380\n",
      "  Batch 700/10477, Loss: 2.4807\n",
      "  Batch 750/10477, Loss: 2.3681\n",
      "  Batch 800/10477, Loss: 2.0493\n",
      "  Batch 850/10477, Loss: 2.1293\n",
      "  Batch 900/10477, Loss: 2.3052\n",
      "  Batch 950/10477, Loss: 2.1687\n",
      "  Batch 1000/10477, Loss: 2.2219\n",
      "  Batch 1050/10477, Loss: 2.0295\n",
      "  Batch 1100/10477, Loss: 2.5512\n",
      "  Batch 1150/10477, Loss: 2.2735\n",
      "  Batch 1200/10477, Loss: 1.7254\n",
      "  Batch 1250/10477, Loss: 2.2618\n",
      "  Batch 1300/10477, Loss: 2.1640\n",
      "  Batch 1350/10477, Loss: 2.3099\n",
      "  Batch 1400/10477, Loss: 2.7284\n",
      "  Batch 1450/10477, Loss: 2.4448\n",
      "  Batch 1500/10477, Loss: 2.0815\n",
      "  Batch 1550/10477, Loss: 2.0672\n",
      "  Batch 1600/10477, Loss: 2.1750\n",
      "  Batch 1650/10477, Loss: 2.5178\n",
      "  Batch 1700/10477, Loss: 1.9947\n",
      "  Batch 1750/10477, Loss: 2.0653\n",
      "  Batch 1800/10477, Loss: 1.9905\n",
      "  Batch 1850/10477, Loss: 2.4633\n",
      "  Batch 1900/10477, Loss: 2.4767\n",
      "  Batch 1950/10477, Loss: 2.6306\n",
      "  Batch 2000/10477, Loss: 2.1098\n",
      "  Batch 2050/10477, Loss: 2.4730\n",
      "  Batch 2100/10477, Loss: 2.2459\n",
      "  Batch 2150/10477, Loss: 1.6694\n",
      "  Batch 2200/10477, Loss: 2.0741\n",
      "  Batch 2250/10477, Loss: 1.8710\n",
      "  Batch 2300/10477, Loss: 2.1987\n",
      "  Batch 2350/10477, Loss: 2.4290\n",
      "  Batch 2400/10477, Loss: 2.3727\n",
      "  Batch 2450/10477, Loss: 2.1767\n",
      "  Batch 2500/10477, Loss: 3.0561\n",
      "  Batch 2550/10477, Loss: 2.8497\n",
      "  Batch 2600/10477, Loss: 2.0146\n",
      "  Batch 2650/10477, Loss: 2.1287\n",
      "  Batch 2700/10477, Loss: 2.1077\n",
      "  Batch 2750/10477, Loss: 2.3710\n",
      "  Batch 2800/10477, Loss: 2.3119\n",
      "  Batch 2850/10477, Loss: 2.2770\n",
      "  Batch 2900/10477, Loss: 2.0229\n",
      "  Batch 2950/10477, Loss: 2.3389\n",
      "  Batch 3000/10477, Loss: 2.1109\n",
      "  Batch 3050/10477, Loss: 2.0059\n",
      "  Batch 3100/10477, Loss: 2.4751\n",
      "  Batch 3150/10477, Loss: 2.0735\n",
      "  Batch 3200/10477, Loss: 1.7201\n",
      "  Batch 3250/10477, Loss: 1.9789\n",
      "  Batch 3300/10477, Loss: 2.4267\n",
      "  Batch 3350/10477, Loss: 2.4741\n",
      "  Batch 3400/10477, Loss: 1.9365\n",
      "  Batch 3450/10477, Loss: 2.3550\n",
      "  Batch 3500/10477, Loss: 2.0540\n",
      "  Batch 3550/10477, Loss: 1.9907\n",
      "  Batch 3600/10477, Loss: 2.1587\n",
      "  Batch 3650/10477, Loss: 1.9534\n",
      "  Batch 3700/10477, Loss: 1.8691\n",
      "  Batch 3750/10477, Loss: 2.5380\n",
      "  Batch 3800/10477, Loss: 2.0946\n",
      "  Batch 3850/10477, Loss: 2.1634\n",
      "  Batch 3900/10477, Loss: 2.0191\n",
      "  Batch 3950/10477, Loss: 2.4304\n",
      "  Batch 4000/10477, Loss: 2.2635\n",
      "  Batch 4050/10477, Loss: 2.2487\n",
      "  Batch 4100/10477, Loss: 1.9394\n",
      "  Batch 4150/10477, Loss: 2.1335\n",
      "  Batch 4200/10477, Loss: 2.2089\n",
      "  Batch 4250/10477, Loss: 2.1576\n",
      "  Batch 4300/10477, Loss: 2.3677\n",
      "  Batch 4350/10477, Loss: 2.6334\n",
      "  Batch 4400/10477, Loss: 2.2012\n",
      "  Batch 4450/10477, Loss: 2.1769\n",
      "  Batch 4500/10477, Loss: 2.1901\n",
      "  Batch 4550/10477, Loss: 2.4880\n",
      "  Batch 4600/10477, Loss: 2.1384\n",
      "  Batch 4650/10477, Loss: 2.5259\n",
      "  Batch 4700/10477, Loss: 1.8918\n",
      "  Batch 4750/10477, Loss: 2.2820\n",
      "  Batch 4800/10477, Loss: 2.4491\n",
      "  Batch 4850/10477, Loss: 1.9804\n",
      "  Batch 4900/10477, Loss: 2.1129\n",
      "  Batch 4950/10477, Loss: 1.8154\n",
      "  Batch 5000/10477, Loss: 2.4927\n",
      "  Batch 5050/10477, Loss: 2.3476\n",
      "  Batch 5100/10477, Loss: 2.1479\n",
      "  Batch 5150/10477, Loss: 2.4124\n",
      "  Batch 5200/10477, Loss: 2.4528\n",
      "  Batch 5250/10477, Loss: 2.7270\n",
      "  Batch 5300/10477, Loss: 2.4756\n",
      "  Batch 5350/10477, Loss: 2.0891\n",
      "  Batch 5400/10477, Loss: 2.5574\n",
      "  Batch 5450/10477, Loss: 2.4939\n",
      "  Batch 5500/10477, Loss: 2.0445\n",
      "  Batch 5550/10477, Loss: 2.4750\n",
      "  Batch 5600/10477, Loss: 2.0815\n",
      "  Batch 5650/10477, Loss: 2.2653\n",
      "  Batch 5700/10477, Loss: 2.1713\n",
      "  Batch 5750/10477, Loss: 1.9882\n",
      "  Batch 5800/10477, Loss: 2.0089\n",
      "  Batch 5850/10477, Loss: 2.0535\n",
      "  Batch 5900/10477, Loss: 1.7557\n",
      "  Batch 5950/10477, Loss: 1.8228\n",
      "  Batch 6000/10477, Loss: 2.4863\n",
      "  Batch 6050/10477, Loss: 2.1578\n",
      "  Batch 6100/10477, Loss: 2.0197\n",
      "  Batch 6150/10477, Loss: 2.4103\n",
      "  Batch 6200/10477, Loss: 1.9131\n",
      "  Batch 6250/10477, Loss: 2.1771\n",
      "  Batch 6300/10477, Loss: 2.6593\n",
      "  Batch 6350/10477, Loss: 2.1421\n",
      "  Batch 6400/10477, Loss: 1.8663\n",
      "  Batch 6450/10477, Loss: 2.2753\n",
      "  Batch 6500/10477, Loss: 2.1789\n",
      "  Batch 6550/10477, Loss: 2.2891\n",
      "  Batch 6600/10477, Loss: 1.8962\n",
      "  Batch 6650/10477, Loss: 2.4305\n",
      "  Batch 6700/10477, Loss: 2.3585\n",
      "  Batch 6750/10477, Loss: 1.6315\n",
      "  Batch 6800/10477, Loss: 1.8071\n",
      "  Batch 6850/10477, Loss: 1.9978\n",
      "  Batch 6900/10477, Loss: 2.5645\n",
      "  Batch 6950/10477, Loss: 2.3880\n",
      "  Batch 7000/10477, Loss: 2.1751\n",
      "  Batch 7050/10477, Loss: 2.5171\n",
      "  Batch 7100/10477, Loss: 2.3043\n",
      "  Batch 7150/10477, Loss: 2.3375\n",
      "  Batch 7200/10477, Loss: 2.2487\n",
      "  Batch 7250/10477, Loss: 2.0312\n",
      "  Batch 7300/10477, Loss: 1.9903\n",
      "  Batch 7350/10477, Loss: 2.1545\n",
      "  Batch 7400/10477, Loss: 2.3522\n",
      "  Batch 7450/10477, Loss: 2.6078\n",
      "  Batch 7500/10477, Loss: 2.1987\n",
      "  Batch 7550/10477, Loss: 1.9696\n",
      "  Batch 7600/10477, Loss: 1.8557\n",
      "  Batch 7650/10477, Loss: 2.1078\n",
      "  Batch 7700/10477, Loss: 2.6393\n",
      "  Batch 7750/10477, Loss: 2.1318\n",
      "  Batch 7800/10477, Loss: 2.6967\n",
      "  Batch 7850/10477, Loss: 1.8107\n",
      "  Batch 7900/10477, Loss: 1.8039\n",
      "  Batch 7950/10477, Loss: 2.2740\n",
      "  Batch 8000/10477, Loss: 2.4729\n",
      "  Batch 8050/10477, Loss: 2.1525\n",
      "  Batch 8100/10477, Loss: 2.3300\n",
      "  Batch 8150/10477, Loss: 2.5396\n",
      "  Batch 8200/10477, Loss: 1.9515\n",
      "  Batch 8250/10477, Loss: 2.5686\n",
      "  Batch 8300/10477, Loss: 1.9864\n",
      "  Batch 8350/10477, Loss: 2.1472\n",
      "  Batch 8400/10477, Loss: 2.4181\n",
      "  Batch 8450/10477, Loss: 2.2365\n",
      "  Batch 8500/10477, Loss: 2.6127\n",
      "  Batch 8550/10477, Loss: 1.8639\n",
      "  Batch 8600/10477, Loss: 2.0808\n",
      "  Batch 8650/10477, Loss: 2.2218\n",
      "  Batch 8700/10477, Loss: 1.6769\n",
      "  Batch 8750/10477, Loss: 2.5685\n",
      "  Batch 8800/10477, Loss: 2.6171\n",
      "  Batch 8850/10477, Loss: 2.3106\n",
      "  Batch 8900/10477, Loss: 2.6955\n",
      "  Batch 8950/10477, Loss: 2.1463\n",
      "  Batch 9000/10477, Loss: 1.8666\n",
      "  Batch 9050/10477, Loss: 2.1846\n",
      "  Batch 9100/10477, Loss: 1.9188\n",
      "  Batch 9150/10477, Loss: 2.1902\n",
      "  Batch 9200/10477, Loss: 2.2272\n",
      "  Batch 9250/10477, Loss: 1.7258\n",
      "  Batch 9300/10477, Loss: 2.1040\n",
      "  Batch 9350/10477, Loss: 2.4809\n",
      "  Batch 9400/10477, Loss: 2.1166\n",
      "  Batch 9450/10477, Loss: 2.7673\n",
      "  Batch 9500/10477, Loss: 2.1765\n",
      "  Batch 9550/10477, Loss: 2.2087\n",
      "  Batch 9600/10477, Loss: 2.6787\n",
      "  Batch 9650/10477, Loss: 2.1874\n",
      "  Batch 9700/10477, Loss: 2.1532\n",
      "  Batch 9750/10477, Loss: 1.8648\n",
      "  Batch 9800/10477, Loss: 2.2970\n",
      "  Batch 9850/10477, Loss: 2.7658\n",
      "  Batch 9900/10477, Loss: 2.3463\n",
      "  Batch 9950/10477, Loss: 1.9181\n",
      "  Batch 10000/10477, Loss: 2.6540\n",
      "  Batch 10050/10477, Loss: 2.1794\n",
      "  Batch 10100/10477, Loss: 2.4077\n",
      "  Batch 10150/10477, Loss: 2.2563\n",
      "  Batch 10200/10477, Loss: 2.7681\n",
      "  Batch 10250/10477, Loss: 2.5927\n",
      "  Batch 10300/10477, Loss: 2.3473\n",
      "  Batch 10350/10477, Loss: 2.2868\n",
      "  Batch 10400/10477, Loss: 2.3262\n",
      "  Batch 10450/10477, Loss: 2.1037\n",
      "  Val Batch 0/366, Val Loss: 3.1162\n",
      "  Val Batch 25/366, Val Loss: 3.1157\n",
      "  Val Batch 50/366, Val Loss: 3.4018\n",
      "  Val Batch 75/366, Val Loss: 2.7588\n",
      "  Val Batch 100/366, Val Loss: 2.9677\n",
      "  Val Batch 125/366, Val Loss: 2.9513\n",
      "  Val Batch 150/366, Val Loss: 2.9295\n",
      "  Val Batch 175/366, Val Loss: 2.8967\n",
      "  Val Batch 200/366, Val Loss: 3.1018\n",
      "  Val Batch 225/366, Val Loss: 3.3074\n",
      "  Val Batch 250/366, Val Loss: 2.8618\n",
      "  Val Batch 275/366, Val Loss: 3.1029\n",
      "  Val Batch 300/366, Val Loss: 3.3120\n",
      "  Val Batch 325/366, Val Loss: 2.9872\n",
      "  Val Batch 350/366, Val Loss: 3.0217\n",
      "\n",
      "Epoch 6/10:\n",
      "  Train Loss: 2.2061 | Train Acc: 56.65%\n",
      "  Val Loss: 2.9978 | Val Acc: 48.22%\n",
      "  -> New best model saved with val_loss: 2.9978\n",
      "  Batch 0/10477, Loss: 2.8873\n",
      "  Batch 50/10477, Loss: 2.1219\n",
      "  Batch 100/10477, Loss: 2.1635\n",
      "  Batch 150/10477, Loss: 2.2024\n",
      "  Batch 200/10477, Loss: 2.5948\n",
      "  Batch 250/10477, Loss: 2.0158\n",
      "  Batch 300/10477, Loss: 1.8153\n",
      "  Batch 350/10477, Loss: 2.1231\n",
      "  Batch 400/10477, Loss: 2.3969\n",
      "  Batch 450/10477, Loss: 2.1400\n",
      "  Batch 500/10477, Loss: 2.0161\n",
      "  Batch 550/10477, Loss: 2.4811\n",
      "  Batch 600/10477, Loss: 1.7527\n",
      "  Batch 650/10477, Loss: 2.0822\n",
      "  Batch 700/10477, Loss: 2.3041\n",
      "  Batch 750/10477, Loss: 2.2664\n",
      "  Batch 800/10477, Loss: 2.0515\n",
      "  Batch 850/10477, Loss: 1.8694\n",
      "  Batch 900/10477, Loss: 2.1435\n",
      "  Batch 950/10477, Loss: 1.8545\n",
      "  Batch 1000/10477, Loss: 1.9973\n",
      "  Batch 1050/10477, Loss: 2.5038\n",
      "  Batch 1100/10477, Loss: 1.4581\n",
      "  Batch 1150/10477, Loss: 2.2098\n",
      "  Batch 1200/10477, Loss: 1.8924\n",
      "  Batch 1250/10477, Loss: 2.0974\n",
      "  Batch 1300/10477, Loss: 1.9539\n",
      "  Batch 1350/10477, Loss: 1.9885\n",
      "  Batch 1400/10477, Loss: 2.0008\n",
      "  Batch 1450/10477, Loss: 1.9764\n",
      "  Batch 1500/10477, Loss: 1.7671\n",
      "  Batch 1550/10477, Loss: 2.2936\n",
      "  Batch 1600/10477, Loss: 2.2339\n",
      "  Batch 1650/10477, Loss: 2.1314\n",
      "  Batch 1700/10477, Loss: 2.3244\n",
      "  Batch 1750/10477, Loss: 1.9332\n",
      "  Batch 1800/10477, Loss: 2.5764\n",
      "  Batch 1850/10477, Loss: 2.1780\n",
      "  Batch 1900/10477, Loss: 1.9610\n",
      "  Batch 1950/10477, Loss: 2.0805\n",
      "  Batch 2000/10477, Loss: 2.1859\n",
      "  Batch 2050/10477, Loss: 2.4697\n",
      "  Batch 2100/10477, Loss: 2.2296\n",
      "  Batch 2150/10477, Loss: 1.9058\n",
      "  Batch 2200/10477, Loss: 2.1462\n",
      "  Batch 2250/10477, Loss: 2.8674\n",
      "  Batch 2300/10477, Loss: 1.8936\n",
      "  Batch 2350/10477, Loss: 2.0077\n",
      "  Batch 2400/10477, Loss: 1.6468\n",
      "  Batch 2450/10477, Loss: 2.1291\n",
      "  Batch 2500/10477, Loss: 2.2906\n",
      "  Batch 2550/10477, Loss: 2.0155\n",
      "  Batch 2600/10477, Loss: 2.0986\n",
      "  Batch 2650/10477, Loss: 2.0878\n",
      "  Batch 2700/10477, Loss: 1.9728\n",
      "  Batch 2750/10477, Loss: 2.0896\n",
      "  Batch 2800/10477, Loss: 1.6996\n",
      "  Batch 2850/10477, Loss: 2.4150\n",
      "  Batch 2900/10477, Loss: 2.0099\n",
      "  Batch 2950/10477, Loss: 2.0611\n",
      "  Batch 3000/10477, Loss: 1.9095\n",
      "  Batch 3050/10477, Loss: 2.0031\n",
      "  Batch 3100/10477, Loss: 2.5744\n",
      "  Batch 3150/10477, Loss: 2.2999\n",
      "  Batch 3200/10477, Loss: 2.0114\n",
      "  Batch 3250/10477, Loss: 2.0467\n",
      "  Batch 3300/10477, Loss: 2.2699\n",
      "  Batch 3350/10477, Loss: 1.9890\n",
      "  Batch 3400/10477, Loss: 2.3797\n",
      "  Batch 3450/10477, Loss: 2.2584\n",
      "  Batch 3500/10477, Loss: 2.7157\n",
      "  Batch 3550/10477, Loss: 2.1343\n",
      "  Batch 3600/10477, Loss: 2.1215\n",
      "  Batch 3650/10477, Loss: 2.3447\n",
      "  Batch 3700/10477, Loss: 2.4566\n",
      "  Batch 3750/10477, Loss: 1.9746\n",
      "  Batch 3800/10477, Loss: 2.0596\n",
      "  Batch 3850/10477, Loss: 2.6035\n",
      "  Batch 3900/10477, Loss: 1.7453\n",
      "  Batch 3950/10477, Loss: 2.6112\n",
      "  Batch 4000/10477, Loss: 2.0401\n",
      "  Batch 4050/10477, Loss: 2.1341\n",
      "  Batch 4100/10477, Loss: 1.8467\n",
      "  Batch 4150/10477, Loss: 2.1399\n",
      "  Batch 4200/10477, Loss: 2.4389\n",
      "  Batch 4250/10477, Loss: 2.5219\n",
      "  Batch 4300/10477, Loss: 2.1336\n",
      "  Batch 4350/10477, Loss: 2.0321\n",
      "  Batch 4400/10477, Loss: 2.2686\n",
      "  Batch 4450/10477, Loss: 2.7625\n",
      "  Batch 4500/10477, Loss: 1.9142\n",
      "  Batch 4550/10477, Loss: 2.8008\n",
      "  Batch 4600/10477, Loss: 2.1714\n",
      "  Batch 4650/10477, Loss: 2.4498\n",
      "  Batch 4700/10477, Loss: 2.0522\n",
      "  Batch 4750/10477, Loss: 2.4764\n",
      "  Batch 4800/10477, Loss: 1.9725\n",
      "  Batch 4850/10477, Loss: 1.7712\n",
      "  Batch 4900/10477, Loss: 2.5896\n",
      "  Batch 4950/10477, Loss: 2.2067\n",
      "  Batch 5000/10477, Loss: 2.3935\n",
      "  Batch 5050/10477, Loss: 1.8396\n",
      "  Batch 5100/10477, Loss: 2.2953\n",
      "  Batch 5150/10477, Loss: 1.8432\n",
      "  Batch 5200/10477, Loss: 2.2074\n",
      "  Batch 5250/10477, Loss: 2.5813\n",
      "  Batch 5300/10477, Loss: 1.9446\n",
      "  Batch 5350/10477, Loss: 1.8744\n",
      "  Batch 5400/10477, Loss: 2.3908\n",
      "  Batch 5450/10477, Loss: 2.3263\n",
      "  Batch 5500/10477, Loss: 2.9317\n",
      "  Batch 5550/10477, Loss: 1.9911\n",
      "  Batch 5600/10477, Loss: 2.0253\n",
      "  Batch 5650/10477, Loss: 1.5892\n",
      "  Batch 5700/10477, Loss: 2.1777\n",
      "  Batch 5750/10477, Loss: 2.1597\n",
      "  Batch 5800/10477, Loss: 2.2353\n",
      "  Batch 5850/10477, Loss: 2.2078\n",
      "  Batch 5900/10477, Loss: 2.0279\n",
      "  Batch 5950/10477, Loss: 1.9866\n",
      "  Batch 6000/10477, Loss: 2.0717\n",
      "  Batch 6050/10477, Loss: 2.5355\n",
      "  Batch 6100/10477, Loss: 2.4432\n",
      "  Batch 6150/10477, Loss: 2.3920\n",
      "  Batch 6200/10477, Loss: 1.9959\n",
      "  Batch 6250/10477, Loss: 2.2900\n",
      "  Batch 6300/10477, Loss: 2.8491\n",
      "  Batch 6350/10477, Loss: 2.2901\n",
      "  Batch 6400/10477, Loss: 2.0991\n",
      "  Batch 6450/10477, Loss: 2.4407\n",
      "  Batch 6500/10477, Loss: 2.2929\n",
      "  Batch 6550/10477, Loss: 2.0969\n",
      "  Batch 6600/10477, Loss: 2.1886\n",
      "  Batch 6650/10477, Loss: 1.7507\n",
      "  Batch 6700/10477, Loss: 2.0089\n",
      "  Batch 6750/10477, Loss: 2.2724\n",
      "  Batch 6800/10477, Loss: 1.8678\n",
      "  Batch 6850/10477, Loss: 2.4315\n",
      "  Batch 6900/10477, Loss: 2.1760\n",
      "  Batch 6950/10477, Loss: 2.4350\n",
      "  Batch 7000/10477, Loss: 2.1088\n",
      "  Batch 7050/10477, Loss: 2.2665\n",
      "  Batch 7100/10477, Loss: 1.6439\n",
      "  Batch 7150/10477, Loss: 2.2372\n",
      "  Batch 7200/10477, Loss: 1.7102\n",
      "  Batch 7250/10477, Loss: 2.6029\n",
      "  Batch 7300/10477, Loss: 2.1864\n",
      "  Batch 7350/10477, Loss: 2.1539\n",
      "  Batch 7400/10477, Loss: 1.8095\n",
      "  Batch 7450/10477, Loss: 2.0217\n",
      "  Batch 7500/10477, Loss: 1.9779\n",
      "  Batch 7550/10477, Loss: 2.0054\n",
      "  Batch 7600/10477, Loss: 2.2516\n",
      "  Batch 7650/10477, Loss: 2.1852\n",
      "  Batch 7700/10477, Loss: 2.5883\n",
      "  Batch 7750/10477, Loss: 1.9887\n",
      "  Batch 7800/10477, Loss: 2.2599\n",
      "  Batch 7850/10477, Loss: 2.5537\n",
      "  Batch 7900/10477, Loss: 2.2813\n",
      "  Batch 7950/10477, Loss: 1.9583\n",
      "  Batch 8000/10477, Loss: 1.9200\n",
      "  Batch 8050/10477, Loss: 2.2165\n",
      "  Batch 8100/10477, Loss: 2.0248\n",
      "  Batch 8150/10477, Loss: 2.2932\n",
      "  Batch 8200/10477, Loss: 2.3054\n",
      "  Batch 8250/10477, Loss: 1.8543\n",
      "  Batch 8300/10477, Loss: 2.9394\n",
      "  Batch 8350/10477, Loss: 2.1523\n",
      "  Batch 8400/10477, Loss: 2.2763\n",
      "  Batch 8450/10477, Loss: 2.0580\n",
      "  Batch 8500/10477, Loss: 2.2691\n",
      "  Batch 8550/10477, Loss: 2.1566\n",
      "  Batch 8600/10477, Loss: 1.9469\n",
      "  Batch 8650/10477, Loss: 2.6650\n",
      "  Batch 8700/10477, Loss: 1.8777\n",
      "  Batch 8750/10477, Loss: 2.3599\n",
      "  Batch 8800/10477, Loss: 1.5614\n",
      "  Batch 8850/10477, Loss: 2.0137\n",
      "  Batch 8900/10477, Loss: 2.2119\n",
      "  Batch 8950/10477, Loss: 2.8029\n",
      "  Batch 9000/10477, Loss: 2.1106\n",
      "  Batch 9050/10477, Loss: 1.6677\n",
      "  Batch 9100/10477, Loss: 2.1377\n",
      "  Batch 9150/10477, Loss: 2.3674\n",
      "  Batch 9200/10477, Loss: 2.1462\n",
      "  Batch 9250/10477, Loss: 2.1961\n",
      "  Batch 9300/10477, Loss: 2.5666\n",
      "  Batch 9350/10477, Loss: 2.7078\n",
      "  Batch 9400/10477, Loss: 2.1952\n",
      "  Batch 9450/10477, Loss: 2.1794\n",
      "  Batch 9500/10477, Loss: 2.2766\n",
      "  Batch 9550/10477, Loss: 2.1601\n",
      "  Batch 9600/10477, Loss: 2.0102\n",
      "  Batch 9650/10477, Loss: 2.1246\n",
      "  Batch 9700/10477, Loss: 2.2626\n",
      "  Batch 9750/10477, Loss: 2.4615\n",
      "  Batch 9800/10477, Loss: 2.2378\n",
      "  Batch 9850/10477, Loss: 2.8797\n",
      "  Batch 9900/10477, Loss: 1.6746\n",
      "  Batch 9950/10477, Loss: 1.8912\n",
      "  Batch 10000/10477, Loss: 2.4926\n",
      "  Batch 10050/10477, Loss: 2.2449\n",
      "  Batch 10100/10477, Loss: 1.8073\n",
      "  Batch 10150/10477, Loss: 2.1523\n",
      "  Batch 10200/10477, Loss: 2.9135\n",
      "  Batch 10250/10477, Loss: 1.9328\n",
      "  Batch 10300/10477, Loss: 2.3180\n",
      "  Batch 10350/10477, Loss: 1.7615\n",
      "  Batch 10400/10477, Loss: 2.6068\n",
      "  Batch 10450/10477, Loss: 2.5570\n",
      "  Val Batch 0/366, Val Loss: 3.1635\n",
      "  Val Batch 25/366, Val Loss: 3.0449\n",
      "  Val Batch 50/366, Val Loss: 3.3134\n",
      "  Val Batch 75/366, Val Loss: 2.7981\n",
      "  Val Batch 100/366, Val Loss: 2.9866\n",
      "  Val Batch 125/366, Val Loss: 2.9510\n",
      "  Val Batch 150/366, Val Loss: 2.8866\n",
      "  Val Batch 175/366, Val Loss: 2.8438\n",
      "  Val Batch 200/366, Val Loss: 3.0950\n",
      "  Val Batch 225/366, Val Loss: 3.3313\n",
      "  Val Batch 250/366, Val Loss: 2.8165\n",
      "  Val Batch 275/366, Val Loss: 3.1039\n",
      "  Val Batch 300/366, Val Loss: 3.3230\n",
      "  Val Batch 325/366, Val Loss: 3.0214\n",
      "  Val Batch 350/366, Val Loss: 2.9461\n",
      "\n",
      "Epoch 7/10:\n",
      "  Train Loss: 2.1684 | Train Acc: 57.19%\n",
      "  Val Loss: 2.9798 | Val Acc: 48.02%\n",
      "  -> New best model saved with val_loss: 2.9798\n",
      "  Batch 0/10477, Loss: 2.1105\n",
      "  Batch 50/10477, Loss: 2.2978\n",
      "  Batch 100/10477, Loss: 1.7470\n",
      "  Batch 150/10477, Loss: 2.0863\n",
      "  Batch 200/10477, Loss: 1.5958\n",
      "  Batch 250/10477, Loss: 2.0710\n",
      "  Batch 300/10477, Loss: 2.2254\n",
      "  Batch 350/10477, Loss: 1.7465\n",
      "  Batch 400/10477, Loss: 2.2463\n",
      "  Batch 450/10477, Loss: 1.8018\n",
      "  Batch 500/10477, Loss: 2.5142\n",
      "  Batch 550/10477, Loss: 2.0390\n",
      "  Batch 600/10477, Loss: 1.9524\n",
      "  Batch 650/10477, Loss: 2.4594\n",
      "  Batch 700/10477, Loss: 2.0692\n",
      "  Batch 750/10477, Loss: 2.0418\n",
      "  Batch 800/10477, Loss: 2.2816\n",
      "  Batch 850/10477, Loss: 1.7165\n",
      "  Batch 900/10477, Loss: 1.9988\n",
      "  Batch 950/10477, Loss: 2.3351\n",
      "  Batch 1000/10477, Loss: 2.0165\n",
      "  Batch 1050/10477, Loss: 2.1290\n",
      "  Batch 1100/10477, Loss: 2.1969\n",
      "  Batch 1150/10477, Loss: 2.1383\n",
      "  Batch 1200/10477, Loss: 2.4401\n",
      "  Batch 1250/10477, Loss: 2.0090\n",
      "  Batch 1300/10477, Loss: 2.4407\n",
      "  Batch 1350/10477, Loss: 1.8488\n",
      "  Batch 1400/10477, Loss: 2.1230\n",
      "  Batch 1450/10477, Loss: 2.0683\n",
      "  Batch 1500/10477, Loss: 2.0157\n",
      "  Batch 1550/10477, Loss: 2.4273\n",
      "  Batch 1600/10477, Loss: 2.2887\n",
      "  Batch 1650/10477, Loss: 2.0436\n",
      "  Batch 1700/10477, Loss: 1.8256\n",
      "  Batch 1750/10477, Loss: 2.3393\n",
      "  Batch 1800/10477, Loss: 1.9805\n",
      "  Batch 1850/10477, Loss: 1.8382\n",
      "  Batch 1900/10477, Loss: 2.0833\n",
      "  Batch 1950/10477, Loss: 2.1615\n",
      "  Batch 2000/10477, Loss: 1.9603\n",
      "  Batch 2050/10477, Loss: 2.5909\n",
      "  Batch 2100/10477, Loss: 1.8857\n",
      "  Batch 2150/10477, Loss: 2.3140\n",
      "  Batch 2200/10477, Loss: 2.0109\n",
      "  Batch 2250/10477, Loss: 1.9785\n",
      "  Batch 2300/10477, Loss: 1.7856\n",
      "  Batch 2350/10477, Loss: 2.3098\n",
      "  Batch 2400/10477, Loss: 2.0354\n",
      "  Batch 2450/10477, Loss: 2.2280\n",
      "  Batch 2500/10477, Loss: 2.3290\n",
      "  Batch 2550/10477, Loss: 2.1838\n",
      "  Batch 2600/10477, Loss: 1.8339\n",
      "  Batch 2650/10477, Loss: 1.9787\n",
      "  Batch 2700/10477, Loss: 1.6424\n",
      "  Batch 2750/10477, Loss: 2.4568\n",
      "  Batch 2800/10477, Loss: 1.7586\n",
      "  Batch 2850/10477, Loss: 1.8224\n",
      "  Batch 2900/10477, Loss: 1.8812\n",
      "  Batch 2950/10477, Loss: 1.9529\n",
      "  Batch 3000/10477, Loss: 1.4969\n",
      "  Batch 3050/10477, Loss: 2.0452\n",
      "  Batch 3100/10477, Loss: 1.5612\n",
      "  Batch 3150/10477, Loss: 2.1580\n",
      "  Batch 3200/10477, Loss: 2.0166\n",
      "  Batch 3250/10477, Loss: 2.0311\n",
      "  Batch 3300/10477, Loss: 2.4043\n",
      "  Batch 3350/10477, Loss: 1.9792\n",
      "  Batch 3400/10477, Loss: 2.1252\n",
      "  Batch 3450/10477, Loss: 2.0415\n",
      "  Batch 3500/10477, Loss: 2.4715\n",
      "  Batch 3550/10477, Loss: 1.8197\n",
      "  Batch 3600/10477, Loss: 2.3994\n",
      "  Batch 3650/10477, Loss: 2.0816\n",
      "  Batch 3700/10477, Loss: 2.3163\n",
      "  Batch 3750/10477, Loss: 2.6156\n",
      "  Batch 3800/10477, Loss: 2.0208\n",
      "  Batch 3850/10477, Loss: 2.0149\n",
      "  Batch 3900/10477, Loss: 2.0510\n",
      "  Batch 3950/10477, Loss: 2.1997\n",
      "  Batch 4000/10477, Loss: 2.2458\n",
      "  Batch 4050/10477, Loss: 2.4051\n",
      "  Batch 4100/10477, Loss: 2.1820\n",
      "  Batch 4150/10477, Loss: 2.2446\n",
      "  Batch 4200/10477, Loss: 2.2769\n",
      "  Batch 4250/10477, Loss: 2.2447\n",
      "  Batch 4300/10477, Loss: 1.9847\n",
      "  Batch 4350/10477, Loss: 2.2985\n",
      "  Batch 4400/10477, Loss: 2.3348\n",
      "  Batch 4450/10477, Loss: 2.1788\n",
      "  Batch 4500/10477, Loss: 2.0841\n",
      "  Batch 4550/10477, Loss: 2.1302\n",
      "  Batch 4600/10477, Loss: 1.7718\n",
      "  Batch 4650/10477, Loss: 1.9718\n",
      "  Batch 4700/10477, Loss: 2.2159\n",
      "  Batch 4750/10477, Loss: 2.2986\n",
      "  Batch 4800/10477, Loss: 2.0576\n",
      "  Batch 4850/10477, Loss: 1.7915\n",
      "  Batch 4900/10477, Loss: 2.0468\n",
      "  Batch 4950/10477, Loss: 2.2210\n",
      "  Batch 5000/10477, Loss: 2.1086\n",
      "  Batch 5050/10477, Loss: 2.3632\n",
      "  Batch 5100/10477, Loss: 2.4383\n",
      "  Batch 5150/10477, Loss: 1.9829\n",
      "  Batch 5200/10477, Loss: 2.4475\n",
      "  Batch 5250/10477, Loss: 1.9130\n",
      "  Batch 5300/10477, Loss: 2.1483\n",
      "  Batch 5350/10477, Loss: 2.5267\n",
      "  Batch 5400/10477, Loss: 2.1304\n",
      "  Batch 5450/10477, Loss: 2.0343\n",
      "  Batch 5500/10477, Loss: 1.8661\n",
      "  Batch 5550/10477, Loss: 1.9404\n",
      "  Batch 5600/10477, Loss: 1.9051\n",
      "  Batch 5650/10477, Loss: 2.0273\n",
      "  Batch 5700/10477, Loss: 1.8422\n",
      "  Batch 5750/10477, Loss: 1.9676\n",
      "  Batch 5800/10477, Loss: 2.7303\n",
      "  Batch 5850/10477, Loss: 2.4975\n",
      "  Batch 5900/10477, Loss: 2.1940\n",
      "  Batch 5950/10477, Loss: 1.9380\n",
      "  Batch 6000/10477, Loss: 1.9620\n",
      "  Batch 6050/10477, Loss: 2.2399\n",
      "  Batch 6100/10477, Loss: 2.0301\n",
      "  Batch 6150/10477, Loss: 2.2630\n",
      "  Batch 6200/10477, Loss: 2.2540\n",
      "  Batch 6250/10477, Loss: 2.1276\n",
      "  Batch 6300/10477, Loss: 2.2301\n",
      "  Batch 6350/10477, Loss: 2.6296\n",
      "  Batch 6400/10477, Loss: 2.3636\n",
      "  Batch 6450/10477, Loss: 2.4517\n",
      "  Batch 6500/10477, Loss: 2.7912\n",
      "  Batch 6550/10477, Loss: 1.8774\n",
      "  Batch 6600/10477, Loss: 2.1038\n",
      "  Batch 6650/10477, Loss: 1.9707\n",
      "  Batch 6700/10477, Loss: 2.2113\n",
      "  Batch 6750/10477, Loss: 1.8939\n",
      "  Batch 6800/10477, Loss: 2.0279\n",
      "  Batch 6850/10477, Loss: 2.3016\n",
      "  Batch 6900/10477, Loss: 2.0916\n",
      "  Batch 6950/10477, Loss: 2.1981\n",
      "  Batch 7000/10477, Loss: 2.0602\n",
      "  Batch 7050/10477, Loss: 2.0024\n",
      "  Batch 7100/10477, Loss: 2.1075\n",
      "  Batch 7150/10477, Loss: 2.1360\n",
      "  Batch 7200/10477, Loss: 2.0547\n",
      "  Batch 7250/10477, Loss: 2.6565\n",
      "  Batch 7300/10477, Loss: 2.5203\n",
      "  Batch 7350/10477, Loss: 2.0212\n",
      "  Batch 7400/10477, Loss: 2.6185\n",
      "  Batch 7450/10477, Loss: 1.8216\n",
      "  Batch 7500/10477, Loss: 2.6017\n",
      "  Batch 7550/10477, Loss: 2.0339\n",
      "  Batch 7600/10477, Loss: 2.1097\n",
      "  Batch 7650/10477, Loss: 2.0230\n",
      "  Batch 7700/10477, Loss: 2.1027\n",
      "  Batch 7750/10477, Loss: 2.2735\n",
      "  Batch 7800/10477, Loss: 2.2619\n",
      "  Batch 7850/10477, Loss: 2.2446\n",
      "  Batch 7900/10477, Loss: 2.1384\n",
      "  Batch 7950/10477, Loss: 2.1193\n",
      "  Batch 8000/10477, Loss: 1.9849\n",
      "  Batch 8050/10477, Loss: 2.4030\n",
      "  Batch 8100/10477, Loss: 1.7335\n",
      "  Batch 8150/10477, Loss: 1.9288\n",
      "  Batch 8200/10477, Loss: 2.2289\n",
      "  Batch 8250/10477, Loss: 1.8342\n",
      "  Batch 8300/10477, Loss: 1.7942\n",
      "  Batch 8350/10477, Loss: 2.0780\n",
      "  Batch 8400/10477, Loss: 2.1292\n",
      "  Batch 8450/10477, Loss: 2.3806\n",
      "  Batch 8500/10477, Loss: 2.4960\n",
      "  Batch 8550/10477, Loss: 2.0188\n",
      "  Batch 8600/10477, Loss: 2.4117\n",
      "  Batch 8650/10477, Loss: 1.8172\n",
      "  Batch 8700/10477, Loss: 1.9559\n",
      "  Batch 8750/10477, Loss: 2.3168\n",
      "  Batch 8800/10477, Loss: 2.2866\n",
      "  Batch 8850/10477, Loss: 2.1746\n",
      "  Batch 8900/10477, Loss: 2.3671\n",
      "  Batch 8950/10477, Loss: 2.5782\n",
      "  Batch 9000/10477, Loss: 2.3973\n",
      "  Batch 9050/10477, Loss: 2.3732\n",
      "  Batch 9100/10477, Loss: 2.4116\n",
      "  Batch 9150/10477, Loss: 2.6160\n",
      "  Batch 9200/10477, Loss: 1.9878\n",
      "  Batch 9250/10477, Loss: 2.1619\n",
      "  Batch 9300/10477, Loss: 1.7034\n",
      "  Batch 9350/10477, Loss: 2.1025\n",
      "  Batch 9400/10477, Loss: 1.8201\n",
      "  Batch 9450/10477, Loss: 2.2208\n",
      "  Batch 9500/10477, Loss: 2.3231\n",
      "  Batch 9550/10477, Loss: 2.1299\n",
      "  Batch 9600/10477, Loss: 2.3656\n",
      "  Batch 9650/10477, Loss: 1.6527\n",
      "  Batch 9700/10477, Loss: 2.1128\n",
      "  Batch 9750/10477, Loss: 1.7683\n",
      "  Batch 9800/10477, Loss: 2.1372\n",
      "  Batch 9850/10477, Loss: 2.1366\n",
      "  Batch 9900/10477, Loss: 2.5734\n",
      "  Batch 9950/10477, Loss: 2.2515\n",
      "  Batch 10000/10477, Loss: 1.7507\n",
      "  Batch 10050/10477, Loss: 1.8625\n",
      "  Batch 10100/10477, Loss: 2.2891\n",
      "  Batch 10150/10477, Loss: 1.8168\n",
      "  Batch 10200/10477, Loss: 2.2166\n",
      "  Batch 10250/10477, Loss: 1.5690\n",
      "  Batch 10300/10477, Loss: 2.6465\n",
      "  Batch 10350/10477, Loss: 2.1994\n",
      "  Batch 10400/10477, Loss: 1.9092\n",
      "  Batch 10450/10477, Loss: 2.4779\n",
      "  Val Batch 0/366, Val Loss: 3.1223\n",
      "  Val Batch 25/366, Val Loss: 3.0211\n",
      "  Val Batch 50/366, Val Loss: 3.2370\n",
      "  Val Batch 75/366, Val Loss: 2.7681\n",
      "  Val Batch 100/366, Val Loss: 2.8991\n",
      "  Val Batch 125/366, Val Loss: 2.8854\n",
      "  Val Batch 150/366, Val Loss: 2.8563\n",
      "  Val Batch 175/366, Val Loss: 2.9247\n",
      "  Val Batch 200/366, Val Loss: 3.0892\n",
      "  Val Batch 225/366, Val Loss: 3.2509\n",
      "  Val Batch 250/366, Val Loss: 2.8098\n",
      "  Val Batch 275/366, Val Loss: 3.1578\n",
      "  Val Batch 300/366, Val Loss: 3.3507\n",
      "  Val Batch 325/366, Val Loss: 3.0492\n",
      "  Val Batch 350/366, Val Loss: 2.9686\n",
      "\n",
      "Epoch 8/10:\n",
      "  Train Loss: 2.1356 | Train Acc: 57.66%\n",
      "  Val Loss: 2.9672 | Val Acc: 48.40%\n",
      "  -> New best model saved with val_loss: 2.9672\n",
      "  Batch 0/10477, Loss: 2.2636\n",
      "  Batch 50/10477, Loss: 2.0046\n",
      "  Batch 100/10477, Loss: 2.2616\n",
      "  Batch 150/10477, Loss: 2.3882\n",
      "  Batch 200/10477, Loss: 1.8273\n",
      "  Batch 250/10477, Loss: 1.9440\n",
      "  Batch 300/10477, Loss: 1.6495\n",
      "  Batch 350/10477, Loss: 2.2011\n",
      "  Batch 400/10477, Loss: 1.7856\n",
      "  Batch 450/10477, Loss: 1.8722\n",
      "  Batch 500/10477, Loss: 2.2727\n",
      "  Batch 550/10477, Loss: 2.0014\n",
      "  Batch 600/10477, Loss: 2.2266\n",
      "  Batch 650/10477, Loss: 2.0948\n",
      "  Batch 700/10477, Loss: 1.8687\n",
      "  Batch 750/10477, Loss: 2.4522\n",
      "  Batch 800/10477, Loss: 1.6173\n",
      "  Batch 850/10477, Loss: 1.9487\n",
      "  Batch 900/10477, Loss: 2.0366\n",
      "  Batch 950/10477, Loss: 1.9082\n",
      "  Batch 1000/10477, Loss: 1.8306\n",
      "  Batch 1050/10477, Loss: 1.6434\n",
      "  Batch 1100/10477, Loss: 2.3732\n",
      "  Batch 1150/10477, Loss: 2.1139\n",
      "  Batch 1200/10477, Loss: 1.6094\n",
      "  Batch 1250/10477, Loss: 1.7241\n",
      "  Batch 1300/10477, Loss: 2.0861\n",
      "  Batch 1350/10477, Loss: 2.0398\n",
      "  Batch 1400/10477, Loss: 1.9484\n",
      "  Batch 1450/10477, Loss: 1.7052\n",
      "  Batch 1500/10477, Loss: 1.8277\n",
      "  Batch 1550/10477, Loss: 1.8691\n",
      "  Batch 1600/10477, Loss: 2.0981\n",
      "  Batch 1650/10477, Loss: 2.0324\n",
      "  Batch 1700/10477, Loss: 2.3245\n",
      "  Batch 1750/10477, Loss: 2.0204\n",
      "  Batch 1800/10477, Loss: 1.8040\n",
      "  Batch 1850/10477, Loss: 1.7815\n",
      "  Batch 1900/10477, Loss: 2.1508\n",
      "  Batch 1950/10477, Loss: 2.1075\n",
      "  Batch 2000/10477, Loss: 2.4105\n",
      "  Batch 2050/10477, Loss: 2.0685\n",
      "  Batch 2100/10477, Loss: 2.2113\n",
      "  Batch 2150/10477, Loss: 2.6342\n",
      "  Batch 2200/10477, Loss: 2.0501\n",
      "  Batch 2250/10477, Loss: 1.7971\n",
      "  Batch 2300/10477, Loss: 2.3524\n",
      "  Batch 2350/10477, Loss: 2.2659\n",
      "  Batch 2400/10477, Loss: 2.2096\n",
      "  Batch 2450/10477, Loss: 1.6558\n",
      "  Batch 2500/10477, Loss: 2.0166\n",
      "  Batch 2550/10477, Loss: 2.1132\n",
      "  Batch 2600/10477, Loss: 2.1445\n",
      "  Batch 2650/10477, Loss: 2.4080\n",
      "  Batch 2700/10477, Loss: 2.1763\n",
      "  Batch 2750/10477, Loss: 1.9521\n",
      "  Batch 2800/10477, Loss: 2.0594\n",
      "  Batch 2850/10477, Loss: 2.4492\n",
      "  Batch 2900/10477, Loss: 1.8073\n",
      "  Batch 2950/10477, Loss: 2.3125\n",
      "  Batch 3000/10477, Loss: 2.0497\n",
      "  Batch 3050/10477, Loss: 2.1183\n",
      "  Batch 3100/10477, Loss: 2.6398\n",
      "  Batch 3150/10477, Loss: 1.9535\n",
      "  Batch 3200/10477, Loss: 2.1975\n",
      "  Batch 3250/10477, Loss: 2.0961\n",
      "  Batch 3300/10477, Loss: 2.0322\n",
      "  Batch 3350/10477, Loss: 1.8586\n",
      "  Batch 3400/10477, Loss: 1.9666\n",
      "  Batch 3450/10477, Loss: 2.5158\n",
      "  Batch 3500/10477, Loss: 2.7350\n",
      "  Batch 3550/10477, Loss: 1.9418\n",
      "  Batch 3600/10477, Loss: 2.2594\n",
      "  Batch 3650/10477, Loss: 2.1966\n",
      "  Batch 3700/10477, Loss: 2.1540\n",
      "  Batch 3750/10477, Loss: 2.0317\n",
      "  Batch 3800/10477, Loss: 2.3092\n",
      "  Batch 3850/10477, Loss: 2.0028\n",
      "  Batch 3900/10477, Loss: 1.9624\n",
      "  Batch 3950/10477, Loss: 2.2693\n",
      "  Batch 4000/10477, Loss: 2.6129\n",
      "  Batch 4050/10477, Loss: 2.0691\n",
      "  Batch 4100/10477, Loss: 1.6976\n",
      "  Batch 4150/10477, Loss: 2.2324\n",
      "  Batch 4200/10477, Loss: 2.0684\n",
      "  Batch 4250/10477, Loss: 2.1599\n",
      "  Batch 4300/10477, Loss: 1.9880\n",
      "  Batch 4350/10477, Loss: 1.9253\n",
      "  Batch 4400/10477, Loss: 2.1574\n",
      "  Batch 4450/10477, Loss: 1.6380\n",
      "  Batch 4500/10477, Loss: 1.6670\n",
      "  Batch 4550/10477, Loss: 2.1284\n",
      "  Batch 4600/10477, Loss: 1.9149\n",
      "  Batch 4650/10477, Loss: 2.0366\n",
      "  Batch 4700/10477, Loss: 1.5607\n",
      "  Batch 4750/10477, Loss: 2.3481\n",
      "  Batch 4800/10477, Loss: 1.8315\n",
      "  Batch 4850/10477, Loss: 1.8502\n",
      "  Batch 4900/10477, Loss: 2.3189\n",
      "  Batch 4950/10477, Loss: 1.7980\n",
      "  Batch 5000/10477, Loss: 2.1623\n",
      "  Batch 5050/10477, Loss: 2.0722\n",
      "  Batch 5100/10477, Loss: 2.1221\n",
      "  Batch 5150/10477, Loss: 2.4860\n",
      "  Batch 5200/10477, Loss: 2.6025\n",
      "  Batch 5250/10477, Loss: 2.1828\n",
      "  Batch 5300/10477, Loss: 2.0143\n",
      "  Batch 5350/10477, Loss: 2.0233\n",
      "  Batch 5400/10477, Loss: 1.9810\n",
      "  Batch 5450/10477, Loss: 2.3393\n",
      "  Batch 5500/10477, Loss: 2.4648\n",
      "  Batch 5550/10477, Loss: 2.0910\n",
      "  Batch 5600/10477, Loss: 1.9734\n",
      "  Batch 5650/10477, Loss: 2.1278\n",
      "  Batch 5700/10477, Loss: 2.0074\n",
      "  Batch 5750/10477, Loss: 1.8349\n",
      "  Batch 5800/10477, Loss: 2.0672\n",
      "  Batch 5850/10477, Loss: 2.5016\n",
      "  Batch 5900/10477, Loss: 2.7586\n",
      "  Batch 5950/10477, Loss: 2.1865\n",
      "  Batch 6000/10477, Loss: 2.1915\n",
      "  Batch 6050/10477, Loss: 2.1655\n",
      "  Batch 6100/10477, Loss: 2.5280\n",
      "  Batch 6150/10477, Loss: 2.6258\n",
      "  Batch 6200/10477, Loss: 2.0784\n",
      "  Batch 6250/10477, Loss: 2.0099\n",
      "  Batch 6300/10477, Loss: 1.8281\n",
      "  Batch 6350/10477, Loss: 2.6234\n",
      "  Batch 6400/10477, Loss: 2.2553\n",
      "  Batch 6450/10477, Loss: 2.3549\n",
      "  Batch 6500/10477, Loss: 2.3042\n",
      "  Batch 6550/10477, Loss: 1.7906\n",
      "  Batch 6600/10477, Loss: 2.1694\n",
      "  Batch 6650/10477, Loss: 1.7378\n",
      "  Batch 6700/10477, Loss: 1.6923\n",
      "  Batch 6750/10477, Loss: 2.0499\n",
      "  Batch 6800/10477, Loss: 1.8107\n",
      "  Batch 6850/10477, Loss: 2.3345\n",
      "  Batch 6900/10477, Loss: 2.3290\n",
      "  Batch 6950/10477, Loss: 1.8662\n",
      "  Batch 7000/10477, Loss: 1.7294\n",
      "  Batch 7050/10477, Loss: 2.3100\n",
      "  Batch 7100/10477, Loss: 2.2704\n",
      "  Batch 7150/10477, Loss: 2.2938\n",
      "  Batch 7200/10477, Loss: 2.0005\n",
      "  Batch 7250/10477, Loss: 2.6540\n",
      "  Batch 7300/10477, Loss: 2.0163\n",
      "  Batch 7350/10477, Loss: 2.0280\n",
      "  Batch 7400/10477, Loss: 2.4022\n",
      "  Batch 7450/10477, Loss: 2.2661\n",
      "  Batch 7500/10477, Loss: 2.0451\n",
      "  Batch 7550/10477, Loss: 1.7837\n",
      "  Batch 7600/10477, Loss: 1.9929\n",
      "  Batch 7650/10477, Loss: 2.6873\n",
      "  Batch 7700/10477, Loss: 2.5372\n",
      "  Batch 7750/10477, Loss: 2.2331\n",
      "  Batch 7800/10477, Loss: 2.4611\n",
      "  Batch 7850/10477, Loss: 2.7321\n",
      "  Batch 7900/10477, Loss: 1.6076\n",
      "  Batch 7950/10477, Loss: 2.0286\n",
      "  Batch 8000/10477, Loss: 1.8877\n",
      "  Batch 8050/10477, Loss: 1.7916\n",
      "  Batch 8100/10477, Loss: 1.7531\n",
      "  Batch 8150/10477, Loss: 1.8335\n",
      "  Batch 8200/10477, Loss: 2.3348\n",
      "  Batch 8250/10477, Loss: 2.3687\n",
      "  Batch 8300/10477, Loss: 2.2396\n",
      "  Batch 8350/10477, Loss: 2.1159\n",
      "  Batch 8400/10477, Loss: 2.1456\n",
      "  Batch 8450/10477, Loss: 2.0726\n",
      "  Batch 8500/10477, Loss: 2.0846\n",
      "  Batch 8550/10477, Loss: 2.6986\n",
      "  Batch 8600/10477, Loss: 2.4332\n",
      "  Batch 8650/10477, Loss: 2.2184\n",
      "  Batch 8700/10477, Loss: 2.0367\n",
      "  Batch 8750/10477, Loss: 1.8177\n",
      "  Batch 8800/10477, Loss: 2.3155\n",
      "  Batch 8850/10477, Loss: 1.9697\n",
      "  Batch 8900/10477, Loss: 1.9361\n",
      "  Batch 8950/10477, Loss: 2.2397\n",
      "  Batch 9000/10477, Loss: 1.9350\n",
      "  Batch 9050/10477, Loss: 2.0422\n",
      "  Batch 9100/10477, Loss: 1.8553\n",
      "  Batch 9150/10477, Loss: 2.3803\n",
      "  Batch 9200/10477, Loss: 1.6070\n",
      "  Batch 9250/10477, Loss: 2.4065\n",
      "  Batch 9300/10477, Loss: 1.9583\n",
      "  Batch 9350/10477, Loss: 2.0284\n",
      "  Batch 9400/10477, Loss: 2.2301\n",
      "  Batch 9450/10477, Loss: 2.2935\n",
      "  Batch 9500/10477, Loss: 2.2559\n",
      "  Batch 9550/10477, Loss: 2.1964\n",
      "  Batch 9600/10477, Loss: 2.1023\n",
      "  Batch 9650/10477, Loss: 2.0870\n",
      "  Batch 9700/10477, Loss: 2.6974\n",
      "  Batch 9750/10477, Loss: 2.3988\n",
      "  Batch 9800/10477, Loss: 1.8020\n",
      "  Batch 9850/10477, Loss: 2.4159\n",
      "  Batch 9900/10477, Loss: 2.1479\n",
      "  Batch 9950/10477, Loss: 1.9116\n",
      "  Batch 10000/10477, Loss: 2.2240\n",
      "  Batch 10050/10477, Loss: 1.9639\n",
      "  Batch 10100/10477, Loss: 2.1624\n",
      "  Batch 10150/10477, Loss: 1.6711\n",
      "  Batch 10200/10477, Loss: 1.6069\n",
      "  Batch 10250/10477, Loss: 2.2417\n",
      "  Batch 10300/10477, Loss: 2.4303\n",
      "  Batch 10350/10477, Loss: 2.0244\n",
      "  Batch 10400/10477, Loss: 1.6321\n",
      "  Batch 10450/10477, Loss: 2.1058\n",
      "  Val Batch 0/366, Val Loss: 3.1747\n",
      "  Val Batch 25/366, Val Loss: 3.0484\n",
      "  Val Batch 50/366, Val Loss: 3.2841\n",
      "  Val Batch 75/366, Val Loss: 2.8657\n",
      "  Val Batch 100/366, Val Loss: 2.9221\n",
      "  Val Batch 125/366, Val Loss: 2.8700\n",
      "  Val Batch 150/366, Val Loss: 2.8884\n",
      "  Val Batch 175/366, Val Loss: 2.9454\n",
      "  Val Batch 200/366, Val Loss: 3.1011\n",
      "  Val Batch 225/366, Val Loss: 3.3172\n",
      "  Val Batch 250/366, Val Loss: 2.8051\n",
      "  Val Batch 275/366, Val Loss: 3.1531\n",
      "  Val Batch 300/366, Val Loss: 3.4004\n",
      "  Val Batch 325/366, Val Loss: 3.0122\n",
      "  Val Batch 350/366, Val Loss: 2.9758\n",
      "\n",
      "Epoch 9/10:\n",
      "  Train Loss: 2.1091 | Train Acc: 58.00%\n",
      "  Val Loss: 2.9881 | Val Acc: 47.33%\n",
      "  -> Validation loss didn't improve. Patience: 1/3\n",
      "  Batch 0/10477, Loss: 1.9584\n",
      "  Batch 50/10477, Loss: 1.6636\n",
      "  Batch 100/10477, Loss: 1.7258\n",
      "  Batch 150/10477, Loss: 1.9566\n",
      "  Batch 200/10477, Loss: 2.2144\n",
      "  Batch 250/10477, Loss: 2.2590\n",
      "  Batch 300/10477, Loss: 1.8851\n",
      "  Batch 350/10477, Loss: 2.2433\n",
      "  Batch 400/10477, Loss: 1.9504\n",
      "  Batch 450/10477, Loss: 2.5507\n",
      "  Batch 500/10477, Loss: 2.1678\n",
      "  Batch 550/10477, Loss: 1.8107\n",
      "  Batch 600/10477, Loss: 2.2711\n",
      "  Batch 650/10477, Loss: 1.8711\n",
      "  Batch 700/10477, Loss: 1.8930\n",
      "  Batch 750/10477, Loss: 1.6614\n",
      "  Batch 800/10477, Loss: 2.1209\n",
      "  Batch 850/10477, Loss: 1.7429\n",
      "  Batch 900/10477, Loss: 1.8930\n",
      "  Batch 950/10477, Loss: 1.8735\n",
      "  Batch 1000/10477, Loss: 2.8681\n",
      "  Batch 1050/10477, Loss: 2.0830\n",
      "  Batch 1100/10477, Loss: 1.4544\n",
      "  Batch 1150/10477, Loss: 2.0039\n",
      "  Batch 1200/10477, Loss: 1.8869\n",
      "  Batch 1250/10477, Loss: 2.4550\n",
      "  Batch 1300/10477, Loss: 2.0590\n",
      "  Batch 1350/10477, Loss: 2.1948\n",
      "  Batch 1400/10477, Loss: 1.6245\n",
      "  Batch 1450/10477, Loss: 1.9412\n",
      "  Batch 1500/10477, Loss: 1.9574\n",
      "  Batch 1550/10477, Loss: 2.2277\n",
      "  Batch 1600/10477, Loss: 1.9040\n",
      "  Batch 1650/10477, Loss: 2.5975\n",
      "  Batch 1700/10477, Loss: 2.3651\n",
      "  Batch 1750/10477, Loss: 2.0438\n",
      "  Batch 1800/10477, Loss: 1.5205\n",
      "  Batch 1850/10477, Loss: 2.0612\n",
      "  Batch 1900/10477, Loss: 2.5565\n",
      "  Batch 1950/10477, Loss: 2.1148\n",
      "  Batch 2000/10477, Loss: 2.1786\n",
      "  Batch 2050/10477, Loss: 2.3088\n",
      "  Batch 2100/10477, Loss: 1.9726\n",
      "  Batch 2150/10477, Loss: 2.2516\n",
      "  Batch 2200/10477, Loss: 2.1888\n",
      "  Batch 2250/10477, Loss: 1.8511\n",
      "  Batch 2300/10477, Loss: 2.2516\n",
      "  Batch 2350/10477, Loss: 2.1294\n",
      "  Batch 2400/10477, Loss: 2.2238\n",
      "  Batch 2450/10477, Loss: 1.9824\n",
      "  Batch 2500/10477, Loss: 2.0831\n",
      "  Batch 2550/10477, Loss: 2.3174\n",
      "  Batch 2600/10477, Loss: 1.7065\n",
      "  Batch 2650/10477, Loss: 2.0505\n",
      "  Batch 2700/10477, Loss: 2.2347\n",
      "  Batch 2750/10477, Loss: 2.0347\n",
      "  Batch 2800/10477, Loss: 2.0603\n",
      "  Batch 2850/10477, Loss: 2.2102\n",
      "  Batch 2900/10477, Loss: 2.2107\n",
      "  Batch 2950/10477, Loss: 2.3234\n",
      "  Batch 3000/10477, Loss: 2.1130\n",
      "  Batch 3050/10477, Loss: 2.0950\n",
      "  Batch 3100/10477, Loss: 2.1713\n",
      "  Batch 3150/10477, Loss: 2.4156\n",
      "  Batch 3200/10477, Loss: 2.0567\n",
      "  Batch 3250/10477, Loss: 1.6743\n",
      "  Batch 3300/10477, Loss: 1.9914\n",
      "  Batch 3350/10477, Loss: 1.7802\n",
      "  Batch 3400/10477, Loss: 2.1752\n",
      "  Batch 3450/10477, Loss: 2.1544\n",
      "  Batch 3500/10477, Loss: 2.2292\n",
      "  Batch 3550/10477, Loss: 2.7780\n",
      "  Batch 3600/10477, Loss: 1.7666\n",
      "  Batch 3650/10477, Loss: 1.7158\n",
      "  Batch 3700/10477, Loss: 1.8990\n",
      "  Batch 3750/10477, Loss: 2.1829\n",
      "  Batch 3800/10477, Loss: 2.4844\n",
      "  Batch 3850/10477, Loss: 1.6378\n",
      "  Batch 3900/10477, Loss: 1.8818\n",
      "  Batch 3950/10477, Loss: 2.1836\n",
      "  Batch 4000/10477, Loss: 2.7089\n",
      "  Batch 4050/10477, Loss: 1.9779\n",
      "  Batch 4100/10477, Loss: 2.1171\n",
      "  Batch 4150/10477, Loss: 2.1949\n",
      "  Batch 4200/10477, Loss: 2.0203\n",
      "  Batch 4250/10477, Loss: 2.1529\n",
      "  Batch 4300/10477, Loss: 2.5592\n",
      "  Batch 4350/10477, Loss: 2.3808\n",
      "  Batch 4400/10477, Loss: 1.6379\n",
      "  Batch 4450/10477, Loss: 1.6726\n",
      "  Batch 4500/10477, Loss: 1.7800\n",
      "  Batch 4550/10477, Loss: 1.9091\n",
      "  Batch 4600/10477, Loss: 1.7227\n",
      "  Batch 4650/10477, Loss: 1.6892\n",
      "  Batch 4700/10477, Loss: 2.3201\n",
      "  Batch 4750/10477, Loss: 2.1253\n",
      "  Batch 4800/10477, Loss: 1.6572\n",
      "  Batch 4850/10477, Loss: 2.8608\n",
      "  Batch 4900/10477, Loss: 2.1735\n",
      "  Batch 4950/10477, Loss: 2.1385\n",
      "  Batch 5000/10477, Loss: 2.0162\n",
      "  Batch 5050/10477, Loss: 1.7257\n",
      "  Batch 5100/10477, Loss: 2.3913\n",
      "  Batch 5150/10477, Loss: 1.8144\n",
      "  Batch 5200/10477, Loss: 1.7027\n",
      "  Batch 5250/10477, Loss: 2.1486\n",
      "  Batch 5300/10477, Loss: 2.1046\n",
      "  Batch 5350/10477, Loss: 2.1765\n",
      "  Batch 5400/10477, Loss: 2.4122\n",
      "  Batch 5450/10477, Loss: 2.0123\n",
      "  Batch 5500/10477, Loss: 2.1741\n",
      "  Batch 5550/10477, Loss: 2.1009\n",
      "  Batch 5600/10477, Loss: 1.8929\n",
      "  Batch 5650/10477, Loss: 2.1648\n",
      "  Batch 5700/10477, Loss: 2.4656\n",
      "  Batch 5750/10477, Loss: 1.5476\n",
      "  Batch 5800/10477, Loss: 2.4361\n",
      "  Batch 5850/10477, Loss: 2.1456\n",
      "  Batch 5900/10477, Loss: 2.3330\n",
      "  Batch 5950/10477, Loss: 2.1482\n",
      "  Batch 6000/10477, Loss: 1.8393\n",
      "  Batch 6050/10477, Loss: 2.0562\n",
      "  Batch 6100/10477, Loss: 2.2347\n",
      "  Batch 6150/10477, Loss: 1.9095\n",
      "  Batch 6200/10477, Loss: 1.7073\n",
      "  Batch 6250/10477, Loss: 2.0491\n",
      "  Batch 6300/10477, Loss: 2.2537\n",
      "  Batch 6350/10477, Loss: 1.5743\n",
      "  Batch 6400/10477, Loss: 1.8767\n",
      "  Batch 6450/10477, Loss: 1.9479\n",
      "  Batch 6500/10477, Loss: 2.0446\n",
      "  Batch 6550/10477, Loss: 1.9585\n",
      "  Batch 6600/10477, Loss: 1.9314\n",
      "  Batch 6650/10477, Loss: 2.2437\n",
      "  Batch 6700/10477, Loss: 2.3489\n",
      "  Batch 6750/10477, Loss: 2.2755\n",
      "  Batch 6800/10477, Loss: 2.4711\n",
      "  Batch 6850/10477, Loss: 2.2382\n",
      "  Batch 6900/10477, Loss: 1.9696\n",
      "  Batch 6950/10477, Loss: 1.7770\n",
      "  Batch 7000/10477, Loss: 1.9045\n",
      "  Batch 7050/10477, Loss: 2.0283\n",
      "  Batch 7100/10477, Loss: 2.6482\n",
      "  Batch 7150/10477, Loss: 1.9023\n",
      "  Batch 7200/10477, Loss: 1.6466\n",
      "  Batch 7250/10477, Loss: 2.3799\n",
      "  Batch 7300/10477, Loss: 2.0588\n",
      "  Batch 7350/10477, Loss: 2.0033\n",
      "  Batch 7400/10477, Loss: 1.9305\n",
      "  Batch 7450/10477, Loss: 2.2823\n",
      "  Batch 7500/10477, Loss: 2.4295\n",
      "  Batch 7550/10477, Loss: 2.0497\n",
      "  Batch 7600/10477, Loss: 1.8234\n",
      "  Batch 7650/10477, Loss: 1.9965\n",
      "  Batch 7700/10477, Loss: 2.3927\n",
      "  Batch 7750/10477, Loss: 1.8260\n",
      "  Batch 7800/10477, Loss: 1.6502\n",
      "  Batch 7850/10477, Loss: 2.3935\n",
      "  Batch 7900/10477, Loss: 2.6184\n",
      "  Batch 7950/10477, Loss: 2.0036\n",
      "  Batch 8000/10477, Loss: 2.1622\n",
      "  Batch 8050/10477, Loss: 2.1865\n",
      "  Batch 8100/10477, Loss: 2.3210\n",
      "  Batch 8150/10477, Loss: 1.7766\n",
      "  Batch 8200/10477, Loss: 2.3321\n",
      "  Batch 8250/10477, Loss: 2.4032\n",
      "  Batch 8300/10477, Loss: 2.5619\n",
      "  Batch 8350/10477, Loss: 2.3834\n",
      "  Batch 8400/10477, Loss: 1.8699\n",
      "  Batch 8450/10477, Loss: 2.1025\n",
      "  Batch 8500/10477, Loss: 2.4305\n",
      "  Batch 8550/10477, Loss: 2.2166\n",
      "  Batch 8600/10477, Loss: 1.7652\n",
      "  Batch 8650/10477, Loss: 2.1990\n",
      "  Batch 8700/10477, Loss: 2.2681\n",
      "  Batch 8750/10477, Loss: 1.9514\n",
      "  Batch 8800/10477, Loss: 2.2493\n",
      "  Batch 8850/10477, Loss: 1.5715\n",
      "  Batch 8900/10477, Loss: 2.5191\n",
      "  Batch 8950/10477, Loss: 2.7211\n",
      "  Batch 9000/10477, Loss: 2.1752\n",
      "  Batch 9050/10477, Loss: 2.1798\n",
      "  Batch 9100/10477, Loss: 2.0197\n",
      "  Batch 9150/10477, Loss: 2.0038\n",
      "  Batch 9200/10477, Loss: 1.6987\n",
      "  Batch 9250/10477, Loss: 2.4242\n",
      "  Batch 9300/10477, Loss: 1.6760\n",
      "  Batch 9350/10477, Loss: 1.8831\n",
      "  Batch 9400/10477, Loss: 2.6176\n",
      "  Batch 9450/10477, Loss: 2.4181\n",
      "  Batch 9500/10477, Loss: 2.1251\n",
      "  Batch 9550/10477, Loss: 2.3458\n",
      "  Batch 9600/10477, Loss: 1.8865\n",
      "  Batch 9650/10477, Loss: 2.5014\n",
      "  Batch 9700/10477, Loss: 2.5156\n",
      "  Batch 9750/10477, Loss: 2.2176\n",
      "  Batch 9800/10477, Loss: 1.7780\n",
      "  Batch 9850/10477, Loss: 2.1448\n",
      "  Batch 9900/10477, Loss: 3.1399\n",
      "  Batch 9950/10477, Loss: 2.1114\n",
      "  Batch 10000/10477, Loss: 2.0752\n",
      "  Batch 10050/10477, Loss: 1.6588\n",
      "  Batch 10100/10477, Loss: 1.7721\n",
      "  Batch 10150/10477, Loss: 2.7869\n",
      "  Batch 10200/10477, Loss: 2.2564\n",
      "  Batch 10250/10477, Loss: 2.2917\n",
      "  Batch 10300/10477, Loss: 1.7882\n",
      "  Batch 10350/10477, Loss: 2.1805\n",
      "  Batch 10400/10477, Loss: 1.7929\n",
      "  Batch 10450/10477, Loss: 1.2191\n",
      "  Val Batch 0/366, Val Loss: 3.1449\n",
      "  Val Batch 25/366, Val Loss: 3.0699\n",
      "  Val Batch 50/366, Val Loss: 3.2294\n",
      "  Val Batch 75/366, Val Loss: 2.8183\n",
      "  Val Batch 100/366, Val Loss: 2.9668\n",
      "  Val Batch 125/366, Val Loss: 2.9441\n",
      "  Val Batch 150/366, Val Loss: 2.8675\n",
      "  Val Batch 175/366, Val Loss: 2.9390\n",
      "  Val Batch 200/366, Val Loss: 3.1260\n",
      "  Val Batch 225/366, Val Loss: 3.2735\n",
      "  Val Batch 250/366, Val Loss: 2.8384\n",
      "  Val Batch 275/366, Val Loss: 3.0661\n",
      "  Val Batch 300/366, Val Loss: 3.3197\n",
      "  Val Batch 325/366, Val Loss: 2.9866\n",
      "  Val Batch 350/366, Val Loss: 3.0058\n",
      "\n",
      "Epoch 10/10:\n",
      "  Train Loss: 2.0855 | Train Acc: 58.33%\n",
      "  Val Loss: 2.9710 | Val Acc: 47.97%\n",
      "  -> Validation loss didn't improve. Patience: 2/3\n",
      "\n",
      "Training completed. Best validation loss: 2.9672\n",
      "Best model saved as wandb artifact.\n",
      "\n",
      "Training complete.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td></td></tr><tr><td>train_accuracy</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>val_accuracy</td><td></td></tr><tr><td>val_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_accuracy</td><td>58.335</td></tr><tr><td>train_loss</td><td>2.0855</td></tr><tr><td>val_accuracy</td><td>47.97218</td></tr><tr><td>val_loss</td><td>2.97103</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LSTM_100000</strong> at: <a href='https://wandb.ai/kanakornmek-/predictive-keyboard-rnn-optimized/runs/8umfjnol' target=\"_blank\">https://wandb.ai/kanakornmek-/predictive-keyboard-rnn-optimized/runs/8umfjnol</a><br> View project at: <a href='https://wandb.ai/kanakornmek-/predictive-keyboard-rnn-optimized' target=\"_blank\">https://wandb.ai/kanakornmek-/predictive-keyboard-rnn-optimized</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/wandb/run-20251018_174000-8umfjnol/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 4. Configuration ---\n",
    "ZIP_FILE_PATH = 'train.zip'\n",
    "TEXT_FILE_NAME = 'train.src.tok'\n",
    "DEV_SET_PATH = 'dev_set.csv'\n",
    "SEQ_LENGTH = 20\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "N_LAYERS = 2\n",
    "\n",
    "# --- WANDB Initialization ---\n",
    "wandb.init(\n",
    "    project=\"predictive-keyboard-rnn-optimized\",\n",
    "    name=\"LSTM_100000\",\n",
    "    config={\n",
    "        \"learning_rate\": LEARNING_RATE, \"epochs\": EPOCHS, \"batch_size\": BATCH_SIZE,\n",
    "        \"seq_length\": SEQ_LENGTH, \"embedding_dim\": EMBEDDING_DIM, \"hidden_dim\": HIDDEN_DIM,\n",
    "        \"n_layers\": N_LAYERS, \"dataset\": \"train.src.tok (first 100000 lines)\",\n",
    "        \"architecture\": \"LSTM_with_first_letter\", \"optimization\": \"parallel_processing\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# --- Load Data ---\n",
    "if not os.path.exists(TEXT_FILE_NAME) or not os.path.exists(DEV_SET_PATH):\n",
    "    print(f\"Error: Make sure '{TEXT_FILE_NAME}' and '{DEV_SET_PATH}' are in the directory.\")\n",
    "else:\n",
    "    with open(TEXT_FILE_NAME) as f:\n",
    "        lines = f.read().splitlines()\n",
    "\n",
    "    if lines:\n",
    "        print(\"Limiting training data to the first 100000 lines for testing.\")\n",
    "        limited_lines = lines[:100000]\n",
    "        \n",
    "        # --- Prepare Datasets and DataLoaders ---\n",
    "        train_dataset = TextDataset(limited_lines, seq_length=SEQ_LENGTH)\n",
    "        val_dataset = DevSetDataset(DEV_SET_PATH, train_dataset.word_to_int, seq_length=SEQ_LENGTH)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE * 2, shuffle=False)\n",
    "        \n",
    "        with open('word_to_int.json', 'w') as f:\n",
    "            json.dump(train_dataset.word_to_int, f)\n",
    "        print(\"Vocabulary saved.\")\n",
    "\n",
    "        # --- Initialize Model ---\n",
    "        model = NextWordModel(train_dataset.vocab_size, EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS)\n",
    "        print(\"\\nModel Architecture:\")\n",
    "        print(model)\n",
    "        print(f\"\\nVocabulary Size: {train_dataset.vocab_size}\")\n",
    "        print(f\"Training sequences: {len(train_dataset)}\")\n",
    "        print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "        # --- Train Model ---\n",
    "        if len(train_dataset) > 0 and len(val_dataset) > 0:\n",
    "            print(\"\\nStarting optimized training...\")\n",
    "            train(model, train_loader, val_loader, train_dataset.vocab_size, \n",
    "                  epochs=EPOCHS, batch_size=BATCH_SIZE, lr=LEARNING_RATE)\n",
    "            print(\"\\nTraining complete.\")\n",
    "        else:\n",
    "            print(\"Not enough data to create training and/or validation sets.\")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Prediction and Evaluation Functions ---\n",
    "\n",
    "def load_model(model_path, vocab_size, embedding_dim=256, hidden_dim=512, n_layers=2):\n",
    "    \"\"\"\n",
    "    Load a trained model from file.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = NextWordModel(vocab_size, embedding_dim, hidden_dim, n_layers)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def predict_next_word(model, context_text, first_letter=None, word_to_int=None, int_to_word=None, seq_length=20, top_k=5):\n",
    "    \"\"\"\n",
    "    Predict the next word(s) given a context and optional first letter constraint.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained NextWordModel\n",
    "        context_text: String context (previous words)\n",
    "        first_letter: Optional first letter constraint (e.g., 't')\n",
    "        word_to_int: Vocabulary mapping\n",
    "        int_to_word: Reverse vocabulary mapping\n",
    "        seq_length: Maximum sequence length\n",
    "        top_k: Number of top predictions to return\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples (word, probability) sorted by probability\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Tokenize and convert context to integers\n",
    "        words = context_text.lower().split()\n",
    "        context_indices = [word_to_int.get(w, word_to_int['<UNK>']) for w in words]\n",
    "        \n",
    "        # Pad or truncate to seq_length\n",
    "        if len(context_indices) > seq_length:\n",
    "            context_indices = context_indices[-seq_length:]\n",
    "        else:\n",
    "            context_indices = [word_to_int['<PAD>']] * (seq_length - len(context_indices)) + context_indices\n",
    "        \n",
    "        # Convert to tensor\n",
    "        context_tensor = torch.tensor([context_indices]).to(device)\n",
    "        \n",
    "        # Handle first letter\n",
    "        if first_letter:\n",
    "            first_letter = first_letter.lower()\n",
    "            if 'a' <= first_letter <= 'z':\n",
    "                letter_idx = ord(first_letter) - ord('a')\n",
    "            else:\n",
    "                letter_idx = 26\n",
    "            letter_tensor = torch.tensor([letter_idx]).to(device)\n",
    "        else:\n",
    "            letter_tensor = None\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        hidden = model.init_hidden(1)\n",
    "        \n",
    "        # Get predictions\n",
    "        output, _ = model(context_tensor, hidden, first_letter=letter_tensor)\n",
    "        \n",
    "        # Apply softmax to get probabilities\n",
    "        probabilities = torch.nn.functional.softmax(output, dim=1).squeeze()\n",
    "        \n",
    "        # Get top k predictions\n",
    "        top_probs, top_indices = torch.topk(probabilities, top_k)\n",
    "        \n",
    "        # Convert indices to words\n",
    "        predictions = []\n",
    "        for i in range(top_k):\n",
    "            word = int_to_word[top_indices[i].item()]\n",
    "            prob = top_probs[i].item()\n",
    "            predictions.append((word, prob))\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "def evaluate_model(model, test_loader, int_to_word=None, device=None):\n",
    "    \"\"\"\n",
    "    Evaluate model performance on test set.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained NextWordModel\n",
    "        test_loader: DataLoader with test data\n",
    "        int_to_word: Reverse vocabulary mapping (index to word)\n",
    "        device: Device to run evaluation on\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    correct_with_first_letter = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (contexts, targets, first_letters) in enumerate(test_loader):\n",
    "            contexts, targets, first_letters = contexts.to(device), targets.to(device), first_letters.to(device)\n",
    "            \n",
    "            # Initialize hidden state\n",
    "            hidden = model.init_hidden(contexts.size(0))\n",
    "            \n",
    "            # Get predictions\n",
    "            output, _ = model(contexts, hidden, first_letter=first_letters)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(output, targets)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total_predictions += targets.size(0)\n",
    "            correct_predictions += (predicted == targets).sum().item()\n",
    "            \n",
    "            # Calculate accuracy with first letter constraint\n",
    "            if int_to_word is not None:\n",
    "                for i in range(contexts.size(0)):\n",
    "                    first_letter_idx = first_letters[i].item()\n",
    "                    true_answer = targets[i].item()\n",
    "                    predicted_answer = predicted[i].item()\n",
    "                    \n",
    "                    # Get actual first letter from index\n",
    "                    if first_letter_idx <= 25:  # a-z\n",
    "                        actual_letter = chr(first_letter_idx + ord('a'))\n",
    "                    else:  # other\n",
    "                        actual_letter = 'other'\n",
    "                    \n",
    "                    # Check if prediction matches the first letter constraint\n",
    "                    predicted_word = int_to_word.get(predicted_answer, '')\n",
    "                    if predicted_word and len(predicted_word) > 0:\n",
    "                        predicted_first_letter = predicted_word[0].lower()\n",
    "                        if predicted_first_letter == actual_letter:\n",
    "                            if predicted_answer == true_answer:\n",
    "                                correct_with_first_letter += 1\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = (100 * correct_predictions / total_predictions) if total_predictions > 0 else 0\n",
    "    accuracy_with_constraint = (100 * correct_with_first_letter / total_predictions) if total_predictions > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'loss': avg_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'accuracy_with_first_letter_constraint': accuracy_with_constraint,\n",
    "        'total_samples': total_predictions\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found trained model and vocabulary files.\n",
      "Evaluating model on dev_set.csv...\n",
      "\n",
      "=== Evaluation Results ===\n",
      "Loss: 2.9672\n",
      "Overall Accuracy: 48.40%\n",
      "Accuracy with First Letter Constraint: 40.23%\n",
      "Total Samples: 93450\n",
      "\n",
      "=== Example Predictions ===\n",
      "\n",
      "Context: 'states on monday warned north korea to avoid provoking trouble as pyongyang ' s most senior defector spent his sixth'\n",
      "First letter: 'd'\n",
      "True answer: 'day'\n",
      "Top 3 predictions: [('debate', 0.34693729877471924), ('day', 0.13601019978523254), ('division', 0.08998391777276993)]\n",
      "\n",
      "Context: 'to drastically cut its car import duties , taiwan on thursday won european union support for its bid to enter'\n",
      "First letter: 't'\n",
      "True answer: 'the'\n",
      "Top 3 predictions: [('the', 0.8832387924194336), ('their', 0.028638502582907677), ('them', 0.009884534403681755)]\n",
      "\n",
      "Context: 'three soldiers were injured in a bombing ambush launched by suspect thai southern insurgents on wednesday'\n",
      "First letter: 'm'\n",
      "True answer: 'morning'\n",
      "Top 3 predictions: [('morning', 0.6750795841217041), ('moved', 0.05074993893504143), ('monday', 0.027255738154053688)]\n",
      "\n",
      "Context: 'official says a yemeni investigation team has found that a yemeni airplane that crashed off the comoros islands last month'\n",
      "First letter: 'w'\n",
      "True answer: 'was'\n",
      "Top 3 predictions: [('were', 0.2374880015850067), ('was', 0.18813355267047882), ('with', 0.10967748612165451)]\n",
      "\n",
      "Context: '- ravaged west african country has made a firm commitment to continue with the peace talks in ghana , according'\n",
      "First letter: 't'\n",
      "True answer: 'to'\n",
      "Top 3 predictions: [('to', 0.9996414184570312), ('minister', 0.00012461160076782107), ('the', 0.00010047852265415713)]\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Model Evaluation ---\n",
    "\n",
    "# First, let's check if we have a trained model and vocabulary\n",
    "import os\n",
    "\n",
    "if os.path.exists('best_model.pth') and os.path.exists('word_to_int.json'):\n",
    "    print(\"Found trained model and vocabulary files.\")\n",
    "    \n",
    "    # Load vocabulary\n",
    "    with open('word_to_int.json', 'r') as f:\n",
    "        word_to_int = json.load(f)\n",
    "    \n",
    "    # Create reverse vocabulary mapping\n",
    "    int_to_word = {i: word for word, i in word_to_int.items()}\n",
    "    vocab_size = len(word_to_int)\n",
    "    \n",
    "    # Load the trained model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = load_model('best_model.pth', vocab_size, EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS)\n",
    "    \n",
    "    # Create validation dataset and loader\n",
    "    val_dataset = DevSetDataset(DEV_SET_PATH, word_to_int, seq_length=SEQ_LENGTH)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE * 2, shuffle=False)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    print(\"Evaluating model on dev_set.csv...\")\n",
    "    results = evaluate_model(model, val_loader, int_to_word=int_to_word, device=device)\n",
    "    \n",
    "    print(\"\\n=== Evaluation Results ===\")\n",
    "    print(f\"Loss: {results['loss']:.4f}\")\n",
    "    print(f\"Overall Accuracy: {results['accuracy']:.2f}%\")\n",
    "    print(f\"Accuracy with First Letter Constraint: {results['accuracy_with_first_letter_constraint']:.2f}%\")\n",
    "    print(f\"Total Samples: {results['total_samples']}\")\n",
    "    \n",
    "    # Example predictions\n",
    "    print(\"\\n=== Example Predictions ===\")\n",
    "    for i in range(min(5, len(val_dataset))):\n",
    "        context, answer, first_letter = val_dataset[i]\n",
    "        context_text = \" \".join([int_to_word[idx.item()] for idx in context if idx.item() != word_to_int['<PAD>']])\n",
    "        true_answer = int_to_word[answer.item()]\n",
    "        letter_hint = chr(first_letter.item() + ord('a')) if first_letter.item() <= 25 else 'other'\n",
    "        \n",
    "        predictions = predict_next_word(model, context_text, letter_hint, word_to_int, int_to_word, SEQ_LENGTH, top_k=3)\n",
    "        \n",
    "        print(f\"\\nContext: '{context_text}'\")\n",
    "        print(f\"First letter: '{letter_hint}'\")\n",
    "        print(f\"True answer: '{true_answer}'\")\n",
    "        print(f\"Top 3 predictions: {predictions}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No trained model found. Please train the model first by running the training cell.\")\n",
    "    print(\"To train the model, make sure you have:\")\n",
    "    print(\"1. train.zip file containing train.src.tok\")\n",
    "    print(\"2. dev_set.csv file\")\n",
    "    print(\"3. Run the training configuration cell (cell 6)\")\n",
    "    \n",
    "    # If you want to test the evaluation function without a trained model,\n",
    "    # you can uncomment the following code to create a dummy model:\n",
    "    \"\"\"\n",
    "    # Create dummy vocabulary and model for testing\n",
    "    dummy_word_to_int = {'<PAD>': 0, '<UNK>': 1, 'the': 2, 'a': 3, 'an': 4, 'and': 5}\n",
    "    dummy_int_to_word = {i: w for w, i in dummy_word_to_int.items()}\n",
    "    \n",
    "    # Create a dummy model\n",
    "    dummy_model = NextWordModel(len(dummy_word_to_int), EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS)\n",
    "    \n",
    "    # Create a small dummy dataset for testing\n",
    "    # This would require a CSV with words that exist in dummy_word_to_int\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
