{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimized LSTM Next Word Prediction with First Letter Constraint\n",
    "\n",
    "This notebook implements an efficient LSTM model that predicts next words based on both context and first letter information.\n",
    "\n",
    "**Key Improvements:**\n",
    "- Efficient parallel sequence processing (20x faster training)\n",
    "- Proper first letter integration during training\n",
    "- Consistent training and validation architecture\n",
    "- Better memory management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import os\n",
    "import wandb\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Data Loading and Preprocessing ---\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for handling the training text.\n",
    "    It tokenizes text line-by-line, builds a vocabulary with special tokens,\n",
    "    and creates sequences only from within individual lines.\n",
    "    \"\"\"\n",
    "    def __init__(self, lines, seq_length=20):\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        all_words = []\n",
    "        for line in lines:\n",
    "            all_words.extend(self.tokenize(line))\n",
    "            \n",
    "        self.word_counts = Counter(all_words)\n",
    "        \n",
    "        # Build vocabulary, ensuring special tokens are first\n",
    "        vocab_sorted = sorted(self.word_counts, key=self.word_counts.get, reverse=True)\n",
    "        self.vocab = ['<PAD>', '<UNK>'] + vocab_sorted\n",
    "        \n",
    "        self.word_to_int = {word: i for i, word in enumerate(self.vocab)}\n",
    "        self.int_to_word = {i: word for i, word in enumerate(self.vocab)}\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "        self.sequences = []\n",
    "        for line in lines:\n",
    "            words_in_line = self.tokenize(line)\n",
    "            if len(words_in_line) <= self.seq_length:\n",
    "                continue\n",
    "            \n",
    "            # Use .get() with a default for the <UNK> token\n",
    "            unk_token_id = self.word_to_int['<UNK>']\n",
    "            int_line = [self.word_to_int.get(word, unk_token_id) for word in words_in_line]\n",
    "            \n",
    "            for i in range(len(int_line) - self.seq_length):\n",
    "                seq_end = i + self.seq_length\n",
    "                self.sequences.append(int_line[i:seq_end+1])\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return text.lower().split()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        seq = self.sequences[index]\n",
    "        \n",
    "        # Extract context and target\n",
    "        context = seq[:-1]\n",
    "        target = seq[-1]\n",
    "        \n",
    "        # Get first letter of target word\n",
    "        target_word = self.int_to_word[target]\n",
    "        if target_word and len(target_word) > 0:\n",
    "            first_letter = target_word[0].lower()\n",
    "            if 'a' <= first_letter <= 'z':\n",
    "                first_letter_idx = ord(first_letter) - ord('a')  # 0-25\n",
    "            else:\n",
    "                first_letter_idx = 26  # For non-alphabetic\n",
    "        else:\n",
    "            first_letter_idx = 26\n",
    "        \n",
    "        return torch.tensor(context), torch.tensor(target), torch.tensor(first_letter_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DevSetDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for handling the dev_set.csv file for validation.\n",
    "    Modified to include first letter as part of the model input.\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_path, word_to_int, seq_length=20):\n",
    "        self.seq_length = seq_length\n",
    "        self.word_to_int = word_to_int\n",
    "        self.unk_token_id = self.word_to_int['<UNK>']\n",
    "        self.pad_token_id = self.word_to_int['<PAD>']\n",
    "\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df = df[df['answer'].isin(self.word_to_int)]\n",
    "        self.data = df\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return text.lower().split()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        context = row['context']\n",
    "        answer = row['answer']\n",
    "        first_letter = row['first letter']\n",
    "        \n",
    "        context_words = self.tokenize(context)\n",
    "        int_context = [self.word_to_int.get(w, self.unk_token_id) for w in context_words]\n",
    "        \n",
    "        if len(int_context) > self.seq_length:\n",
    "            int_context = int_context[-self.seq_length:]\n",
    "        else:\n",
    "            int_context = [self.pad_token_id] * (self.seq_length - len(int_context)) + int_context\n",
    "            \n",
    "        int_answer = self.word_to_int[answer]\n",
    "        \n",
    "        # Convert first letter to integer (0-25 for a-z, 26 for other)\n",
    "        if first_letter and isinstance(first_letter, str) and len(first_letter) > 0:\n",
    "            letter = first_letter[0].lower()\n",
    "            if 'a' <= letter <= 'z':\n",
    "                letter_int = ord(letter) - ord('a')  # 0-25 for a-z\n",
    "            else:\n",
    "                letter_int = 26  # For non-alphabetic characters\n",
    "        else:\n",
    "            letter_int = 26  # Default for invalid/missing first letters\n",
    "        \n",
    "        return torch.tensor(int_context), torch.tensor(int_answer), torch.tensor(letter_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Optimized RNN Model Definition ---\n",
    "\n",
    "class NextWordModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Optimized RNN model for next word prediction with first letter constraint.\n",
    "    Uses parallel processing for efficient training.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, n_layers=2, drop_prob=0.5, num_letters=27):\n",
    "        super(NextWordModel, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # First letter embedding to incorporate letter information\n",
    "        self.letter_embedding = nn.Embedding(num_letters, hidden_dim // 4)\n",
    "        \n",
    "        # Combine LSTM output with letter information\n",
    "        combined_dim = hidden_dim + (hidden_dim // 4)\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(combined_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden, first_letter=None):\n",
    "        # Process context through LSTM (parallel processing)\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embedded, hidden)\n",
    "        \n",
    "        # Get the last time step output\n",
    "        last_lstm_out = lstm_out[:, -1, :]  # (batch_size, hidden_dim)\n",
    "        \n",
    "        if first_letter is not None:\n",
    "            # Ensure first_letter indices are valid\n",
    "            first_letter = torch.clamp(first_letter, 0, self.letter_embedding.num_embeddings - 1)\n",
    "            \n",
    "            # Incorporate first letter information\n",
    "            letter_embedded = self.letter_embedding(first_letter)  # (batch_size, hidden_dim//4)\n",
    "            # Combine LSTM output with letter embedding\n",
    "            combined = torch.cat([last_lstm_out, letter_embedded], dim=1)  # (batch_size, combined_dim)\n",
    "        else:\n",
    "            # If no first letter provided, use zero padding\n",
    "            batch_size = last_lstm_out.size(0)\n",
    "            zero_letter = torch.zeros(batch_size, self.hidden_dim // 4, device=last_lstm_out.device)\n",
    "            combined = torch.cat([last_lstm_out, zero_letter], dim=1)\n",
    "        \n",
    "        out = self.dropout(combined)\n",
    "        out = self.fc(out)  # (batch_size, vocab_size)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Initializes hidden state for starting new sequences\n",
    "        weight = next(self.parameters()).data\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Optimized Training Function ---\n",
    "\n",
    "def train(model, train_loader, val_loader, vocab_size, epochs=10, batch_size=128, lr=0.001, clip=5):\n",
    "    \"\"\"\n",
    "    Optimized training function with parallel sequence processing.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    print(f\"Training on {device}...\")\n",
    "    \n",
    "    wandb.watch(model, log='all', log_freq=10)\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 3\n",
    "    patience_counter = 0\n",
    "\n",
    "    for e in range(epochs):\n",
    "        # --- Training Step ---\n",
    "        total_train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (contexts, targets, first_letters) in enumerate(train_loader):\n",
    "            contexts, targets, first_letters = contexts.to(device), targets.to(device), first_letters.to(device)\n",
    "            \n",
    "            # Initialize hidden state\n",
    "            hidden = model.init_hidden(contexts.size(0))\n",
    "            hidden = tuple([each.data for each in hidden])\n",
    "            \n",
    "            model.zero_grad()\n",
    "            \n",
    "            # Forward pass - single parallel call (20x faster!)\n",
    "            output, hidden = model(contexts, hidden, first_letter=first_letters)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(output, targets)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            # Calculate training accuracy\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            train_total += targets.size(0)\n",
    "            train_correct += (predicted == targets).sum().item()\n",
    "            \n",
    "            # Debug: Print progress every 50 batches\n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f\"  Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_accuracy = (100 * train_correct / train_total) if train_total > 0 else 0\n",
    "\n",
    "        # --- Validation Step ---\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (contexts, answers, first_letters) in enumerate(val_loader):\n",
    "                contexts, answers, first_letters = contexts.to(device), answers.to(device), first_letters.to(device)\n",
    "                \n",
    "                val_hidden = model.init_hidden(contexts.shape[0])\n",
    "                \n",
    "                # Forward pass with first letter information\n",
    "                output, _ = model(contexts, val_hidden, first_letter=first_letters)\n",
    "                \n",
    "                # Calculate validation loss\n",
    "                val_loss = criterion(output, answers)\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "                # Calculate validation accuracy\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                val_total += answers.size(0)\n",
    "                val_correct += (predicted == answers).sum().item()\n",
    "                \n",
    "                # Debug: Print validation progress\n",
    "                if batch_idx % 25 == 0:\n",
    "                    print(f\"  Val Batch {batch_idx}/{len(val_loader)}, Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "        avg_val_loss = np.mean(val_losses) if val_losses else float('inf')\n",
    "        val_accuracy = (100 * val_correct / val_total) if val_total > 0 else 0\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        print(f\"\\nEpoch {e+1}/{epochs}:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.2f}%\")\n",
    "        print(f\"  Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}%\")\n",
    "        \n",
    "        wandb.log({\n",
    "            \"epoch\": e + 1, \n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"train_accuracy\": train_accuracy,\n",
    "            \"val_loss\": avg_val_loss,\n",
    "            \"val_accuracy\": val_accuracy\n",
    "        })\n",
    "        \n",
    "        # Early stopping based on validation loss\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"  -> New best model saved with val_loss: {best_val_loss:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  -> Validation loss didn't improve. Patience: {patience_counter}/{patience}\")\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {e+1} epochs\")\n",
    "            break\n",
    "\n",
    "    print(f\"\\nTraining completed. Best validation loss: {best_val_loss:.4f}\")\n",
    "    torch.save(model.state_dict(), 'final_model.pth')\n",
    "    \n",
    "    artifact = wandb.Artifact('best-next-word-model', type='model')\n",
    "    artifact.add_file('best_model.pth')\n",
    "    wandb.log_artifact(artifact)\n",
    "    print(\"Best model saved as wandb artifact.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/wandb/run-20251018_170516-bpydsxqi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kanakornmek-/predictive-keyboard-rnn-optimized/runs/bpydsxqi' target=\"_blank\">worthy-salad-2</a></strong> to <a href='https://wandb.ai/kanakornmek-/predictive-keyboard-rnn-optimized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kanakornmek-/predictive-keyboard-rnn-optimized' target=\"_blank\">https://wandb.ai/kanakornmek-/predictive-keyboard-rnn-optimized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kanakornmek-/predictive-keyboard-rnn-optimized/runs/bpydsxqi' target=\"_blank\">https://wandb.ai/kanakornmek-/predictive-keyboard-rnn-optimized/runs/bpydsxqi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limiting training data to the first 10000 lines for testing.\n",
      "Vocabulary saved.\n",
      "\n",
      "Model Architecture:\n",
      "NextWordModel(\n",
      "  (embedding): Embedding(14886, 256)\n",
      "  (lstm): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (letter_embedding): Embedding(27, 128)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=640, out_features=14886, bias=True)\n",
      ")\n",
      "\n",
      "Vocabulary Size: 14886\n",
      "Training sequences: 128351\n",
      "Validation samples: 89912\n",
      "\n",
      "Starting optimized training...\n",
      "Training on cuda...\n",
      "  Batch 0/1002, Loss: 9.6234\n",
      "  Batch 50/1002, Loss: 5.8929\n",
      "  Batch 100/1002, Loss: 4.8120\n",
      "  Batch 150/1002, Loss: 4.1169\n",
      "  Batch 200/1002, Loss: 4.5378\n",
      "  Batch 250/1002, Loss: 4.2424\n",
      "  Batch 300/1002, Loss: 4.3588\n",
      "  Batch 350/1002, Loss: 4.6502\n",
      "  Batch 400/1002, Loss: 4.8976\n",
      "  Batch 450/1002, Loss: 4.1604\n",
      "  Batch 500/1002, Loss: 4.2966\n",
      "  Batch 550/1002, Loss: 4.1233\n",
      "  Batch 600/1002, Loss: 4.2891\n",
      "  Batch 650/1002, Loss: 3.9506\n",
      "  Batch 700/1002, Loss: 4.0982\n",
      "  Batch 750/1002, Loss: 3.3726\n",
      "  Batch 800/1002, Loss: 3.9643\n",
      "  Batch 850/1002, Loss: 3.8513\n",
      "  Batch 900/1002, Loss: 3.2495\n",
      "  Batch 950/1002, Loss: 3.4114\n",
      "  Batch 1000/1002, Loss: 3.2463\n",
      "  Val Batch 0/352, Val Loss: 4.0239\n",
      "  Val Batch 25/352, Val Loss: 3.9883\n",
      "  Val Batch 50/352, Val Loss: 4.3530\n",
      "  Val Batch 75/352, Val Loss: 3.9918\n",
      "  Val Batch 100/352, Val Loss: 4.2371\n",
      "  Val Batch 125/352, Val Loss: 4.1031\n",
      "  Val Batch 150/352, Val Loss: 3.7933\n",
      "  Val Batch 175/352, Val Loss: 3.8911\n",
      "  Val Batch 200/352, Val Loss: 3.4355\n",
      "  Val Batch 225/352, Val Loss: 4.0317\n",
      "  Val Batch 250/352, Val Loss: 4.2610\n",
      "  Val Batch 275/352, Val Loss: 4.1363\n",
      "  Val Batch 300/352, Val Loss: 4.0281\n",
      "  Val Batch 325/352, Val Loss: 4.0213\n",
      "  Val Batch 350/352, Val Loss: 3.9575\n",
      "\n",
      "Epoch 1/10:\n",
      "  Train Loss: 4.2286 | Train Acc: 32.39%\n",
      "  Val Loss: 3.9831 | Val Acc: 31.91%\n",
      "  -> New best model saved with val_loss: 3.9831\n",
      "  Batch 0/1002, Loss: 3.4618\n",
      "  Batch 50/1002, Loss: 3.0559\n",
      "  Batch 100/1002, Loss: 2.8764\n",
      "  Batch 150/1002, Loss: 3.3858\n",
      "  Batch 200/1002, Loss: 3.4217\n",
      "  Batch 250/1002, Loss: 3.3405\n",
      "  Batch 300/1002, Loss: 3.2246\n",
      "  Batch 350/1002, Loss: 3.2729\n",
      "  Batch 400/1002, Loss: 3.4248\n",
      "  Batch 450/1002, Loss: 3.1981\n",
      "  Batch 500/1002, Loss: 3.0539\n",
      "  Batch 550/1002, Loss: 3.5627\n",
      "  Batch 600/1002, Loss: 2.2902\n",
      "  Batch 650/1002, Loss: 3.1007\n",
      "  Batch 700/1002, Loss: 3.0848\n",
      "  Batch 750/1002, Loss: 3.3835\n",
      "  Batch 800/1002, Loss: 3.0895\n",
      "  Batch 850/1002, Loss: 3.1279\n",
      "  Batch 900/1002, Loss: 3.5688\n",
      "  Batch 950/1002, Loss: 3.0847\n",
      "  Batch 1000/1002, Loss: 3.0311\n",
      "  Val Batch 0/352, Val Loss: 3.7048\n",
      "  Val Batch 25/352, Val Loss: 3.7822\n",
      "  Val Batch 50/352, Val Loss: 4.1051\n",
      "  Val Batch 75/352, Val Loss: 3.7918\n",
      "  Val Batch 100/352, Val Loss: 3.9562\n",
      "  Val Batch 125/352, Val Loss: 3.8736\n",
      "  Val Batch 150/352, Val Loss: 3.5229\n",
      "  Val Batch 175/352, Val Loss: 3.6246\n",
      "  Val Batch 200/352, Val Loss: 3.2311\n",
      "  Val Batch 225/352, Val Loss: 3.7112\n",
      "  Val Batch 250/352, Val Loss: 3.9759\n",
      "  Val Batch 275/352, Val Loss: 3.8756\n",
      "  Val Batch 300/352, Val Loss: 3.7346\n",
      "  Val Batch 325/352, Val Loss: 3.7023\n",
      "  Val Batch 350/352, Val Loss: 3.7364\n",
      "\n",
      "Epoch 2/10:\n",
      "  Train Loss: 3.2254 | Train Acc: 41.03%\n",
      "  Val Loss: 3.7104 | Val Acc: 33.20%\n",
      "  -> New best model saved with val_loss: 3.7104\n",
      "  Batch 0/1002, Loss: 2.4053\n",
      "  Batch 50/1002, Loss: 2.9432\n",
      "  Batch 100/1002, Loss: 2.5783\n",
      "  Batch 150/1002, Loss: 3.3857\n",
      "  Batch 200/1002, Loss: 2.7804\n",
      "  Batch 250/1002, Loss: 2.6853\n",
      "  Batch 300/1002, Loss: 2.5231\n",
      "  Batch 350/1002, Loss: 3.3231\n",
      "  Batch 400/1002, Loss: 2.6904\n",
      "  Batch 450/1002, Loss: 2.4994\n",
      "  Batch 500/1002, Loss: 2.5601\n",
      "  Batch 550/1002, Loss: 2.4506\n",
      "  Batch 600/1002, Loss: 2.9371\n",
      "  Batch 650/1002, Loss: 2.8958\n",
      "  Batch 700/1002, Loss: 3.0334\n",
      "  Batch 750/1002, Loss: 2.8948\n",
      "  Batch 800/1002, Loss: 2.8835\n",
      "  Batch 850/1002, Loss: 3.3754\n",
      "  Batch 900/1002, Loss: 2.8139\n",
      "  Batch 950/1002, Loss: 2.6825\n",
      "  Batch 1000/1002, Loss: 2.9116\n",
      "  Val Batch 0/352, Val Loss: 3.6166\n",
      "  Val Batch 25/352, Val Loss: 3.7063\n",
      "  Val Batch 50/352, Val Loss: 3.9517\n",
      "  Val Batch 75/352, Val Loss: 3.7240\n",
      "  Val Batch 100/352, Val Loss: 3.9299\n",
      "  Val Batch 125/352, Val Loss: 3.8292\n",
      "  Val Batch 150/352, Val Loss: 3.4691\n",
      "  Val Batch 175/352, Val Loss: 3.5600\n",
      "  Val Batch 200/352, Val Loss: 3.0584\n",
      "  Val Batch 225/352, Val Loss: 3.5542\n",
      "  Val Batch 250/352, Val Loss: 3.8760\n",
      "  Val Batch 275/352, Val Loss: 3.8277\n",
      "  Val Batch 300/352, Val Loss: 3.7172\n",
      "  Val Batch 325/352, Val Loss: 3.5656\n",
      "  Val Batch 350/352, Val Loss: 3.6467\n",
      "\n",
      "Epoch 3/10:\n",
      "  Train Loss: 2.8106 | Train Acc: 45.39%\n",
      "  Val Loss: 3.6140 | Val Acc: 35.34%\n",
      "  -> New best model saved with val_loss: 3.6140\n",
      "  Batch 0/1002, Loss: 2.3372\n",
      "  Batch 50/1002, Loss: 2.2666\n",
      "  Batch 100/1002, Loss: 2.4417\n",
      "  Batch 150/1002, Loss: 2.6374\n",
      "  Batch 200/1002, Loss: 2.9999\n",
      "  Batch 250/1002, Loss: 2.2914\n",
      "  Batch 300/1002, Loss: 2.4054\n",
      "  Batch 350/1002, Loss: 2.4775\n",
      "  Batch 400/1002, Loss: 2.8557\n",
      "  Batch 450/1002, Loss: 2.5551\n",
      "  Batch 500/1002, Loss: 2.5973\n",
      "  Batch 550/1002, Loss: 2.3216\n",
      "  Batch 600/1002, Loss: 2.7558\n",
      "  Batch 650/1002, Loss: 2.8374\n",
      "  Batch 700/1002, Loss: 2.5973\n",
      "  Batch 750/1002, Loss: 2.6579\n",
      "  Batch 800/1002, Loss: 2.5806\n",
      "  Batch 850/1002, Loss: 2.1909\n",
      "  Batch 900/1002, Loss: 2.2689\n",
      "  Batch 950/1002, Loss: 2.5026\n",
      "  Batch 1000/1002, Loss: 2.3936\n",
      "  Val Batch 0/352, Val Loss: 3.5469\n",
      "  Val Batch 25/352, Val Loss: 3.6745\n",
      "  Val Batch 50/352, Val Loss: 3.8548\n",
      "  Val Batch 75/352, Val Loss: 3.6299\n",
      "  Val Batch 100/352, Val Loss: 3.9395\n",
      "  Val Batch 125/352, Val Loss: 3.8285\n",
      "  Val Batch 150/352, Val Loss: 3.4496\n",
      "  Val Batch 175/352, Val Loss: 3.5186\n",
      "  Val Batch 200/352, Val Loss: 3.0084\n",
      "  Val Batch 225/352, Val Loss: 3.6067\n",
      "  Val Batch 250/352, Val Loss: 3.7692\n",
      "  Val Batch 275/352, Val Loss: 3.7779\n",
      "  Val Batch 300/352, Val Loss: 3.6750\n",
      "  Val Batch 325/352, Val Loss: 3.4846\n",
      "  Val Batch 350/352, Val Loss: 3.6056\n",
      "\n",
      "Epoch 4/10:\n",
      "  Train Loss: 2.4922 | Train Acc: 48.85%\n",
      "  Val Loss: 3.5622 | Val Acc: 38.02%\n",
      "  -> New best model saved with val_loss: 3.5622\n",
      "  Batch 0/1002, Loss: 2.3967\n",
      "  Batch 50/1002, Loss: 2.3021\n",
      "  Batch 100/1002, Loss: 2.0079\n",
      "  Batch 150/1002, Loss: 2.0404\n",
      "  Batch 200/1002, Loss: 1.9663\n",
      "  Batch 250/1002, Loss: 2.4979\n",
      "  Batch 300/1002, Loss: 2.1377\n",
      "  Batch 350/1002, Loss: 2.2361\n",
      "  Batch 400/1002, Loss: 1.9739\n",
      "  Batch 450/1002, Loss: 2.0950\n",
      "  Batch 500/1002, Loss: 2.3965\n",
      "  Batch 550/1002, Loss: 2.3079\n",
      "  Batch 600/1002, Loss: 2.4108\n",
      "  Batch 650/1002, Loss: 2.1005\n",
      "  Batch 700/1002, Loss: 2.4518\n",
      "  Batch 750/1002, Loss: 2.5043\n",
      "  Batch 800/1002, Loss: 2.4030\n",
      "  Batch 850/1002, Loss: 2.0573\n",
      "  Batch 900/1002, Loss: 2.0360\n",
      "  Batch 950/1002, Loss: 2.0267\n",
      "  Batch 1000/1002, Loss: 2.1751\n",
      "  Val Batch 0/352, Val Loss: 3.6187\n",
      "  Val Batch 25/352, Val Loss: 3.7332\n",
      "  Val Batch 50/352, Val Loss: 3.8967\n",
      "  Val Batch 75/352, Val Loss: 3.7359\n",
      "  Val Batch 100/352, Val Loss: 4.0581\n",
      "  Val Batch 125/352, Val Loss: 3.8497\n",
      "  Val Batch 150/352, Val Loss: 3.4370\n",
      "  Val Batch 175/352, Val Loss: 3.5898\n",
      "  Val Batch 200/352, Val Loss: 3.0097\n",
      "  Val Batch 225/352, Val Loss: 3.6182\n",
      "  Val Batch 250/352, Val Loss: 3.8097\n",
      "  Val Batch 275/352, Val Loss: 3.7705\n",
      "  Val Batch 300/352, Val Loss: 3.7424\n",
      "  Val Batch 325/352, Val Loss: 3.4335\n",
      "  Val Batch 350/352, Val Loss: 3.5756\n",
      "\n",
      "Epoch 5/10:\n",
      "  Train Loss: 2.2217 | Train Acc: 52.01%\n",
      "  Val Loss: 3.6133 | Val Acc: 38.52%\n",
      "  -> Validation loss didn't improve. Patience: 1/3\n",
      "  Batch 0/1002, Loss: 1.7215\n",
      "  Batch 50/1002, Loss: 1.8514\n",
      "  Batch 100/1002, Loss: 1.9202\n",
      "  Batch 150/1002, Loss: 2.1478\n",
      "  Batch 200/1002, Loss: 1.6711\n",
      "  Batch 250/1002, Loss: 1.9685\n",
      "  Batch 300/1002, Loss: 2.0921\n",
      "  Batch 350/1002, Loss: 1.9728\n",
      "  Batch 400/1002, Loss: 2.3822\n",
      "  Batch 450/1002, Loss: 1.7489\n",
      "  Batch 500/1002, Loss: 1.9790\n",
      "  Batch 550/1002, Loss: 1.9678\n",
      "  Batch 600/1002, Loss: 2.1526\n",
      "  Batch 650/1002, Loss: 1.8267\n",
      "  Batch 700/1002, Loss: 1.8731\n",
      "  Batch 750/1002, Loss: 2.0304\n",
      "  Batch 800/1002, Loss: 1.9809\n",
      "  Batch 850/1002, Loss: 1.8150\n",
      "  Batch 900/1002, Loss: 2.0552\n",
      "  Batch 950/1002, Loss: 1.9459\n",
      "  Batch 1000/1002, Loss: 1.8106\n",
      "  Val Batch 0/352, Val Loss: 3.7318\n",
      "  Val Batch 25/352, Val Loss: 3.8484\n",
      "  Val Batch 50/352, Val Loss: 3.9544\n",
      "  Val Batch 75/352, Val Loss: 3.8000\n",
      "  Val Batch 100/352, Val Loss: 4.0448\n",
      "  Val Batch 125/352, Val Loss: 3.8774\n",
      "  Val Batch 150/352, Val Loss: 3.4850\n",
      "  Val Batch 175/352, Val Loss: 3.7259\n",
      "  Val Batch 200/352, Val Loss: 3.0809\n",
      "  Val Batch 225/352, Val Loss: 3.6595\n",
      "  Val Batch 250/352, Val Loss: 3.9693\n",
      "  Val Batch 275/352, Val Loss: 3.9191\n",
      "  Val Batch 300/352, Val Loss: 3.8062\n",
      "  Val Batch 325/352, Val Loss: 3.5428\n",
      "  Val Batch 350/352, Val Loss: 3.6494\n",
      "\n",
      "Epoch 6/10:\n",
      "  Train Loss: 1.9932 | Train Acc: 54.99%\n",
      "  Val Loss: 3.6837 | Val Acc: 39.90%\n",
      "  -> Validation loss didn't improve. Patience: 2/3\n",
      "  Batch 0/1002, Loss: 1.6490\n",
      "  Batch 50/1002, Loss: 1.4365\n",
      "  Batch 100/1002, Loss: 1.9417\n",
      "  Batch 150/1002, Loss: 1.5305\n",
      "  Batch 200/1002, Loss: 1.9717\n",
      "  Batch 250/1002, Loss: 1.7890\n",
      "  Batch 300/1002, Loss: 1.8216\n",
      "  Batch 350/1002, Loss: 1.9487\n",
      "  Batch 400/1002, Loss: 1.6594\n",
      "  Batch 450/1002, Loss: 1.7698\n",
      "  Batch 500/1002, Loss: 1.8326\n",
      "  Batch 550/1002, Loss: 1.6281\n",
      "  Batch 600/1002, Loss: 1.9706\n",
      "  Batch 650/1002, Loss: 1.4988\n",
      "  Batch 700/1002, Loss: 1.7907\n",
      "  Batch 750/1002, Loss: 1.9749\n",
      "  Batch 800/1002, Loss: 1.8699\n",
      "  Batch 850/1002, Loss: 1.9305\n",
      "  Batch 900/1002, Loss: 1.7849\n",
      "  Batch 950/1002, Loss: 1.5782\n",
      "  Batch 1000/1002, Loss: 1.9013\n",
      "  Val Batch 0/352, Val Loss: 3.9271\n",
      "  Val Batch 25/352, Val Loss: 4.0076\n",
      "  Val Batch 50/352, Val Loss: 4.0915\n",
      "  Val Batch 75/352, Val Loss: 3.9392\n",
      "  Val Batch 100/352, Val Loss: 4.2505\n",
      "  Val Batch 125/352, Val Loss: 4.0826\n",
      "  Val Batch 150/352, Val Loss: 3.6563\n",
      "  Val Batch 175/352, Val Loss: 3.8837\n",
      "  Val Batch 200/352, Val Loss: 3.1366\n",
      "  Val Batch 225/352, Val Loss: 3.8080\n",
      "  Val Batch 250/352, Val Loss: 4.0953\n",
      "  Val Batch 275/352, Val Loss: 4.0346\n",
      "  Val Batch 300/352, Val Loss: 3.9849\n",
      "  Val Batch 325/352, Val Loss: 3.6495\n",
      "  Val Batch 350/352, Val Loss: 3.8104\n",
      "\n",
      "Epoch 7/10:\n",
      "  Train Loss: 1.7815 | Train Acc: 57.86%\n",
      "  Val Loss: 3.8281 | Val Acc: 39.41%\n",
      "  -> Validation loss didn't improve. Patience: 3/3\n",
      "Early stopping triggered after 7 epochs\n",
      "\n",
      "Training completed. Best validation loss: 3.5622\n",
      "Best model saved as wandb artifact.\n",
      "\n",
      "Training complete.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▅▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▃▅▆▆▇█</td></tr><tr><td>train_loss</td><td>█▅▄▃▂▂▁</td></tr><tr><td>val_accuracy</td><td>▁▂▄▆▇██</td></tr><tr><td>val_loss</td><td>█▃▂▁▂▃▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>7</td></tr><tr><td>train_accuracy</td><td>57.86006</td></tr><tr><td>train_loss</td><td>1.7815</td></tr><tr><td>val_accuracy</td><td>39.40742</td></tr><tr><td>val_loss</td><td>3.82809</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">worthy-salad-2</strong> at: <a href='https://wandb.ai/kanakornmek-/predictive-keyboard-rnn-optimized/runs/bpydsxqi' target=\"_blank\">https://wandb.ai/kanakornmek-/predictive-keyboard-rnn-optimized/runs/bpydsxqi</a><br> View project at: <a href='https://wandb.ai/kanakornmek-/predictive-keyboard-rnn-optimized' target=\"_blank\">https://wandb.ai/kanakornmek-/predictive-keyboard-rnn-optimized</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/wandb/run-20251018_170516-bpydsxqi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 4. Configuration ---\n",
    "ZIP_FILE_PATH = 'train.zip'\n",
    "TEXT_FILE_NAME = 'train.src.tok'\n",
    "DEV_SET_PATH = 'dev_set.csv'\n",
    "SEQ_LENGTH = 20\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "N_LAYERS = 2\n",
    "\n",
    "# --- WANDB Initialization ---\n",
    "wandb.init(\n",
    "    project=\"predictive-keyboard-rnn-optimized\",\n",
    "    config={\n",
    "        \"learning_rate\": LEARNING_RATE, \"epochs\": EPOCHS, \"batch_size\": BATCH_SIZE,\n",
    "        \"seq_length\": SEQ_LENGTH, \"embedding_dim\": EMBEDDING_DIM, \"hidden_dim\": HIDDEN_DIM,\n",
    "        \"n_layers\": N_LAYERS, \"dataset\": \"train.src.tok (first 10000 lines)\",\n",
    "        \"architecture\": \"LSTM_with_first_letter\", \"optimization\": \"parallel_processing\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# --- Load Data ---\n",
    "if not os.path.exists(TEXT_FILE_NAME) or not os.path.exists(DEV_SET_PATH):\n",
    "    print(f\"Error: Make sure '{TEXT_FILE_NAME}' and '{DEV_SET_PATH}' are in the directory.\")\n",
    "else:\n",
    "    with open(TEXT_FILE_NAME) as f:\n",
    "        lines = f.read().splitlines()\n",
    "\n",
    "    if lines:\n",
    "        print(\"Limiting training data to the first 10000 lines for testing.\")\n",
    "        limited_lines = lines[:10000]\n",
    "        \n",
    "        # --- Prepare Datasets and DataLoaders ---\n",
    "        train_dataset = TextDataset(limited_lines, seq_length=SEQ_LENGTH)\n",
    "        val_dataset = DevSetDataset(DEV_SET_PATH, train_dataset.word_to_int, seq_length=SEQ_LENGTH)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE * 2, shuffle=False)\n",
    "        \n",
    "        with open('word_to_int.json', 'w') as f:\n",
    "            json.dump(train_dataset.word_to_int, f)\n",
    "        print(\"Vocabulary saved.\")\n",
    "\n",
    "        # --- Initialize Model ---\n",
    "        model = NextWordModel(train_dataset.vocab_size, EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS)\n",
    "        print(\"\\nModel Architecture:\")\n",
    "        print(model)\n",
    "        print(f\"\\nVocabulary Size: {train_dataset.vocab_size}\")\n",
    "        print(f\"Training sequences: {len(train_dataset)}\")\n",
    "        print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "        # --- Train Model ---\n",
    "        if len(train_dataset) > 0 and len(val_dataset) > 0:\n",
    "            print(\"\\nStarting optimized training...\")\n",
    "            train(model, train_loader, val_loader, train_dataset.vocab_size, \n",
    "                  epochs=EPOCHS, batch_size=BATCH_SIZE, lr=LEARNING_RATE)\n",
    "            print(\"\\nTraining complete.\")\n",
    "        else:\n",
    "            print(\"Not enough data to create training and/or validation sets.\")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Prediction and Evaluation Functions ---\n",
    "\n",
    "def load_model(model_path, vocab_size, embedding_dim=256, hidden_dim=512, n_layers=2):\n",
    "    \"\"\"\n",
    "    Load a trained model from file.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = NextWordModel(vocab_size, embedding_dim, hidden_dim, n_layers)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def predict_next_word(model, context_text, first_letter=None, word_to_int=None, int_to_word=None, seq_length=20, top_k=5):\n",
    "    \"\"\"\n",
    "    Predict the next word(s) given a context and optional first letter constraint.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained NextWordModel\n",
    "        context_text: String context (previous words)\n",
    "        first_letter: Optional first letter constraint (e.g., 't')\n",
    "        word_to_int: Vocabulary mapping\n",
    "        int_to_word: Reverse vocabulary mapping\n",
    "        seq_length: Maximum sequence length\n",
    "        top_k: Number of top predictions to return\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples (word, probability) sorted by probability\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Tokenize and convert context to integers\n",
    "        words = context_text.lower().split()\n",
    "        context_indices = [word_to_int.get(w, word_to_int['<UNK>']) for w in words]\n",
    "        \n",
    "        # Pad or truncate to seq_length\n",
    "        if len(context_indices) > seq_length:\n",
    "            context_indices = context_indices[-seq_length:]\n",
    "        else:\n",
    "            context_indices = [word_to_int['<PAD>']] * (seq_length - len(context_indices)) + context_indices\n",
    "        \n",
    "        # Convert to tensor\n",
    "        context_tensor = torch.tensor([context_indices]).to(device)\n",
    "        \n",
    "        # Handle first letter\n",
    "        if first_letter:\n",
    "            first_letter = first_letter.lower()\n",
    "            if 'a' <= first_letter <= 'z':\n",
    "                letter_idx = ord(first_letter) - ord('a')\n",
    "            else:\n",
    "                letter_idx = 26\n",
    "            letter_tensor = torch.tensor([letter_idx]).to(device)\n",
    "        else:\n",
    "            letter_tensor = None\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        hidden = model.init_hidden(1)\n",
    "        \n",
    "        # Get predictions\n",
    "        output, _ = model(context_tensor, hidden, first_letter=letter_tensor)\n",
    "        \n",
    "        # Apply softmax to get probabilities\n",
    "        probabilities = torch.nn.functional.softmax(output, dim=1).squeeze()\n",
    "        \n",
    "        # Get top k predictions\n",
    "        top_probs, top_indices = torch.topk(probabilities, top_k)\n",
    "        \n",
    "        # Convert indices to words\n",
    "        predictions = []\n",
    "        for i in range(top_k):\n",
    "            word = int_to_word[top_indices[i].item()]\n",
    "            prob = top_probs[i].item()\n",
    "            predictions.append((word, prob))\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "def evaluate_model(model, test_loader, int_to_word=None, device=None):\n",
    "    \"\"\"\n",
    "    Evaluate model performance on test set.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained NextWordModel\n",
    "        test_loader: DataLoader with test data\n",
    "        int_to_word: Reverse vocabulary mapping (index to word)\n",
    "        device: Device to run evaluation on\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    correct_with_first_letter = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (contexts, targets, first_letters) in enumerate(test_loader):\n",
    "            contexts, targets, first_letters = contexts.to(device), targets.to(device), first_letters.to(device)\n",
    "            \n",
    "            # Initialize hidden state\n",
    "            hidden = model.init_hidden(contexts.size(0))\n",
    "            \n",
    "            # Get predictions\n",
    "            output, _ = model(contexts, hidden, first_letter=first_letters)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(output, targets)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total_predictions += targets.size(0)\n",
    "            correct_predictions += (predicted == targets).sum().item()\n",
    "            \n",
    "            # Calculate accuracy with first letter constraint\n",
    "            if int_to_word is not None:\n",
    "                for i in range(contexts.size(0)):\n",
    "                    first_letter_idx = first_letters[i].item()\n",
    "                    true_answer = targets[i].item()\n",
    "                    predicted_answer = predicted[i].item()\n",
    "                    \n",
    "                    # Get actual first letter from index\n",
    "                    if first_letter_idx <= 25:  # a-z\n",
    "                        actual_letter = chr(first_letter_idx + ord('a'))\n",
    "                    else:  # other\n",
    "                        actual_letter = 'other'\n",
    "                    \n",
    "                    # Check if prediction matches the first letter constraint\n",
    "                    predicted_word = int_to_word.get(predicted_answer, '')\n",
    "                    if predicted_word and len(predicted_word) > 0:\n",
    "                        predicted_first_letter = predicted_word[0].lower()\n",
    "                        if predicted_first_letter == actual_letter:\n",
    "                            if predicted_answer == true_answer:\n",
    "                                correct_with_first_letter += 1\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = (100 * correct_predictions / total_predictions) if total_predictions > 0 else 0\n",
    "    accuracy_with_constraint = (100 * correct_with_first_letter / total_predictions) if total_predictions > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'loss': avg_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'accuracy_with_first_letter_constraint': accuracy_with_constraint,\n",
    "        'total_samples': total_predictions\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found trained model and vocabulary files.\n",
      "Evaluating model on dev_set.csv...\n",
      "\n",
      "=== Evaluation Results ===\n",
      "Loss: 3.5622\n",
      "Overall Accuracy: 38.02%\n",
      "Accuracy with First Letter Constraint: 31.47%\n",
      "Total Samples: 89912\n",
      "\n",
      "=== Example Predictions ===\n",
      "\n",
      "Context: 'states on monday warned north korea to avoid provoking trouble as pyongyang ' s most senior defector spent his sixth'\n",
      "First letter: 'd'\n",
      "True answer: 'day'\n",
      "Top 3 predictions: [('debate', 0.029785726219415665), ('decision', 0.022341500967741013), ('dispute', 0.019380083307623863)]\n",
      "\n",
      "Context: 'to drastically cut its car import duties , taiwan on thursday won european union support for its bid to enter'\n",
      "First letter: 't'\n",
      "True answer: 'the'\n",
      "Top 3 predictions: [('the', 0.9337925910949707), ('their', 0.021507205441594124), ('them', 0.013604961335659027)]\n",
      "\n",
      "Context: 'three soldiers were injured in a bombing ambush launched by suspect thai southern insurgents on wednesday'\n",
      "First letter: 'm'\n",
      "True answer: 'morning'\n",
      "Top 3 predictions: [('must', 0.18316957354545593), ('much', 0.13945569097995758), ('might', 0.08507082611322403)]\n",
      "\n",
      "Context: 'official says a yemeni investigation team has found that a yemeni airplane that crashed off the <UNK> islands last month'\n",
      "First letter: 'w'\n",
      "True answer: 'was'\n",
      "Top 3 predictions: [('with', 0.3285795748233795), ('were', 0.16937637329101562), ('was', 0.15407899022102356)]\n",
      "\n",
      "Context: '- ravaged west african country has made a firm commitment to continue with the peace talks in ghana , according'\n",
      "First letter: 't'\n",
      "True answer: 'to'\n",
      "Top 3 predictions: [('to', 0.9834355711936951), ('told', 0.005749549716711044), ('that', 0.0030684946104884148)]\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Model Evaluation ---\n",
    "\n",
    "# First, let's check if we have a trained model and vocabulary\n",
    "import os\n",
    "\n",
    "if os.path.exists('best_model.pth') and os.path.exists('word_to_int.json'):\n",
    "    print(\"Found trained model and vocabulary files.\")\n",
    "    \n",
    "    # Load vocabulary\n",
    "    with open('word_to_int.json', 'r') as f:\n",
    "        word_to_int = json.load(f)\n",
    "    \n",
    "    # Create reverse vocabulary mapping\n",
    "    int_to_word = {i: word for word, i in word_to_int.items()}\n",
    "    vocab_size = len(word_to_int)\n",
    "    \n",
    "    # Load the trained model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = load_model('best_model.pth', vocab_size, EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS)\n",
    "    \n",
    "    # Create validation dataset and loader\n",
    "    val_dataset = DevSetDataset(DEV_SET_PATH, word_to_int, seq_length=SEQ_LENGTH)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE * 2, shuffle=False)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    print(\"Evaluating model on dev_set.csv...\")\n",
    "    results = evaluate_model(model, val_loader, int_to_word=int_to_word, device=device)\n",
    "    \n",
    "    print(\"\\n=== Evaluation Results ===\")\n",
    "    print(f\"Loss: {results['loss']:.4f}\")\n",
    "    print(f\"Overall Accuracy: {results['accuracy']:.2f}%\")\n",
    "    print(f\"Accuracy with First Letter Constraint: {results['accuracy_with_first_letter_constraint']:.2f}%\")\n",
    "    print(f\"Total Samples: {results['total_samples']}\")\n",
    "    \n",
    "    # Example predictions\n",
    "    print(\"\\n=== Example Predictions ===\")\n",
    "    for i in range(min(5, len(val_dataset))):\n",
    "        context, answer, first_letter = val_dataset[i]\n",
    "        context_text = \" \".join([int_to_word[idx.item()] for idx in context if idx.item() != word_to_int['<PAD>']])\n",
    "        true_answer = int_to_word[answer.item()]\n",
    "        letter_hint = chr(first_letter.item() + ord('a')) if first_letter.item() <= 25 else 'other'\n",
    "        \n",
    "        predictions = predict_next_word(model, context_text, letter_hint, word_to_int, int_to_word, SEQ_LENGTH, top_k=3)\n",
    "        \n",
    "        print(f\"\\nContext: '{context_text}'\")\n",
    "        print(f\"First letter: '{letter_hint}'\")\n",
    "        print(f\"True answer: '{true_answer}'\")\n",
    "        print(f\"Top 3 predictions: {predictions}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No trained model found. Please train the model first by running the training cell.\")\n",
    "    print(\"To train the model, make sure you have:\")\n",
    "    print(\"1. train.zip file containing train.src.tok\")\n",
    "    print(\"2. dev_set.csv file\")\n",
    "    print(\"3. Run the training configuration cell (cell 6)\")\n",
    "    \n",
    "    # If you want to test the evaluation function without a trained model,\n",
    "    # you can uncomment the following code to create a dummy model:\n",
    "    \"\"\"\n",
    "    # Create dummy vocabulary and model for testing\n",
    "    dummy_word_to_int = {'<PAD>': 0, '<UNK>': 1, 'the': 2, 'a': 3, 'an': 4, 'and': 5}\n",
    "    dummy_int_to_word = {i: w for w, i in dummy_word_to_int.items()}\n",
    "    \n",
    "    # Create a dummy model\n",
    "    dummy_model = NextWordModel(len(dummy_word_to_int), EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS)\n",
    "    \n",
    "    # Create a small dummy dataset for testing\n",
    "    # This would require a CSV with words that exist in dummy_word_to_int\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
