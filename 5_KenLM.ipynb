{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# 5) KenLM with Kneser-Ney Smoothing\n",
    "\n",
    "This notebook implements a language model using KenLM, an optimized n-gram library with Modified Kneser-Ney smoothing.\n",
    "\n",
    "**Model Description:**\n",
    "- Uses KenLM library (highly optimized C++ implementation)\n",
    "- Modified Kneser-Ney smoothing (state-of-the-art for n-grams)\n",
    "- Supports 5-gram with proper backoff\n",
    "- Much faster and more memory-efficient than pure Python\n",
    "\n",
    "**Expected Performance:**\n",
    "- 10K data: ~25-30% accuracy\n",
    "- 100K data: ~45-50% accuracy\n",
    "- 1M data: ~55-60% accuracy\n",
    "- Full (3.8M) data: ~58-65% accuracy\n",
    "\n",
    "**Why KenLM?**\n",
    "- Modified Kneser-Ney > Simple MLE (our previous models)\n",
    "- Handles unseen n-grams much better\n",
    "- Industry standard for n-gram models\n",
    "- Used in Moses MT, speech recognition, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports_header",
   "metadata": {},
   "source": [
    "## 5.1 Setup and Imports\n",
    "\n",
    "First, we need to install KenLM. This requires:\n",
    "1. Installing the KenLM library\n",
    "2. Building the language model with `lmplz`\n",
    "3. Querying the model with Python bindings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "install_kenlm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KenLM installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install KenLM\n",
    "# This may take 2-3 minutes\n",
    "!pip install https://github.com/kpu/kenlm/archive/master.zip -q\n",
    "\n",
    "print(\"KenLM installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful!\n",
      "KenLM version: installed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import kenlm\n",
    "import os\n",
    "import subprocess\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(f\"KenLM version: {kenlm.__version__ if hasattr(kenlm, '__version__') else 'installed'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_data_header",
   "metadata": {},
   "source": [
    "## 5.2 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "load_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Total training sentences: 3,803,957\n",
      "\n",
      "Loading development set...\n",
      "Development set size: 94,825 predictions\n",
      "Columns: ['context', 'first letter', 'answer']\n",
      "\n",
      "Sample dev set entries:\n",
      "                                             context first letter   answer\n",
      "0  south korea and the united states on monday wa...            d      day\n",
      "1  after agreeing to drastically cut its car impo...            t      the\n",
      "2  three soldiers were injured in a bombing ambus...            m  morning\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "print(\"Loading training data...\")\n",
    "with open('train.src.tok', 'r', encoding='utf-8') as f:\n",
    "    train_lines = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print(f\"Total training sentences: {len(train_lines):,}\")\n",
    "\n",
    "# Load dev set\n",
    "print(\"\\nLoading development set...\")\n",
    "dev_df = pd.read_csv('dev_set.csv')\n",
    "print(f\"Development set size: {len(dev_df):,} predictions\")\n",
    "print(f\"Columns: {list(dev_df.columns)}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample dev set entries:\")\n",
    "print(dev_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_sampling_header",
   "metadata": {},
   "source": [
    "## 5.3 Data Sampling\n",
    "\n",
    "We'll use simple sequential sampling (first N sentences) as decided in EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "data_sampling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using debug dataset: 10,000 sentences\n",
      "\n",
      "First 3 training sentences:\n",
      "1. australia ' s current account deficit shrunk by a record 1 . 11 billion dollars - lrb - 1 . 11 billion us - rrb - in the june quarter due to soaring commodity prices , figures released monday showed .\n",
      "2. at least two people were killed in a suspected bomb attack on a passenger bus in the strife - torn southern philippines on monday , the military said .\n",
      "3. australian shares closed down 1 . 1 percent monday following a weak lead from the united states and lower commodity prices , dealers said .\n"
     ]
    }
   ],
   "source": [
    "# Data sizes for experiments\n",
    "DATA_SIZES = {\n",
    "    'debug': 10_000,\n",
    "    'dev': 100_000,\n",
    "    'large': 1_000_000,\n",
    "    'full': 3_803_957\n",
    "}\n",
    "\n",
    "def sample_data(train_lines: List[str], size_key: str = 'debug') -> List[str]:\n",
    "    \"\"\"\n",
    "    Sample training data sequentially (simple, no shuffling).\n",
    "    \n",
    "    Args:\n",
    "        train_lines: Full training corpus\n",
    "        size_key: One of 'debug', 'dev', 'large', 'full'\n",
    "    \n",
    "    Returns:\n",
    "        First N sentences from corpus\n",
    "    \"\"\"\n",
    "    size = DATA_SIZES[size_key]\n",
    "    if size >= len(train_lines):\n",
    "        return train_lines\n",
    "    return train_lines[:size]\n",
    "\n",
    "# Start with debug size (10K) for fast testing\n",
    "# Change to 'dev', 'large', or 'full' later\n",
    "CURRENT_SIZE = 'debug'\n",
    "# CURRENT_SIZE = 'dev'\n",
    "# CURRENT_SIZE = 'large'\n",
    "# CURRENT_SIZE = 'full'\n",
    "\n",
    "train_data = sample_data(train_lines, CURRENT_SIZE)\n",
    "print(f\"Using {CURRENT_SIZE} dataset: {len(train_data):,} sentences\")\n",
    "print(f\"\\nFirst 3 training sentences:\")\n",
    "for i, sent in enumerate(train_data[:3]):\n",
    "    print(f\"{i+1}. {sent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepare_data_header",
   "metadata": {},
   "source": [
    "## 5.4 Prepare Training Data for KenLM\n",
    "\n",
    "KenLM requires:\n",
    "1. Text file with one sentence per line\n",
    "2. We'll add sentence boundaries `<s>` and `</s>` (KenLM adds these automatically, but we can control it)\n",
    "3. Build model using `lmplz` command-line tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "prepare_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing training data to kenlm_train_debug.txt...\n",
      "Saved 10,000 sentences to kenlm_train_debug.txt\n",
      "File size: 1.75 MB\n"
     ]
    }
   ],
   "source": [
    "# Save training data to file for KenLM\n",
    "train_file = f'kenlm_train_{CURRENT_SIZE}.txt'\n",
    "\n",
    "print(f\"Writing training data to {train_file}...\")\n",
    "with open(train_file, 'w', encoding='utf-8') as f:\n",
    "    for sentence in train_data:\n",
    "        # KenLM automatically adds <s> and </s>, so we just write the sentence\n",
    "        f.write(sentence + '\\n')\n",
    "\n",
    "print(f\"Saved {len(train_data):,} sentences to {train_file}\")\n",
    "print(f\"File size: {os.path.getsize(train_file) / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "build_model_header",
   "metadata": {},
   "source": [
    "## 5.5 Build KenLM Model\n",
    "\n",
    "We'll use `lmplz` to build a 5-gram model with Modified Kneser-Ney smoothing.\n",
    "\n",
    "**Command explanation:**\n",
    "- `-o 5`: Build 5-gram model\n",
    "- `--discount_fallback`: Handle edge cases in discounting\n",
    "- `-S 80%`: Use 80% of RAM for sorting (adjust if needed)\n",
    "- `-T /tmp`: Use /tmp for temporary files\n",
    "\n",
    "**Note:** This takes:\n",
    "- 10K: ~5-10 seconds\n",
    "- 100K: ~30-60 seconds\n",
    "- 1M: ~5-10 minutes\n",
    "- Full: ~30-60 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "build_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building 5-gram KenLM model...\n",
      "This may take a while for large datasets...\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'lmplz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(train_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m input_f:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(model_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m output_f:\n\u001b[0;32m---> 20\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstdin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_f\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_f\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/subprocess.py:548\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstdout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[1;32m    546\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[0;32m--> 548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    550\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mcommunicate(\u001b[38;5;28minput\u001b[39m, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[1;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m   1024\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/subprocess.py:1950\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session, process_group)\u001b[0m\n\u001b[1;32m   1948\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errno_num \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1949\u001b[0m         err_msg \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1950\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1951\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'lmplz'"
     ]
    }
   ],
   "source": [
    "# Build KenLM model\n",
    "model_file = f'kenlm_{CURRENT_SIZE}_5gram.arpa'\n",
    "\n",
    "print(f\"Building 5-gram KenLM model...\")\n",
    "print(f\"This may take a while for large datasets...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Build model using lmplz\n",
    "cmd = [\n",
    "    'lmplz',\n",
    "    '-o', '5',  # 5-gram\n",
    "    '--discount_fallback',\n",
    "    '-S', '80%',  # Use 80% RAM\n",
    "    '-T', '/tmp',  # Temp directory\n",
    "]\n",
    "\n",
    "with open(train_file, 'r') as input_f:\n",
    "    with open(model_file, 'w') as output_f:\n",
    "        result = subprocess.run(\n",
    "            cmd,\n",
    "            stdin=input_f,\n",
    "            stdout=output_f,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True\n",
    "        )\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(f\"✓ Model built successfully in {elapsed:.2f} seconds\")\n",
    "    print(f\"Model saved to: {model_file}\")\n",
    "    print(f\"Model size: {os.path.getsize(model_file) / (1024*1024):.2f} MB\")\n",
    "else:\n",
    "    print(f\"✗ Error building model:\")\n",
    "    print(result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_model_header",
   "metadata": {},
   "source": [
    "## 5.6 Load KenLM Model\n",
    "\n",
    "Now we'll load the model into Python for querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "print(f\"Loading KenLM model from {model_file}...\")\n",
    "model = kenlm.Model(model_file)\n",
    "\n",
    "print(f\"✓ Model loaded successfully!\")\n",
    "print(f\"Model order: {model.order}\")\n",
    "\n",
    "# Test the model\n",
    "test_sentence = \"the president of the united states\"\n",
    "score = model.score(test_sentence, bos=True, eos=True)\n",
    "perplexity = model.perplexity(test_sentence)\n",
    "\n",
    "print(f\"\\nTest sentence: '{test_sentence}'\")\n",
    "print(f\"Log10 probability: {score:.4f}\")\n",
    "print(f\"Perplexity: {perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "build_vocab_header",
   "metadata": {},
   "source": [
    "## 5.7 Build Vocabulary Index\n",
    "\n",
    "We need to index vocabulary by first letter for efficient candidate filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build_vocab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary from training data\n",
    "print(\"Building vocabulary index...\")\n",
    "\n",
    "vocab_by_first_char = defaultdict(set)\n",
    "\n",
    "for sentence in tqdm(train_data, desc=\"Indexing vocabulary\"):\n",
    "    tokens = sentence.split()\n",
    "    for token in tokens:\n",
    "        first_char = token[0]\n",
    "        vocab_by_first_char[first_char].add(token)\n",
    "\n",
    "print(f\"\\nVocabulary indexed by first character:\")\n",
    "print(f\"Total unique words: {sum(len(words) for words in vocab_by_first_char.values()):,}\")\n",
    "print(f\"Number of first characters: {len(vocab_by_first_char)}\")\n",
    "\n",
    "# Show sample\n",
    "print(f\"\\nSample: Words starting with 'a': {len(vocab_by_first_char['a']):,}\")\n",
    "print(f\"Sample: Words starting with 't': {len(vocab_by_first_char['t']):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prediction_header",
   "metadata": {},
   "source": [
    "## 5.8 Prediction Function\n",
    "\n",
    "We'll score all candidate words and select the one with highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prediction_function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(context: str, first_letter: str, model: kenlm.Model, vocab_by_first_char: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Predict next word given context and first letter constraint.\n",
    "    \n",
    "    Args:\n",
    "        context: Previous words as string (e.g., \"the cat sat on the\")\n",
    "        first_letter: Required first character of prediction\n",
    "        model: KenLM model\n",
    "        vocab_by_first_char: Dictionary mapping first char to set of words\n",
    "    \n",
    "    Returns:\n",
    "        Predicted word (most likely word starting with first_letter)\n",
    "    \"\"\"\n",
    "    # Get candidate words\n",
    "    candidates = vocab_by_first_char.get(first_letter, set())\n",
    "    \n",
    "    if not candidates:\n",
    "        # No words in vocabulary start with this letter\n",
    "        return first_letter\n",
    "    \n",
    "    # Score each candidate\n",
    "    best_word = None\n",
    "    best_score = float('-inf')\n",
    "    \n",
    "    for word in candidates:\n",
    "        # Create full sentence with candidate word\n",
    "        full_sentence = context + ' ' + word if context else word\n",
    "        \n",
    "        # Score with KenLM\n",
    "        # We use bos=True to add <s>, eos=False since we're predicting next word\n",
    "        score = model.score(full_sentence, bos=True, eos=False)\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_word = word\n",
    "    \n",
    "    return best_word if best_word else first_letter\n",
    "\n",
    "print(\"Prediction function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test_predictions_header",
   "metadata": {},
   "source": [
    "## 5.9 Test Predictions\n",
    "\n",
    "Let's test on a few manual examples before evaluating on dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test examples\n",
    "test_cases = [\n",
    "    (\"the cat sat on the\", \"m\"),  # mat?\n",
    "    (\"president of the united\", \"s\"),  # states?\n",
    "    (\"new york\", \"c\"),  # city?\n",
    "    (\"in the\", \"m\"),  # morning? middle?\n",
    "    (\"on\", \"m\"),  # monday?\n",
    "]\n",
    "\n",
    "print(\"Testing predictions:\\n\")\n",
    "for context, first_letter in test_cases:\n",
    "    prediction = predict(context, first_letter, model, vocab_by_first_char)\n",
    "    print(f\"Context: '{context}'\")\n",
    "    print(f\"First letter: '{first_letter}'\")\n",
    "    print(f\"Prediction: {prediction}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluate_header",
   "metadata": {},
   "source": [
    "## 5.10 Evaluate on Dev Set\n",
    "\n",
    "Now let's evaluate on the full development set (all 94,825 examples).\n",
    "\n",
    "**Note:** This may take 5-10 minutes depending on dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate_dev",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on dev set\n",
    "print(f\"\\nEvaluating on development set...\")\n",
    "\n",
    "# For testing, you can limit examples\n",
    "max_examples = None  # Set to 1000 for quick testing\n",
    "eval_df = dev_df.head(max_examples) if max_examples else dev_df\n",
    "\n",
    "correct = 0\n",
    "total = len(eval_df)\n",
    "predictions = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for idx, row in tqdm(eval_df.iterrows(), total=total, desc=\"Predicting\"):\n",
    "    context = row['context']\n",
    "    first_letter = row['first letter']\n",
    "    answer = row['answer']\n",
    "    \n",
    "    # Predict\n",
    "    prediction = predict(context, first_letter, model, vocab_by_first_char)\n",
    "    predictions.append(prediction)\n",
    "    \n",
    "    # Check correctness\n",
    "    if prediction == answer:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / total\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Total examples: {total:,}\")\n",
    "print(f\"  Correct: {correct:,}\")\n",
    "print(f\"  Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"  Time: {elapsed:.2f} seconds ({elapsed/total*1000:.2f} ms/prediction)\")\n",
    "\n",
    "print(f\"\\nExpected accuracy for {CURRENT_SIZE} dataset: ~25-30%\")\n",
    "print(f\"Actual accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "error_analysis_header",
   "metadata": {},
   "source": [
    "## 5.11 Error Analysis\n",
    "\n",
    "Let's look at some examples where the model got it right vs wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "error_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predictions to dev_df\n",
    "dev_sample = eval_df.copy()\n",
    "dev_sample['prediction'] = predictions\n",
    "dev_sample['correct'] = dev_sample['prediction'] == dev_sample['answer']\n",
    "\n",
    "# Show correct predictions\n",
    "print(\"=\"*80)\n",
    "print(\"CORRECT PREDICTIONS (Sample of 5)\")\n",
    "print(\"=\"*80)\n",
    "correct_samples = dev_sample[dev_sample['correct']].head(5)\n",
    "for idx, row in correct_samples.iterrows():\n",
    "    print(f\"\\nContext: {row['context']}\")\n",
    "    print(f\"First letter: '{row['first letter']}'\")\n",
    "    print(f\"Prediction: {row['prediction']}\")\n",
    "    print(f\"Answer: {row['answer']}\")\n",
    "    print(f\"✓ CORRECT\")\n",
    "\n",
    "# Show incorrect predictions\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INCORRECT PREDICTIONS (Sample of 5)\")\n",
    "print(\"=\"*80)\n",
    "incorrect_samples = dev_sample[~dev_sample['correct']].head(5)\n",
    "for idx, row in incorrect_samples.iterrows():\n",
    "    print(f\"\\nContext: {row['context']}\")\n",
    "    print(f\"First letter: '{row['first letter']}'\")\n",
    "    print(f\"Prediction: {row['prediction']}\")\n",
    "    print(f\"Answer: {row['answer']}\")\n",
    "    print(f\"✗ INCORRECT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scaling_header",
   "metadata": {},
   "source": [
    "## 5.12 Scaling Experiments\n",
    "\n",
    "Now let's see how accuracy changes with more training data.\n",
    "\n",
    "**Note:** This will take progressively longer:\n",
    "- 10K: ~1-2 minutes total\n",
    "- 100K: ~5-10 minutes total\n",
    "- 1M: ~20-30 minutes total\n",
    "- Full (3.8M): ~1-2 hours total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "scaling_experiments",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING ON DEBUG DATASET (10,000 sentences)\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'lmplz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(train_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m input_f:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(model_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m output_f:\n\u001b[0;32m---> 32\u001b[0m         \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Load model\u001b[39;00m\n\u001b[1;32m     35\u001b[0m model \u001b[38;5;241m=\u001b[39m kenlm\u001b[38;5;241m.\u001b[39mModel(model_file)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/subprocess.py:548\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstdout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[1;32m    546\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[0;32m--> 548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    550\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mcommunicate(\u001b[38;5;28minput\u001b[39m, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[1;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m   1024\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/subprocess.py:1950\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session, process_group)\u001b[0m\n\u001b[1;32m   1948\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errno_num \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1949\u001b[0m         err_msg \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1950\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1951\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'lmplz'"
     ]
    }
   ],
   "source": [
    "# Run experiments on different data sizes\n",
    "# Uncomment to run scaling experiments\n",
    "sizes_to_test = [\n",
    "    'debug',   # 10K\n",
    "    # 'dev',     # 100K\n",
    "    # 'large',   # 1M\n",
    "    # 'full',    # 3.8M\n",
    "]\n",
    "\n",
    "scaling_results = []\n",
    "\n",
    "for size_key in sizes_to_test:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"TRAINING ON {size_key.upper()} DATASET ({DATA_SIZES[size_key]:,} sentences)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Sample data\n",
    "    data = sample_data(train_lines, size_key)\n",
    "    \n",
    "    # Prepare training file\n",
    "    train_file = f'kenlm_train_{size_key}.txt'\n",
    "    with open(train_file, 'w', encoding='utf-8') as f:\n",
    "        for sentence in data:\n",
    "            f.write(sentence + '\\n')\n",
    "    \n",
    "    # Build model\n",
    "    model_file = f'kenlm_{size_key}_5gram.arpa'\n",
    "    cmd = ['lmplz', '-o', '5', '--discount_fallback', '-S', '80%', '-T', '/tmp']\n",
    "    \n",
    "    with open(train_file, 'r') as input_f:\n",
    "        with open(model_file, 'w') as output_f:\n",
    "            subprocess.run(cmd, stdin=input_f, stdout=output_f, stderr=subprocess.PIPE)\n",
    "    \n",
    "    # Load model\n",
    "    model = kenlm.Model(model_file)\n",
    "    \n",
    "    # Build vocabulary\n",
    "    vocab = defaultdict(set)\n",
    "    for sentence in data:\n",
    "        for token in sentence.split():\n",
    "            vocab[token[0]].add(token)\n",
    "    \n",
    "    # Evaluate (use sample for speed)\n",
    "    eval_sample = dev_df.head(1000)  # Use 1000 for faster testing\n",
    "    correct = 0\n",
    "    for idx, row in eval_sample.iterrows():\n",
    "        pred = predict(row['context'], row['first letter'], model, vocab)\n",
    "        if pred == row['answer']:\n",
    "            correct += 1\n",
    "    \n",
    "    accuracy = correct / len(eval_sample)\n",
    "    \n",
    "    # Store results\n",
    "    scaling_results.append({\n",
    "        'size': size_key,\n",
    "        'num_sentences': len(data),\n",
    "        'accuracy': accuracy\n",
    "    })\n",
    "\n",
    "# Show summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SCALING RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Dataset':<15} {'# Sentences':<15} {'Accuracy':<15}\")\n",
    "print(\"-\"*45)\n",
    "for result in scaling_results:\n",
    "    print(f\"{result['size']:<15} {result['num_sentences']:<15,} {result['accuracy']*100:<14.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next_steps_header",
   "metadata": {},
   "source": [
    "## 5.13 Next Steps\n",
    "\n",
    "**Current Status:**\n",
    "- ✅ Trigram model implemented (58.12% on full data)\n",
    "- ✅ 4-gram model implemented\n",
    "- ✅ 5-gram model implemented\n",
    "- ✅ KenLM with Kneser-Ney implemented\n",
    "- ✅ Best n-gram baseline complete\n",
    "\n",
    "**KenLM vs Our Models:**\n",
    "- Our 5-gram: Simple MLE with backoff\n",
    "- KenLM: Modified Kneser-Ney smoothing (better generalization)\n",
    "- Expected improvement: +3-8% over our 5-gram\n",
    "\n",
    "**To improve further:**\n",
    "\n",
    "1. **Neural Models**: LSTM/GRU\n",
    "   - Expected: 60-70% accuracy\n",
    "   - Can capture longer dependencies\n",
    "\n",
    "2. **Fine-tune GPT-2**: Transformer-based\n",
    "   - Expected: 65-75% accuracy\n",
    "   - Best single model performance\n",
    "\n",
    "3. **Ensemble**: Combine KenLM + LSTM + GPT-2\n",
    "   - Expected: 70-80% accuracy\n",
    "   - Weighted voting or stacking\n",
    "\n",
    "**Next notebook:**\n",
    "- `6_LSTM.ipynb` - Implement LSTM from scratch\n",
    "- Or `7_GPT2.ipynb` - Fine-tune GPT-2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
