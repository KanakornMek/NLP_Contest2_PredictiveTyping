{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8) Bi-LSTM with N-gram Features v2 (Train on train.src.tok, Validate on dev_set.csv)\n",
    "\n",
    "**Research Paper**: \"Enhancing Bangla Language Next Word Prediction...\" (arXiv 2405.01873, 2024)\n",
    "\n",
    "**Expected Accuracy**: 60-75% (realistic: 60-70%, optimistic: 75%)\n",
    "\n",
    "## Difference from v1\n",
    "\n",
    "- **Training**: Still trains on train.src.tok\n",
    "- **Validation**: Computes loss and accuracy on dev_set.csv after each epoch\n",
    "- **Purpose**: Monitor performance on actual task format during training\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements a Bidirectional LSTM that combines neural and statistical approaches:\n",
    "- **Bidirectional LSTM**: Reads context both forward and backward\n",
    "- **N-gram Features**: Incorporates statistical n-gram probabilities\n",
    "- **Hybrid Architecture**: Best of both worlds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Dict, List, Tuple\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import wandb\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Login to wandb\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"WANDB LOGIN\")\n",
    "print(\"=\"*80)\n",
    "print(\"Please login to wandb to track your experiments.\")\n",
    "print(\"Get your API key from: https://wandb.ai/authorize\")\n",
    "print()\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "print(\"\\nâœ“ wandb login successful!\")\n",
    "print(\"You can view your runs at: https://wandb.ai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Load Training Data and Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "print(\"Loading training data...\")\n",
    "with open('train.src.tok', 'r', encoding='utf-8') as f:\n",
    "    train_sentences = [line.strip() for line in f]\n",
    "\n",
    "print(f\"Loaded {len(train_sentences)} training sentences\")\n",
    "print(f\"First 3 sentences:\")\n",
    "for i in range(3):\n",
    "    print(f\"{i+1}: {train_sentences[i]}\")\n",
    "\n",
    "# Build vocabulary\n",
    "print(\"\\nBuilding vocabulary...\")\n",
    "word_counts = defaultdict(int)\n",
    "for sentence in tqdm(train_sentences, desc=\"Counting words\"):\n",
    "    for word in sentence.split():\n",
    "        word_counts[word] += 1\n",
    "\n",
    "# Create word2idx and idx2word\n",
    "vocab = ['<PAD>', '<UNK>', '<s>', '</s>'] + sorted(word_counts.keys(), key=word_counts.get, reverse=True)\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"Most common words: {vocab[4:14]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Build N-gram Models\n",
    "\n",
    "Extract n-gram statistics for use as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building n-gram models...\")\n",
    "\n",
    "# Initialize count dictionaries\n",
    "unigram_counts = defaultdict(int)\n",
    "bigram_counts = defaultdict(int)\n",
    "trigram_counts = defaultdict(int)\n",
    "fourgram_counts = defaultdict(int)\n",
    "\n",
    "# Count n-grams from full dataset (remove slice for full dataset)\n",
    "for sentence in tqdm(train_sentences[:1000000], desc=\"Extracting n-grams\"):\n",
    "    words = ['<s>', '<s>', '<s>'] + sentence.split() + ['</s>']\n",
    "    \n",
    "    for i in range(3, len(words)):\n",
    "        # Unigram\n",
    "        unigram_counts[words[i]] += 1\n",
    "        \n",
    "        # Bigram\n",
    "        bigram = (words[i-1], words[i])\n",
    "        bigram_counts[bigram] += 1\n",
    "        \n",
    "        # Trigram\n",
    "        trigram = (words[i-2], words[i-1], words[i])\n",
    "        trigram_counts[trigram] += 1\n",
    "        \n",
    "        # 4-gram\n",
    "        fourgram = (words[i-3], words[i-2], words[i-1], words[i])\n",
    "        fourgram_counts[fourgram] += 1\n",
    "\n",
    "print(f\"\\nN-gram statistics:\")\n",
    "print(f\"Unique unigrams: {len(unigram_counts)}\")\n",
    "print(f\"Unique bigrams: {len(bigram_counts)}\")\n",
    "print(f\"Unique trigrams: {len(trigram_counts)}\")\n",
    "print(f\"Unique 4-grams: {len(fourgram_counts)}\")\n",
    "\n",
    "# Save n-gram models\n",
    "print(\"\\nSaving n-gram models...\")\n",
    "with open('ngram_models_v2.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'unigram': unigram_counts,\n",
    "        'bigram': bigram_counts,\n",
    "        'trigram': trigram_counts,\n",
    "        'fourgram': fourgram_counts\n",
    "    }, f)\n",
    "print(\"N-gram models saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 Load Development Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load development set\n",
    "dev_df = pd.read_csv('dev_set.csv')\n",
    "print(f\"Development set loaded: {len(dev_df)} examples\")\n",
    "print(f\"\\nFirst 3 examples:\")\n",
    "print(dev_df.head(3))\n",
    "\n",
    "# Build vocabulary by first letter\n",
    "print(\"\\nBuilding vocabulary by first letter...\")\n",
    "vocab_by_first_letter = defaultdict(set)\n",
    "for word in vocab:\n",
    "    if word not in ['<PAD>', '<UNK>', '<s>', '</s>'] and len(word) > 0:\n",
    "        vocab_by_first_letter[word[0].lower()].add(word)\n",
    "\n",
    "print(f\"Vocabulary organized by {len(vocab_by_first_letter)} first letters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5 Bi-LSTM with N-gram Features Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_Ngram(nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional LSTM with N-gram features for next word prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, embedding_dim: int = 256, \n",
    "                 hidden_dim: int = 512, num_layers: int = 2, \n",
    "                 dropout: float = 0.3, ngram_feature_dim: int = 4):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: Size of vocabulary\n",
    "            embedding_dim: Dimension of word embeddings\n",
    "            hidden_dim: Dimension of LSTM hidden state\n",
    "            num_layers: Number of LSTM layers\n",
    "            dropout: Dropout rate\n",
    "            ngram_feature_dim: Number of n-gram features (unigram, bigram, trigram, 4-gram)\n",
    "        \"\"\"\n",
    "        super(BiLSTM_Ngram, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Bidirectional LSTM\n",
    "        self.bilstm = nn.LSTM(\n",
    "            embedding_dim, \n",
    "            hidden_dim, \n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Combine BiLSTM output (2 * hidden_dim) with n-gram features\n",
    "        self.fc = nn.Linear(2 * hidden_dim + ngram_feature_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, ngram_features: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Input token indices, shape (batch_size, seq_len)\n",
    "            ngram_features: N-gram probability features, shape (batch_size, ngram_feature_dim)\n",
    "            \n",
    "        Returns:\n",
    "            Output logits, shape (batch_size, vocab_size)\n",
    "        \"\"\"\n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        # Bidirectional LSTM\n",
    "        lstm_out, (h_n, c_n) = self.bilstm(embedded)\n",
    "        # lstm_out: (batch_size, seq_len, 2 * hidden_dim)\n",
    "        # h_n: (2 * num_layers, batch_size, hidden_dim)\n",
    "        \n",
    "        # Take the last output\n",
    "        last_output = lstm_out[:, -1, :]  # (batch_size, 2 * hidden_dim)\n",
    "        \n",
    "        # Concatenate with n-gram features\n",
    "        combined = torch.cat([last_output, ngram_features], dim=1)\n",
    "        # combined: (batch_size, 2 * hidden_dim + ngram_feature_dim)\n",
    "        \n",
    "        # Final prediction\n",
    "        output = self.fc(combined)  # (batch_size, vocab_size)\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\"BiLSTM_Ngram model defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.6 Dataset Class with N-gram Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset that provides sequences with n-gram features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sentences: List[str], word2idx: Dict, \n",
    "                 ngram_models: Dict, max_len: int = 50):\n",
    "        self.sentences = sentences\n",
    "        self.word2idx = word2idx\n",
    "        self.ngram_models = ngram_models\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def compute_ngram_features(self, context: List[str], target_word: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute n-gram probability features.\n",
    "        \n",
    "        Returns:\n",
    "            Array of [unigram_prob, bigram_prob, trigram_prob, fourgram_prob]\n",
    "        \"\"\"\n",
    "        features = np.zeros(4)\n",
    "        \n",
    "        # Unigram probability\n",
    "        unigram_count = self.ngram_models['unigram'].get(target_word, 0)\n",
    "        total_unigrams = sum(self.ngram_models['unigram'].values())\n",
    "        features[0] = unigram_count / total_unigrams if total_unigrams > 0 else 0\n",
    "        \n",
    "        if len(context) >= 1:\n",
    "            # Bigram probability\n",
    "            bigram = (context[-1], target_word)\n",
    "            bigram_count = self.ngram_models['bigram'].get(bigram, 0)\n",
    "            context_count = self.ngram_models['unigram'].get(context[-1], 0)\n",
    "            features[1] = bigram_count / context_count if context_count > 0 else 0\n",
    "        \n",
    "        if len(context) >= 2:\n",
    "            # Trigram probability\n",
    "            trigram = (context[-2], context[-1], target_word)\n",
    "            trigram_count = self.ngram_models['trigram'].get(trigram, 0)\n",
    "            bigram_context = (context[-2], context[-1])\n",
    "            bigram_context_count = self.ngram_models['bigram'].get(bigram_context, 0)\n",
    "            features[2] = trigram_count / bigram_context_count if bigram_context_count > 0 else 0\n",
    "        \n",
    "        if len(context) >= 3:\n",
    "            # 4-gram probability\n",
    "            fourgram = (context[-3], context[-2], context[-1], target_word)\n",
    "            fourgram_count = self.ngram_models['fourgram'].get(fourgram, 0)\n",
    "            trigram_context = (context[-3], context[-2], context[-1])\n",
    "            trigram_context_count = self.ngram_models['trigram'].get(trigram_context, 0)\n",
    "            features[3] = fourgram_count / trigram_context_count if trigram_context_count > 0 else 0\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        words = ['<s>'] + sentence.split()\n",
    "        \n",
    "        # Randomly select a position to predict\n",
    "        if len(words) < 2:\n",
    "            return self.__getitem__((idx + 1) % len(self.sentences))\n",
    "        \n",
    "        target_pos = np.random.randint(1, len(words))\n",
    "        context_words = words[:target_pos]\n",
    "        target_word = words[target_pos]\n",
    "        \n",
    "        # Convert to indices\n",
    "        context_indices = [self.word2idx.get(w, self.word2idx['<UNK>']) for w in context_words]\n",
    "        target_idx = self.word2idx.get(target_word, self.word2idx['<UNK>'])\n",
    "        \n",
    "        # Pad/truncate context\n",
    "        if len(context_indices) > self.max_len:\n",
    "            context_indices = context_indices[-self.max_len:]\n",
    "            context_words = context_words[-self.max_len:]\n",
    "        else:\n",
    "            padding = [0] * (self.max_len - len(context_indices))\n",
    "            context_indices = padding + context_indices\n",
    "        \n",
    "        # Compute n-gram features\n",
    "        ngram_features = self.compute_ngram_features(context_words, target_word)\n",
    "        \n",
    "        return {\n",
    "            'context': torch.tensor(context_indices, dtype=torch.long),\n",
    "            'ngram_features': torch.tensor(ngram_features, dtype=torch.float32),\n",
    "            'target': torch.tensor(target_idx, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print(\"NgramDataset class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.7 Dev Set Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ngram_features_for_candidate(context_words: List[str], candidate: str, ngram_models: Dict) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute n-gram features for a candidate word given context.\n",
    "    \"\"\"\n",
    "    features = np.zeros(4, dtype=np.float32)  # Specify dtype for efficiency\n",
    "    \n",
    "    # Unigram\n",
    "    unigram_count = ngram_models['unigram'].get(candidate, 0)\n",
    "    total_unigrams = sum(ngram_models['unigram'].values())\n",
    "    features[0] = unigram_count / total_unigrams if total_unigrams > 0 else 0\n",
    "    \n",
    "    if len(context_words) >= 1:\n",
    "        # Bigram\n",
    "        bigram = (context_words[-1], candidate)\n",
    "        bigram_count = ngram_models['bigram'].get(bigram, 0)\n",
    "        context_count = ngram_models['unigram'].get(context_words[-1], 0)\n",
    "        features[1] = bigram_count / context_count if context_count > 0 else 0\n",
    "    \n",
    "    if len(context_words) >= 2:\n",
    "        # Trigram\n",
    "        trigram = (context_words[-2], context_words[-1], candidate)\n",
    "        trigram_count = ngram_models['trigram'].get(trigram, 0)\n",
    "        bigram_context = (context_words[-2], context_words[-1])\n",
    "        bigram_context_count = ngram_models['bigram'].get(bigram_context, 0)\n",
    "        features[2] = trigram_count / bigram_context_count if bigram_context_count > 0 else 0\n",
    "    \n",
    "    if len(context_words) >= 3:\n",
    "        # 4-gram\n",
    "        fourgram = (context_words[-3], context_words[-2], context_words[-1], candidate)\n",
    "        fourgram_count = ngram_models['fourgram'].get(fourgram, 0)\n",
    "        trigram_context = (context_words[-3], context_words[-2], context_words[-1])\n",
    "        trigram_context_count = ngram_models['trigram'].get(trigram_context, 0)\n",
    "        features[3] = fourgram_count / trigram_context_count if trigram_context_count > 0 else 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def evaluate_on_dev_set(model, dev_df, word2idx, vocab_by_first_letter, ngram_models, max_len, device, criterion):\n",
    "    \"\"\"\n",
    "    Evaluate model on dev_set.csv and compute both loss and accuracy.\n",
    "    \n",
    "    Returns:\n",
    "        (average_loss, accuracy)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _, row in dev_df.iterrows():\n",
    "            context = row['context']\n",
    "            first_letter = row['first letter']\n",
    "            answer = row['answer']\n",
    "            \n",
    "            # Tokenize context\n",
    "            words = ['<s>'] + context.lower().split()\n",
    "            \n",
    "            # Convert to indices\n",
    "            context_indices = [word2idx.get(w, word2idx['<UNK>']) for w in words]\n",
    "            \n",
    "            # Pad/truncate\n",
    "            if len(context_indices) > max_len:\n",
    "                context_indices = context_indices[-max_len:]\n",
    "                words = words[-max_len:]\n",
    "            else:\n",
    "                padding = [0] * (max_len - len(context_indices))\n",
    "                context_indices = padding + context_indices\n",
    "            \n",
    "            # Get candidates\n",
    "            candidates = vocab_by_first_letter.get(first_letter.lower(), set())\n",
    "            if not candidates:\n",
    "                continue\n",
    "            \n",
    "            context_tensor = torch.tensor([context_indices], dtype=torch.long).to(device)\n",
    "            \n",
    "            # Find best candidate and compute loss for answer\n",
    "            best_score = float('-inf')\n",
    "            best_word = None\n",
    "            answer_loss = None\n",
    "            \n",
    "            for candidate in candidates:\n",
    "                # Compute n-gram features (returns numpy array)\n",
    "                ngram_features = compute_ngram_features_for_candidate(words, candidate, ngram_models)\n",
    "                # Convert numpy array to tensor efficiently\n",
    "                ngram_tensor = torch.from_numpy(ngram_features).unsqueeze(0).to(device)\n",
    "                \n",
    "                # Get model output\n",
    "                output = model(context_tensor, ngram_tensor)\n",
    "                candidate_idx = word2idx.get(candidate, word2idx['<UNK>'])\n",
    "                score = output[0, candidate_idx].item()\n",
    "                \n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_word = candidate\n",
    "                \n",
    "                # If this is the answer, compute loss\n",
    "                if candidate == answer:\n",
    "                    answer_idx = torch.tensor([candidate_idx], dtype=torch.long).to(device)\n",
    "                    answer_loss = criterion(output, answer_idx).item()\n",
    "            \n",
    "            # Add to metrics\n",
    "            if answer_loss is not None:\n",
    "                total_loss += answer_loss\n",
    "            \n",
    "            if best_word == answer:\n",
    "                correct += 1\n",
    "            \n",
    "            total += 1\n",
    "    \n",
    "    avg_loss = total_loss / total if total > 0 else 0.0\n",
    "    accuracy = (correct / total * 100) if total > 0 else 0.0\n",
    "    \n",
    "    return avg_loss, accuracy, correct, total\n",
    "\n",
    "print(\"Dev set evaluation function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.8 Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.3\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 5\n",
    "MAX_LEN = 50\n",
    "\n",
    "# Use subset for faster training (adjust as needed)\n",
    "TRAIN_SIZE = 500000  # Use 500K sentences\n",
    "\n",
    "print(\"Hyperparameters:\")\n",
    "print(f\"Embedding dim: {EMBEDDING_DIM}\")\n",
    "print(f\"Hidden dim: {HIDDEN_DIM}\")\n",
    "print(f\"Num layers: {NUM_LAYERS}\")\n",
    "print(f\"Dropout: {DROPOUT}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Num epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Training size: {TRAIN_SIZE} sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.9 Create Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load n-gram models\n",
    "print(\"Loading n-gram models...\")\n",
    "with open('ngram_models_v2.pkl', 'rb') as f:\n",
    "    ngram_models = pickle.load(f)\n",
    "print(\"N-gram models loaded!\")\n",
    "\n",
    "# Create dataset\n",
    "print(f\"\\nCreating dataset with {TRAIN_SIZE} sentences...\")\n",
    "train_dataset = NgramDataset(\n",
    "    train_sentences[:TRAIN_SIZE],\n",
    "    word2idx,\n",
    "    ngram_models,\n",
    "    max_len=MAX_LEN\n",
    ")\n",
    "\n",
    "# Create dataloader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"Dataset created: {len(train_dataset)} examples\")\n",
    "print(f\"Number of batches: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.10 Initialize Model and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb\n",
    "wandb.init(\n",
    "    project=\"predictive-keyboard-bilstm-ngram-v2\",\n",
    "    config={\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"epochs\": NUM_EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"embedding_dim\": EMBEDDING_DIM,\n",
    "        \"hidden_dim\": HIDDEN_DIM,\n",
    "        \"num_layers\": NUM_LAYERS,\n",
    "        \"dropout\": DROPOUT,\n",
    "        \"max_len\": MAX_LEN,\n",
    "        \"train_size\": TRAIN_SIZE,\n",
    "        \"architecture\": \"BiLSTM with N-gram features v2\",\n",
    "        \"ngram_features\": \"unigram, bigram, trigram, 4-gram\",\n",
    "        \"validation\": \"dev_set.csv\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "model = BiLSTM_Ngram(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    ngram_feature_dim=4\n",
    ").to(device)\n",
    "\n",
    "print(\"Model initialized:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Watch model with wandb\n",
    "wandb.watch(model, log='all', log_freq=100)\n",
    "\n",
    "print(\"\\nOptimizer: Adam\")\n",
    "print(f\"Loss function: CrossEntropyLoss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.11 Training Loop with Dev Set Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\\n\")\n",
    "\n",
    "best_dev_accuracy = 0.0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # ====== TRAINING PHASE ======\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [TRAIN]\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        context = batch['context'].to(device)\n",
    "        ngram_features = batch['ngram_features'].to(device)\n",
    "        target = batch['target'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output = model(context, ngram_features)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        total_train_loss += loss.item()\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        correct_train += (predicted == target).sum().item()\n",
    "        total_train += target.size(0)\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': total_train_loss / (progress_bar.n + 1),\n",
    "            'acc': 100 * correct_train / total_train\n",
    "        })\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    \n",
    "    # ====== VALIDATION PHASE ======\n",
    "    print(f\"\\nEpoch {epoch+1}: Evaluating on dev set...\")\n",
    "    dev_loss, dev_accuracy, dev_correct, dev_total = evaluate_on_dev_set(\n",
    "        model, dev_df, word2idx, vocab_by_first_letter, ngram_models, MAX_LEN, device, criterion\n",
    "    )\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} Summary:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"TRAIN - Loss: {avg_train_loss:.4f} | Accuracy: {train_accuracy:.2f}%\")\n",
    "    print(f\"DEV   - Loss: {dev_loss:.4f} | Accuracy: {dev_accuracy:.2f}% ({dev_correct}/{dev_total})\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Log to wandb\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": avg_train_loss,\n",
    "        \"train_accuracy\": train_accuracy,\n",
    "        \"dev_loss\": dev_loss,\n",
    "        \"dev_accuracy\": dev_accuracy,\n",
    "        \"dev_correct\": dev_correct,\n",
    "        \"dev_total\": dev_total\n",
    "    })\n",
    "    \n",
    "    # Save checkpoint\n",
    "    checkpoint_path = f'bilstm_ngram_v2_epoch{epoch+1}.pt'\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': avg_train_loss,\n",
    "        'dev_loss': dev_loss,\n",
    "        'dev_accuracy': dev_accuracy,\n",
    "    }, checkpoint_path)\n",
    "    print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if dev_accuracy > best_dev_accuracy:\n",
    "        best_dev_accuracy = dev_accuracy\n",
    "        torch.save(model.state_dict(), 'bilstm_ngram_v2_best.pt')\n",
    "        print(f\"ðŸŽ‰ New best dev accuracy: {dev_accuracy:.2f}% - Model saved!\\n\")\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(f\"Best dev accuracy achieved: {best_dev_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.12 Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "torch.save(model.state_dict(), 'bilstm_ngram_v2_final.pt')\n",
    "print(\"Final model saved: bilstm_ngram_v2_final.pt\")\n",
    "\n",
    "# Save vocabulary\n",
    "with open('bilstm_vocab_v2.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'word2idx': word2idx,\n",
    "        'idx2word': idx2word\n",
    "    }, f)\n",
    "print(\"Vocabulary saved: bilstm_vocab_v2.pkl\")\n",
    "\n",
    "# Save model as wandb artifact\n",
    "artifact = wandb.Artifact('bilstm-ngram-model-v2', type='model')\n",
    "artifact.add_file('bilstm_ngram_v2_best.pt')\n",
    "artifact.add_file('bilstm_ngram_v2_final.pt')\n",
    "artifact.add_file('bilstm_vocab_v2.pkl')\n",
    "wandb.log_artifact(artifact)\n",
    "print(\"Model saved as wandb artifact!\")\n",
    "\n",
    "# Update summary\n",
    "wandb.summary.update({\n",
    "    \"best_dev_accuracy\": best_dev_accuracy,\n",
    "    \"total_parameters\": sum(p.numel() for p in model.parameters())\n",
    "})\n",
    "\n",
    "# Finish wandb run\n",
    "wandb.finish()\n",
    "print(\"wandb run finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.13 Test Set Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load('bilstm_ngram_v2_best.pt', map_location=device))\n",
    "model.eval()\n",
    "print(\"Loaded best model\")\n",
    "\n",
    "# Load test set\n",
    "test_df = pd.read_csv('test_set_no_answer.csv')\n",
    "print(f\"Test set loaded: {len(test_df)} examples\")\n",
    "\n",
    "# Generate predictions\n",
    "test_predictions = []\n",
    "\n",
    "for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Predicting test set\"):\n",
    "    context = row['context']\n",
    "    first_letter = row['first letter']\n",
    "    \n",
    "    # Tokenize context\n",
    "    words = ['<s>'] + context.lower().split()\n",
    "    \n",
    "    # Convert to indices\n",
    "    context_indices = [word2idx.get(w, word2idx['<UNK>']) for w in words]\n",
    "    \n",
    "    # Pad/truncate\n",
    "    if len(context_indices) > MAX_LEN:\n",
    "        context_indices = context_indices[-MAX_LEN:]\n",
    "        words = words[-MAX_LEN:]\n",
    "    else:\n",
    "        padding = [0] * (MAX_LEN - len(context_indices))\n",
    "        context_indices = padding + context_indices\n",
    "    \n",
    "    # Get candidates\n",
    "    candidates = vocab_by_first_letter.get(first_letter.lower(), set())\n",
    "    if not candidates:\n",
    "        test_predictions.append(first_letter)\n",
    "        continue\n",
    "    \n",
    "    # Score each candidate\n",
    "    best_score = float('-inf')\n",
    "    best_word = None\n",
    "    \n",
    "    context_tensor = torch.tensor([context_indices], dtype=torch.long).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for candidate in candidates:\n",
    "            # Compute n-gram features (returns numpy array)\n",
    "            ngram_features = compute_ngram_features_for_candidate(words, candidate, ngram_models)\n",
    "            # Convert numpy array to tensor efficiently\n",
    "            ngram_tensor = torch.from_numpy(ngram_features).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Get model output\n",
    "            output = model(context_tensor, ngram_tensor)\n",
    "            candidate_idx = word2idx.get(candidate, word2idx['<UNK>'])\n",
    "            score = output[0, candidate_idx].item()\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_word = candidate\n",
    "    \n",
    "    test_predictions.append(best_word if best_word else list(candidates)[0])\n",
    "\n",
    "# Save predictions\n",
    "with open('test_predictions_bilstm_ngram_v2.txt', 'w') as f:\n",
    "    for pred in test_predictions:\n",
    "        f.write(f\"{pred}\\n\")\n",
    "\n",
    "print(f\"\\nTest predictions saved to 'test_predictions_bilstm_ngram_v2.txt'\")\n",
    "print(f\"Total predictions: {len(test_predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.14 Summary\n",
    "\n",
    "**Bi-LSTM with N-gram Features v2 Performance:**\n",
    "- Architecture: Bidirectional LSTM (2 layers, 512 hidden units)\n",
    "- N-gram features: 4 features (unigram, bigram, trigram, 4-gram probabilities)\n",
    "- Embedding dimension: 256\n",
    "- Dropout: 0.3\n",
    "- Expected accuracy: 60-75%\n",
    "\n",
    "**Key Differences from v1:**\n",
    "- Validation on dev_set.csv after each epoch\n",
    "- Both loss and accuracy tracked on dev set\n",
    "- Best model saved based on dev accuracy\n",
    "- More realistic training dynamics\n",
    "\n",
    "**Advantages:**\n",
    "- Better monitoring of generalization\n",
    "- Early stopping possible based on dev performance\n",
    "- More faithful to actual task format"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
